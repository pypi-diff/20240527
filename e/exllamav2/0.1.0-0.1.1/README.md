# Comparing `tmp/exllamav2-0.1.0-py3-none-any.whl.zip` & `tmp/exllamav2-0.1.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,159 +1,157 @@
-Zip file size: 247086 bytes, number of entries: 157
--rw-r--r--  2.0 unx      482 b- defN 24-May-25 20:52 exllamav2/__init__.py
--rw-r--r--  2.0 unx    23883 b- defN 24-May-20 21:07 exllamav2/architecture.py
--rw-r--r--  2.0 unx    34762 b- defN 24-May-25 20:52 exllamav2/attn.py
--rw-r--r--  2.0 unx    16389 b- defN 24-May-25 20:52 exllamav2/cache.py
--rw-r--r--  2.0 unx     2001 b- defN 24-May-16 22:11 exllamav2/compat.py
--rw-r--r--  2.0 unx    12385 b- defN 24-May-16 01:34 exllamav2/config.py
--rw-r--r--  2.0 unx     4545 b- defN 24-May-25 20:52 exllamav2/embedding.py
--rw-r--r--  2.0 unx    12302 b- defN 24-May-25 23:35 exllamav2/ext.py
--rw-r--r--  2.0 unx     6399 b- defN 24-May-11 12:57 exllamav2/fasttensors.py
--rw-r--r--  2.0 unx     3969 b- defN 24-Apr-18 13:58 exllamav2/headnorm.py
--rw-r--r--  2.0 unx     3502 b- defN 24-Apr-18 13:58 exllamav2/layernorm.py
--rw-r--r--  2.0 unx    11159 b- defN 24-May-25 20:52 exllamav2/linear.py
--rw-r--r--  2.0 unx     6408 b- defN 24-Apr-18 13:58 exllamav2/lora.py
--rw-r--r--  2.0 unx    12405 b- defN 24-May-08 15:48 exllamav2/mlp.py
--rw-r--r--  2.0 unx    31792 b- defN 24-May-25 20:52 exllamav2/model.py
--rw-r--r--  2.0 unx     4737 b- defN 24-Mar-29 18:08 exllamav2/model_init.py
--rw-r--r--  2.0 unx     7100 b- defN 24-May-11 12:57 exllamav2/module.py
--rw-r--r--  2.0 unx    14288 b- defN 24-Apr-18 13:58 exllamav2/moe_mlp.py
--rw-r--r--  2.0 unx     5262 b- defN 24-Apr-18 13:58 exllamav2/parallel_decoder.py
--rw-r--r--  2.0 unx     3172 b- defN 24-May-11 12:57 exllamav2/pos_embedding.py
--rw-r--r--  2.0 unx     3646 b- defN 24-Apr-18 13:58 exllamav2/rmsnorm.py
--rw-r--r--  2.0 unx     6187 b- defN 24-May-25 20:52 exllamav2/util.py
--rw-r--r--  2.0 unx       21 b- defN 24-May-25 20:52 exllamav2/version.py
--rw-r--r--  2.0 unx      337 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/config.h
--rw-r--r--  2.0 unx     4378 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_bindings.cpp
--rw-r--r--  2.0 unx     8037 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_cache.cpp
--rw-r--r--  2.0 unx     1105 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_cache.h
--rw-r--r--  2.0 unx     1455 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_gemm.cpp
--rw-r--r--  2.0 unx      161 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_gemm.h
--rw-r--r--  2.0 unx     2505 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_hadamard.cpp
--rw-r--r--  2.0 unx       82 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_hadamard.h
--rw-r--r--  2.0 unx     3158 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_norm.cpp
--rw-r--r--  2.0 unx      639 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_norm.h
--rw-r--r--  2.0 unx     6577 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_qattn.cpp
--rw-r--r--  2.0 unx     1764 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_qattn.h
--rw-r--r--  2.0 unx     5665 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_qmatrix.cpp
--rw-r--r--  2.0 unx      818 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/ext_qmatrix.h
--rw-r--r--  2.0 unx     8846 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_qmlp.cpp
--rw-r--r--  2.0 unx     2274 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_qmlp.h
--rw-r--r--  2.0 unx     5533 b- defN 24-Apr-28 15:55 exllamav2/exllamav2_ext/ext_quant.cpp
--rw-r--r--  2.0 unx      895 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/ext_quant.h
--rw-r--r--  2.0 unx     1339 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_rope.cpp
--rw-r--r--  2.0 unx      186 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/ext_rope.h
--rw-r--r--  2.0 unx      344 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_safetensors.cpp
--rw-r--r--  2.0 unx       29 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/ext_safetensors.h
--rw-r--r--  2.0 unx     9654 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/ext_sampling.cpp
--rw-r--r--  2.0 unx     1379 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/ext_sampling.h
--rw-r--r--  2.0 unx     1275 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/avx2_target.h
--rw-r--r--  2.0 unx    24414 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/avx_mathfun.h
--rw-r--r--  2.0 unx     1299 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/generator.cpp
--rw-r--r--  2.0 unx      274 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/generator.h
--rw-r--r--  2.0 unx     1627 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/profiling.cpp
--rw-r--r--  2.0 unx      362 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/profiling.h
--rw-r--r--  2.0 unx     2265 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
--rw-r--r--  2.0 unx      611 b- defN 24-Mar-19 16:45 exllamav2/exllamav2_ext/cpp/quantize_func.h
--rw-r--r--  2.0 unx     8308 b- defN 24-Feb-27 07:47 exllamav2/exllamav2_ext/cpp/safetensors.cpp
--rw-r--r--  2.0 unx      801 b- defN 24-Jan-20 11:49 exllamav2/exllamav2_ext/cpp/safetensors.h
--rw-r--r--  2.0 unx    20446 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cpp/sampling.cpp
--rw-r--r--  2.0 unx     2283 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/sampling.h
--rw-r--r--  2.0 unx     3089 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
--rw-r--r--  2.0 unx      334 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cpp/sampling_avx2.h
--rw-r--r--  2.0 unx     2204 b- defN 24-Mar-27 03:08 exllamav2/exllamav2_ext/cpp/util.h
--rw-r--r--  2.0 unx    12970 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cuda/cache.cu
--rw-r--r--  2.0 unx     1658 b- defN 24-May-25 20:52 exllamav2/exllamav2_ext/cuda/cache.cuh
--rw-r--r--  2.0 unx     2831 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/compat.cuh
--rw-r--r--  2.0 unx     2604 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_add.cu
--rw-r--r--  2.0 unx      265 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_add.cuh
--rw-r--r--  2.0 unx     7202 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_gemm.cu
--rw-r--r--  2.0 unx      667 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
--rw-r--r--  2.0 unx     3160 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/head_norm.cu
--rw-r--r--  2.0 unx      329 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/head_norm.cuh
--rw-r--r--  2.0 unx     4970 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/layer_norm.cu
--rw-r--r--  2.0 unx      302 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/layer_norm.cuh
--rw-r--r--  2.0 unx      935 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/lora.cu
--rw-r--r--  2.0 unx      463 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/lora.cuh
--rw-r--r--  2.0 unx     4293 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
--rw-r--r--  2.0 unx     7287 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
--rw-r--r--  2.0 unx      549 b- defN 24-Apr-16 01:26 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
--rw-r--r--  2.0 unx     6642 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/q_attn.cu
--rw-r--r--  2.0 unx     2392 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/q_attn.cuh
--rw-r--r--  2.0 unx     8809 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_gemm.cu
--rw-r--r--  2.0 unx      614 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
--rw-r--r--  2.0 unx     1705 b- defN 24-Mar-19 16:45 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
--rw-r--r--  2.0 unx    19672 b- defN 24-Apr-14 21:33 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
--rw-r--r--  2.0 unx     7516 b- defN 24-Jan-20 11:49 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
--rw-r--r--  2.0 unx     7516 b- defN 24-Jan-12 04:49 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq_old.cuh
--rw-r--r--  2.0 unx    21853 b- defN 24-Apr-21 13:11 exllamav2/exllamav2_ext/cuda/q_matrix.cu
--rw-r--r--  2.0 unx     1910 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
--rw-r--r--  2.0 unx     8195 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/q_mlp.cu
--rw-r--r--  2.0 unx     2924 b- defN 24-Mar-19 17:20 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
--rw-r--r--  2.0 unx     5762 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
--rw-r--r--  2.0 unx     7296 b- defN 24-Mar-30 08:21 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
--rw-r--r--  2.0 unx     9975 b- defN 24-May-11 12:57 exllamav2/exllamav2_ext/cuda/quantize.cu
--rw-r--r--  2.0 unx     1303 b- defN 24-Apr-28 15:54 exllamav2/exllamav2_ext/cuda/quantize.cuh
--rw-r--r--  2.0 unx     3834 b- defN 24-Apr-18 13:58 exllamav2/exllamav2_ext/cuda/rms_norm.cu
--rw-r--r--  2.0 unx      277 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/rms_norm.cuh
--rw-r--r--  2.0 unx     5085 b- defN 24-Mar-10 20:56 exllamav2/exllamav2_ext/cuda/rms_norm_.cu
--rw-r--r--  2.0 unx     7215 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/rope.cu
--rw-r--r--  2.0 unx      736 b- defN 24-Apr-28 15:50 exllamav2/exllamav2_ext/cuda/rope.cuh
--rw-r--r--  2.0 unx      624 b- defN 23-Dec-16 22:37 exllamav2/exllamav2_ext/cuda/util.cu
--rw-r--r--  2.0 unx     3345 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/util.cuh
--rw-r--r--  2.0 unx     1313 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
--rw-r--r--  2.0 unx     8080 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
--rw-r--r--  2.0 unx      694 b- defN 24-Mar-17 09:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
--rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
--rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
--rw-r--r--  2.0 unx      634 b- defN 24-Feb-01 05:02 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
--rw-r--r--  2.0 unx     2881 b- defN 23-Nov-27 16:47 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
--rw-r--r--  2.0 unx     5782 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
--rw-r--r--  2.0 unx     5755 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
--rw-r--r--  2.0 unx     7342 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh
--rw-r--r--  2.0 unx     4530 b- defN 24-Mar-11 04:47 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
--rw-r--r--  2.0 unx      643 b- defN 23-Oct-14 13:43 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh
--rw-r--r--  2.0 unx     1367 b- defN 24-May-01 22:03 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
--rw-r--r--  2.0 unx      513 b- defN 24-May-25 20:52 exllamav2/generator/__init__.py
--rw-r--r--  2.0 unx    12853 b- defN 24-May-25 20:52 exllamav2/generator/base.py
--rw-r--r--  2.0 unx    80386 b- defN 24-May-25 22:25 exllamav2/generator/dynamic.py
--rw-r--r--  2.0 unx     2687 b- defN 24-May-25 20:52 exllamav2/generator/dynamic_async.py
--rw-r--r--  2.0 unx      497 b- defN 24-Mar-29 18:08 exllamav2/generator/hooks.py
--rw-r--r--  2.0 unx     2370 b- defN 24-Mar-29 18:08 exllamav2/generator/ngram.py
--rw-r--r--  2.0 unx    10162 b- defN 24-May-25 20:52 exllamav2/generator/sampler.py
--rw-r--r--  2.0 unx    39172 b- defN 24-May-25 20:52 exllamav2/generator/streaming.py
--rw-r--r--  2.0 unx      242 b- defN 24-Feb-27 07:47 exllamav2/generator/filters/__init__.py
--rw-r--r--  2.0 unx      756 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/base.py
--rw-r--r--  2.0 unx     1866 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/prefix.py
--rw-r--r--  2.0 unx     3965 b- defN 24-Mar-29 18:08 exllamav2/generator/filters/select.py
--rw-r--r--  2.0 unx     4346 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard.py
--rw-r--r--  2.0 unx        1 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_1.txt
--rw-r--r--  2.0 unx    10099 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_100.txt
--rw-r--r--  2.0 unx    13571 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_116.txt
--rw-r--r--  2.0 unx    24491 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_156.txt
--rw-r--r--  2.0 unx    29755 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_172.txt
--rw-r--r--  2.0 unx    35531 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_188.txt
--rw-r--r--  2.0 unx    55931 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_236.txt
--rw-r--r--  2.0 unx    59779 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_244.txt
--rw-r--r--  2.0 unx   183612 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_428.txt
--rw-r--r--  2.0 unx     2755 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_52.txt
--rw-r--r--  2.0 unx     8555 b- defN 24-Apr-21 13:11 exllamav2/hadamard/hadamard_92.txt
--rw-r--r--  2.0 unx    58982 b- defN 24-Apr-21 13:11 exllamav2/hadamard/primes.txt
--rw-r--r--  2.0 unx      107 b- defN 23-Oct-14 13:47 exllamav2/server/__init__.py
--rw-r--r--  2.0 unx     1623 b- defN 24-Jan-20 11:49 exllamav2/server/websocket.py
--rw-r--r--  2.0 unx    10260 b- defN 24-May-09 16:29 exllamav2/server/websocket_actions.py
--rw-r--r--  2.0 unx      217 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/__init__.py
--rw-r--r--  2.0 unx     2157 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/base.py
--rw-r--r--  2.0 unx     3028 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/hf.py
--rw-r--r--  2.0 unx     1972 b- defN 24-Mar-29 18:08 exllamav2/tokenizer/spm.py
--rw-r--r--  2.0 unx    23709 b- defN 24-May-25 20:52 exllamav2/tokenizer/tokenizer.py
--rw-r--r--  2.0 unx     1035 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx      427 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    14579 b- defN 24-May-26 07:21 exllamav2-0.1.0.dist-info/RECORD
-157 files, 1295085 bytes uncompressed, 223716 bytes compressed:  82.7%
+Zip file size: 245009 bytes, number of entries: 155
+-rw-r--r--  2.0 unx      482 b- defN 24-May-27 16:52 exllamav2/__init__.py
+-rw-r--r--  2.0 unx    23883 b- defN 24-May-27 16:52 exllamav2/architecture.py
+-rw-r--r--  2.0 unx    39138 b- defN 24-May-27 16:52 exllamav2/attn.py
+-rw-r--r--  2.0 unx    16389 b- defN 24-May-27 16:52 exllamav2/cache.py
+-rw-r--r--  2.0 unx     2001 b- defN 24-May-27 16:52 exllamav2/compat.py
+-rw-r--r--  2.0 unx    12574 b- defN 24-May-27 16:52 exllamav2/config.py
+-rw-r--r--  2.0 unx     4545 b- defN 24-May-27 16:52 exllamav2/embedding.py
+-rw-r--r--  2.0 unx    12302 b- defN 24-May-27 16:52 exllamav2/ext.py
+-rw-r--r--  2.0 unx     6399 b- defN 24-May-27 16:52 exllamav2/fasttensors.py
+-rw-r--r--  2.0 unx     3969 b- defN 24-May-27 16:52 exllamav2/headnorm.py
+-rw-r--r--  2.0 unx     3502 b- defN 24-May-27 16:52 exllamav2/layernorm.py
+-rw-r--r--  2.0 unx    11159 b- defN 24-May-27 16:52 exllamav2/linear.py
+-rw-r--r--  2.0 unx     6408 b- defN 24-May-27 16:52 exllamav2/lora.py
+-rw-r--r--  2.0 unx    12405 b- defN 24-May-27 16:52 exllamav2/mlp.py
+-rw-r--r--  2.0 unx    31876 b- defN 24-May-27 16:52 exllamav2/model.py
+-rw-r--r--  2.0 unx     4931 b- defN 24-May-27 16:52 exllamav2/model_init.py
+-rw-r--r--  2.0 unx     7100 b- defN 24-May-27 16:52 exllamav2/module.py
+-rw-r--r--  2.0 unx    14288 b- defN 24-May-27 16:52 exllamav2/moe_mlp.py
+-rw-r--r--  2.0 unx     5262 b- defN 24-May-27 16:52 exllamav2/parallel_decoder.py
+-rw-r--r--  2.0 unx     3172 b- defN 24-May-27 16:52 exllamav2/pos_embedding.py
+-rw-r--r--  2.0 unx     3646 b- defN 24-May-27 16:52 exllamav2/rmsnorm.py
+-rw-r--r--  2.0 unx     6187 b- defN 24-May-27 16:52 exllamav2/util.py
+-rw-r--r--  2.0 unx       21 b- defN 24-May-27 16:52 exllamav2/version.py
+-rw-r--r--  2.0 unx      337 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/config.h
+-rw-r--r--  2.0 unx     4378 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_bindings.cpp
+-rw-r--r--  2.0 unx     8037 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_cache.cpp
+-rw-r--r--  2.0 unx     1105 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_cache.h
+-rw-r--r--  2.0 unx     1455 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_gemm.cpp
+-rw-r--r--  2.0 unx      161 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_gemm.h
+-rw-r--r--  2.0 unx     2505 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_hadamard.cpp
+-rw-r--r--  2.0 unx       82 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_hadamard.h
+-rw-r--r--  2.0 unx     3158 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_norm.cpp
+-rw-r--r--  2.0 unx      639 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_norm.h
+-rw-r--r--  2.0 unx     6577 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qattn.cpp
+-rw-r--r--  2.0 unx     1764 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qattn.h
+-rw-r--r--  2.0 unx     5665 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmatrix.cpp
+-rw-r--r--  2.0 unx      818 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmatrix.h
+-rw-r--r--  2.0 unx     8846 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmlp.cpp
+-rw-r--r--  2.0 unx     2274 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_qmlp.h
+-rw-r--r--  2.0 unx     5533 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_quant.cpp
+-rw-r--r--  2.0 unx      895 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_quant.h
+-rw-r--r--  2.0 unx     1339 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_rope.cpp
+-rw-r--r--  2.0 unx      186 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_rope.h
+-rw-r--r--  2.0 unx      344 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_safetensors.cpp
+-rw-r--r--  2.0 unx       29 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_safetensors.h
+-rw-r--r--  2.0 unx    10976 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_sampling.cpp
+-rw-r--r--  2.0 unx     1379 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/ext_sampling.h
+-rw-r--r--  2.0 unx     1275 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/avx2_target.h
+-rw-r--r--  2.0 unx    24414 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/avx_mathfun.h
+-rw-r--r--  2.0 unx     1299 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/generator.cpp
+-rw-r--r--  2.0 unx      274 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/generator.h
+-rw-r--r--  2.0 unx     1627 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/profiling.cpp
+-rw-r--r--  2.0 unx      362 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/profiling.h
+-rw-r--r--  2.0 unx     2265 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/quantize_func.cpp
+-rw-r--r--  2.0 unx      611 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/quantize_func.h
+-rw-r--r--  2.0 unx     8308 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/safetensors.cpp
+-rw-r--r--  2.0 unx      801 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/safetensors.h
+-rw-r--r--  2.0 unx    20446 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling.cpp
+-rw-r--r--  2.0 unx     2283 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling.h
+-rw-r--r--  2.0 unx     3089 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp
+-rw-r--r--  2.0 unx      334 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/sampling_avx2.h
+-rw-r--r--  2.0 unx     2204 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cpp/util.h
+-rw-r--r--  2.0 unx    13564 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/cache.cu
+-rw-r--r--  2.0 unx     1658 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/cache.cuh
+-rw-r--r--  2.0 unx     2831 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/compat.cuh
+-rw-r--r--  2.0 unx     2604 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_add.cu
+-rw-r--r--  2.0 unx      265 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_add.cuh
+-rw-r--r--  2.0 unx     7202 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_gemm.cu
+-rw-r--r--  2.0 unx      667 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/h_gemm.cuh
+-rw-r--r--  2.0 unx     3160 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/head_norm.cu
+-rw-r--r--  2.0 unx      329 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/head_norm.cuh
+-rw-r--r--  2.0 unx     4970 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/layer_norm.cu
+-rw-r--r--  2.0 unx      302 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/layer_norm.cuh
+-rw-r--r--  2.0 unx      935 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/lora.cu
+-rw-r--r--  2.0 unx      463 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/lora.cuh
+-rw-r--r--  2.0 unx     4293 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/matrix_view.cuh
+-rw-r--r--  2.0 unx     7287 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/pack_tensor.cu
+-rw-r--r--  2.0 unx      549 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh
+-rw-r--r--  2.0 unx     6642 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_attn.cu
+-rw-r--r--  2.0 unx     2392 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_attn.cuh
+-rw-r--r--  2.0 unx     8809 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm.cu
+-rw-r--r--  2.0 unx      614 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm.cuh
+-rw-r--r--  2.0 unx     1705 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh
+-rw-r--r--  2.0 unx    19672 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
+-rw-r--r--  2.0 unx     7516 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
+-rw-r--r--  2.0 unx    21853 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_matrix.cu
+-rw-r--r--  2.0 unx     1910 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_matrix.cuh
+-rw-r--r--  2.0 unx     8195 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp.cu
+-rw-r--r--  2.0 unx     2924 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp.cuh
+-rw-r--r--  2.0 unx     5762 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh
+-rw-r--r--  2.0 unx     7296 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh
+-rw-r--r--  2.0 unx     9975 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quantize.cu
+-rw-r--r--  2.0 unx     1303 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quantize.cuh
+-rw-r--r--  2.0 unx     3834 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rms_norm.cu
+-rw-r--r--  2.0 unx      277 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rms_norm.cuh
+-rw-r--r--  2.0 unx     7215 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rope.cu
+-rw-r--r--  2.0 unx      736 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/rope.cuh
+-rw-r--r--  2.0 unx      624 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/util.cu
+-rw-r--r--  2.0 unx     3345 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/util.cuh
+-rw-r--r--  2.0 unx     1313 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu
+-rw-r--r--  2.0 unx     8080 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_2b.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3a.cu
+-rw-r--r--  2.0 unx      694 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_3b.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_1.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_2.cu
+-rw-r--r--  2.0 unx      634 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/comp_units/unit_gptq_3.cu
+-rw-r--r--  2.0 unx     2881 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_2.cuh
+-rw-r--r--  2.0 unx     5782 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_3.cuh
+-rw-r--r--  2.0 unx     5755 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh
+-rw-r--r--  2.0 unx     7342 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh
+-rw-r--r--  2.0 unx     4530 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh
+-rw-r--r--  2.0 unx      643 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh
+-rw-r--r--  2.0 unx     1367 b- defN 24-May-27 16:52 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh
+-rw-r--r--  2.0 unx      513 b- defN 24-May-27 16:52 exllamav2/generator/__init__.py
+-rw-r--r--  2.0 unx    12853 b- defN 24-May-27 16:52 exllamav2/generator/base.py
+-rw-r--r--  2.0 unx    80867 b- defN 24-May-27 16:52 exllamav2/generator/dynamic.py
+-rw-r--r--  2.0 unx     2687 b- defN 24-May-27 16:52 exllamav2/generator/dynamic_async.py
+-rw-r--r--  2.0 unx      497 b- defN 24-May-27 16:52 exllamav2/generator/hooks.py
+-rw-r--r--  2.0 unx     2370 b- defN 24-May-27 16:52 exllamav2/generator/ngram.py
+-rw-r--r--  2.0 unx    10380 b- defN 24-May-27 16:52 exllamav2/generator/sampler.py
+-rw-r--r--  2.0 unx    39172 b- defN 24-May-27 16:52 exllamav2/generator/streaming.py
+-rw-r--r--  2.0 unx      242 b- defN 24-May-27 16:52 exllamav2/generator/filters/__init__.py
+-rw-r--r--  2.0 unx      756 b- defN 24-May-27 16:52 exllamav2/generator/filters/base.py
+-rw-r--r--  2.0 unx     1866 b- defN 24-May-27 16:52 exllamav2/generator/filters/prefix.py
+-rw-r--r--  2.0 unx     3965 b- defN 24-May-27 16:52 exllamav2/generator/filters/select.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard.py
+-rw-r--r--  2.0 unx        1 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_1.txt
+-rw-r--r--  2.0 unx    10099 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_100.txt
+-rw-r--r--  2.0 unx    13571 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_116.txt
+-rw-r--r--  2.0 unx    24491 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_156.txt
+-rw-r--r--  2.0 unx    29755 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_172.txt
+-rw-r--r--  2.0 unx    35531 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_188.txt
+-rw-r--r--  2.0 unx    55931 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_236.txt
+-rw-r--r--  2.0 unx    59779 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_244.txt
+-rw-r--r--  2.0 unx   183612 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_428.txt
+-rw-r--r--  2.0 unx     2755 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_52.txt
+-rw-r--r--  2.0 unx     8555 b- defN 24-May-27 16:52 exllamav2/hadamard/hadamard_92.txt
+-rw-r--r--  2.0 unx    58982 b- defN 24-May-27 16:52 exllamav2/hadamard/primes.txt
+-rw-r--r--  2.0 unx      107 b- defN 24-May-27 16:52 exllamav2/server/__init__.py
+-rw-r--r--  2.0 unx     1623 b- defN 24-May-27 16:52 exllamav2/server/websocket.py
+-rw-r--r--  2.0 unx    10260 b- defN 24-May-27 16:52 exllamav2/server/websocket_actions.py
+-rw-r--r--  2.0 unx      217 b- defN 24-May-27 16:52 exllamav2/tokenizer/__init__.py
+-rw-r--r--  2.0 unx     2157 b- defN 24-May-27 16:52 exllamav2/tokenizer/base.py
+-rw-r--r--  2.0 unx     3028 b- defN 24-May-27 16:52 exllamav2/tokenizer/hf.py
+-rw-r--r--  2.0 unx     1972 b- defN 24-May-27 16:52 exllamav2/tokenizer/spm.py
+-rw-r--r--  2.0 unx    23709 b- defN 24-May-27 16:52 exllamav2/tokenizer/tokenizer.py
+-rw-r--r--  2.0 unx     1035 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx      421 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    14370 b- defN 24-May-27 16:53 exllamav2-0.1.1.dist-info/RECORD
+155 files, 1289727 bytes uncompressed, 221983 bytes compressed:  82.8%
```

## zipnote {}

```diff
@@ -249,17 +249,14 @@
 
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh
 Comment: 
 
-Filename: exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq_old.cuh
-Comment: 
-
 Filename: exllamav2/exllamav2_ext/cuda/q_matrix.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_matrix.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/q_mlp.cu
@@ -282,17 +279,14 @@
 
 Filename: exllamav2/exllamav2_ext/cuda/rms_norm.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/rms_norm.cuh
 Comment: 
 
-Filename: exllamav2/exllamav2_ext/cuda/rms_norm_.cu
-Comment: 
-
 Filename: exllamav2/exllamav2_ext/cuda/rope.cu
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/rope.cuh
 Comment: 
 
 Filename: exllamav2/exllamav2_ext/cuda/util.cu
@@ -450,23 +444,23 @@
 
 Filename: exllamav2/tokenizer/spm.py
 Comment: 
 
 Filename: exllamav2/tokenizer/tokenizer.py
 Comment: 
 
-Filename: exllamav2-0.1.0.dist-info/LICENSE
+Filename: exllamav2-0.1.1.dist-info/LICENSE
 Comment: 
 
-Filename: exllamav2-0.1.0.dist-info/METADATA
+Filename: exllamav2-0.1.1.dist-info/METADATA
 Comment: 
 
-Filename: exllamav2-0.1.0.dist-info/WHEEL
+Filename: exllamav2-0.1.1.dist-info/WHEEL
 Comment: 
 
-Filename: exllamav2-0.1.0.dist-info/top_level.txt
+Filename: exllamav2-0.1.1.dist-info/top_level.txt
 Comment: 
 
-Filename: exllamav2-0.1.0.dist-info/RECORD
+Filename: exllamav2-0.1.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## exllamav2/attn.py

```diff
@@ -9,15 +9,14 @@
 from exllamav2.linear import ExLlamaV2Linear
 from exllamav2.cache import ExLlamaV2CacheBase
 from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
 from exllamav2.compat import safe_move_tensor
 from exllamav2.lora import ExLlamaV2Lora
 from exllamav2.architecture import RopeStyle
 import math
-# import xformers.ops as xops
 # from exllamav2.util import list_live_tensors, set_snapshot, diff_snapshot, print_vram_usage_peak
 # import torch.nn.functional as F
 
 from typing import TYPE_CHECKING
 if TYPE_CHECKING:
     from exllamav2.model import ExLlamaV2
 
@@ -42,14 +41,24 @@
         from flash_attn import flash_attn_func, flash_attn_with_kvcache
         has_flash_attn = True
         has_flash_attn_with_paged = True
 
 except ModuleNotFoundError:
     pass
 
+has_xformers = False
+try:
+    import xformers.ops as xops
+    # LowerTriangularFromBottomRightMask was added in xformers version 2.4
+    from xformers.ops.fmha import LowerTriangularFromBottomRightMask
+    has_xformers = True
+except ModuleNotFoundError:
+    pass
+
+
 def assert_paged_attn():
     global has_flash_attn_with_paged
     assert has_flash_attn_with_paged, \
         "Paged attention required Flash Attention 2.5.7 or later"
 
 
 class ExLlamaV2Attention(ExLlamaV2Module):
@@ -455,18 +464,21 @@
 
         if self.model.config.num_key_value_heads == self.model.config.num_attention_heads: return 0
         return 2 * self.model.config.max_seq_len * self.model.config.max_batch_size * self.model.config.num_attention_heads * self.model.config.head_dim * 2 + 128
 
 
     def temp_attn_size(self):
         global has_flash_attn
+        global has_xformers
 
         att_max = min(self.model.config.max_attention_size, self.model.config.max_seq_len ** 2)
 
-        if has_flash_attn and not self.model.config.no_flash_attn:
+        if (has_flash_attn and not self.model.config.no_flash_attn) or (has_xformers and not self.model.config.no_xformers) :
+            #in sm>=80 devices, xformers uses the same memory as flash_attn
+            #todo: due to the different implementions. in sm<80 devices, xformers uses less memory than it in sm>=80. There may still be room for optimization.
             eff = self.model.config.max_attention_size ** 0.5 / 190  # based on supposed memory savings listed in flash-attn repo + some fudging
             att_max //= eff
 
         return 2 * att_max * self.model.config.num_attention_heads * 2 + 128
 
 
     def set_device_idx(self, idx):
@@ -494,51 +506,80 @@
     def forward_paged(self,
                       hidden_states: torch.Tensor,
                       cache: ExLlamaV2CacheBase | None = None,
                       attn_params: ExLlamaV2Attention.PagedParams | None = None,
                       loras: list[ExLlamaV2Lora] | None = None,
                       **kwargs) -> torch.Tensor:
 
+        is_q = self.q_handle is not None
         cfg = self.model.config
-        constants = self.model.get_device_tensors(self.device_idx)
+        constants = self.model.get_device_tensors(self.device_idx, scratch = is_q)
 
         page_size = attn_params.page_size
 
         batch_size, q_len, _ = hidden_states.shape
-        q = torch.empty((batch_size, q_len, cfg.num_attention_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
-        k = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
-        v = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
 
         cache_seqlens = attn_params.get_cache_seqlens(self.device())
         block_table = attn_params.get_block_index(self.device())
 
         k_cache, v_cache = cache.get_kv_state(self.layer_idx, batch_size, 0, 1, page_size, cache_seqlens, block_table)
         k_cache = k_cache.view(k_cache.shape[1] // page_size, page_size, k_cache.shape[2], k_cache.shape[3])
         v_cache = v_cache.view(v_cache.shape[1] // page_size, page_size, v_cache.shape[2], v_cache.shape[3])
 
-        if loras is None or self.temp_lora_size == 0:
-            pass_loras, pass_lora_temp = [], none_tensor
-        else:
-            pass_loras, pass_lora_temp = [id(x) for x in loras], torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
+        if is_q:
+            q = torch.empty((batch_size, q_len, cfg.num_attention_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+            k = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+            v = torch.empty((batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim), device = hidden_states.device, dtype = torch.half)
+
+            if loras is None or self.temp_lora_size == 0:
+                pass_loras = []
+                pass_lora_temp = none_tensor
+            else:
+                pass_loras = [id(x) for x in loras]
+                pass_lora_temp = torch.empty((self.temp_lora_size,), dtype = torch.half, device = hidden_states.device)
 
-        ext_c.q_attn_forward_1(
-            self.q_handle,
-            hidden_states,
-            batch_size,
-            q_len,
-            0,
-            attn_params.get_cache_seqlens(self.device()),
-            q,
-            k,
-            v,
-            constants.sin,
-            constants.cos,
-            pass_loras,
-            pass_lora_temp
-        )
+            ext_c.q_attn_forward_1(
+                self.q_handle,
+                hidden_states,
+                batch_size,
+                q_len,
+                0,
+                attn_params.get_cache_seqlens(self.device()),
+                q,
+                k,
+                v,
+                constants.sin,
+                constants.cos,
+                pass_loras,
+                pass_lora_temp
+            )
+        else:
+            residual = hidden_states
+            hidden_states = self.input_layernorm.forward(hidden_states) if self.has_norm else hidden_states
+            q = self.q_proj.forward(hidden_states, loras = loras)
+            k = self.k_proj.forward(hidden_states, loras = loras)
+            v = self.v_proj.forward(hidden_states, loras = loras)
+            q = q.view(batch_size, q_len, cfg.num_attention_heads, cfg.head_dim)
+            k = k.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
+            v = v.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
+            if cfg.use_qk_norm:
+                q = self.q_norm.forward(q)
+                k = self.k_norm.forward(k)
+            if cfg.arch.rope_style != RopeStyle.NONE:
+                for t, heads in [(q, cfg.num_attention_heads), (k, cfg.num_key_value_heads)]:
+                    ext_c.rope_(
+                        t,
+                        constants.sin,
+                        constants.cos,
+                        0,
+                        heads,
+                        cfg.head_dim,
+                        attn_params.get_cache_seqlens(self.device()),
+                        cfg.arch.rope_style == RopeStyle.NEOX
+                    )
 
         attn_output = flash_attn_with_kvcache(
             q = q,
             k = k,
             v = v,
             k_cache = k_cache,
             v_cache = v_cache,
@@ -548,23 +589,28 @@
         )
         attn_output = attn_output.view((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
         cache.store_kv_state(self.layer_idx, batch_size, 0, q_len, page_size, cache_seqlens, block_table)
 
         # Output projection
 
-        ext_c.q_attn_forward_2(
-            self.q_handle,
-            hidden_states,
-            attn_output,
-            batch_size,
-            q_len,
-            pass_loras,
-            pass_lora_temp
-        )
+        if is_q:
+            ext_c.q_attn_forward_2(
+                self.q_handle,
+                hidden_states,
+                attn_output,
+                batch_size,
+                q_len,
+                pass_loras,
+                pass_lora_temp
+            )
+        else:
+            hidden_states = self.o_proj.forward(attn_output, loras = loras)
+            if self.has_residual:
+                hidden_states += residual
 
         return hidden_states
 
 
     def _attn_matmul(self, batch_size, q_len, q_states, k_states, v_states, attn_params, cfg):
 
         q_states = q_states.transpose(1, 2)
@@ -597,24 +643,52 @@
             v_states,
             causal = True
         )
         attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
         return attn_output
 
 
+    def _attn_xformers(self, batch_size, q_len, q_states, k_states, v_states, attn_params, cfg):
+
+        # xformers memory_efficient_attention, could be beneficial if your device's architecture is less than <sm_80
+        # xformer does not expand the kv automatically, we need to do it manually. The efficiency between
+        # xformers.memory_efficient_attention and flash_attn in >sm_80 are almost the same. But the martix operation
+        # make this implemention much slower.
+
+        k_states = k_states.transpose(1, 2)
+        v_states = v_states.transpose(1, 2)
+
+        k_states = self.repeat_kv(k_states, cfg.num_key_value_groups)
+        v_states = self.repeat_kv(v_states, cfg.num_key_value_groups)
+
+        k_states = k_states.transpose(1, 2)
+        v_states = v_states.transpose(1, 2)
+
+        attn_output = xops.memory_efficient_attention(
+            q_states,
+            k_states,
+            v_states,
+            attn_bias = LowerTriangularFromBottomRightMask()
+        )
+        attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
+
+        return attn_output
+
+
     def forward(self,
                 hidden_states: torch.Tensor,
                 cache: ExLlamaV2CacheBase | None = None,
                 attn_params: ExLlamaV2Attention.Params | None = None,
                 past_len: int | None = None,
                 intermediates: bool = False,
                 loras: list[ExLlamaV2Lora] | None = None,
                 **kwargs) -> torch.Tensor | dict[str: torch.Tensor]:
 
         global has_flash_attn
+        global has_xformers
 
         if isinstance(attn_params, ExLlamaV2Attention.PagedParams):
             return self.forward_paged(
                 hidden_states,
                 cache,
                 attn_params,
                 loras = loras,
@@ -683,18 +757,20 @@
             constants.cos,
             pass_loras,
             pass_lora_temp
         )
 
         # Select attention function
 
-        if cfg.no_flash_attn or not has_flash_attn or not attn_params.is_causal():
-            attn_func = self._attn_matmul
-        else:
+        if (has_flash_attn and not cfg.no_flash_attn) and attn_params.is_causal():
             attn_func = self._attn_flash
+        elif (has_xformers and not cfg.no_xformers) and attn_params.is_causal():
+            attn_func = self._attn_xformers
+        else:
+            attn_func = self._attn_matmul
 
         # Straight attention without cache
 
         if cache is None:
 
             q_states = q_states.view(batch_size, q_len, cfg.num_attention_heads, cfg.head_dim)
             k_states = k_states.view(batch_size, q_len, cfg.num_key_value_heads, cfg.head_dim)
@@ -741,14 +817,16 @@
                       hidden_states: torch.Tensor,
                       cache: ExLlamaV2CacheBase | None = None,
                       attn_params: ExLlamaV2Attention.Params | None = None,
                       past_len: int | None = None,
                       intermediates: bool = False,
                       loras: list[ExLlamaV2Lora] | None = None,
                       **kwargs) -> torch.Tensor | dict:
+        global has_flash_attn
+        global has_xformers
 
         cfg = self.model.config
         num_attention_heads = cfg.num_attention_heads
         num_key_value_heads = cfg.num_key_value_heads
         num_key_value_groups = cfg.num_key_value_groups
         head_dim = cfg.head_dim
         hidden_size = cfg.hidden_size
@@ -802,17 +880,20 @@
             new_values.copy_(value_states)
 
             # Key/value tensors with past
 
             key_states = batch_keys.narrow(1, 0, past_len + q_len).narrow(0, 0, batch_size)
             value_states = batch_values.narrow(1, 0, past_len + q_len).narrow(0, 0, batch_size)
 
+        use_flash_attn = has_flash_attn and not cfg.no_flash_attn
+        use_xformers = has_xformers and not cfg.no_xformers
+
         # Torch matmul attention
 
-        if cfg.no_flash_attn or not has_flash_attn or not attn_params.is_causal():
+        if not (use_flash_attn or use_xformers) or not attn_params.is_causal():
 
             query_states = query_states.transpose(1, 2)
             key_states = key_states.transpose(1, 2)
             value_states = value_states.transpose(1, 2)
 
             key_states = self.repeat_kv(key_states, cfg.num_key_value_groups)
             key_states = key_states.transpose(-1, -2)
@@ -830,25 +911,40 @@
             attn_output = torch.matmul(attn_weights, value_states)
 
             attn_output = attn_output.transpose(1, 2)
             attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
         # Flash Attention 2
 
-        else:
+        elif use_flash_attn:
 
             attn_output = flash_attn_func(
                 query_states,
                 key_states,
                 value_states,
                 # softmax_scale = None if self.scale_factor == 1 else self.scale_factor / math.sqrt(head_dim),
                 causal = True
             )
             attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
 
+        # Xformers attention
+
+        else:
+
+            key_states = key_states.transpose(1, 2)
+            value_states = value_states.transpose(1, 2)
+
+            key_states = self.repeat_kv(key_states, num_key_value_groups)
+            value_states = self.repeat_kv(value_states, num_key_value_groups)
+            key_states = key_states.transpose(1, 2)
+            value_states = value_states.transpose(1, 2)
+
+            attn_output = xops.memory_efficient_attention(query_states, key_states, value_states, attn_bias = LowerTriangularFromBottomRightMask())
+            attn_output = attn_output.reshape((batch_size, q_len, cfg.num_attention_heads * cfg.head_dim))
+
         # Update 8-bit/Q4 cache
 
         if cache is not None:
             cache.store_kv_state(self.layer_idx, batch_size, past_len, q_len)
 
         # Output projection
```

## exllamav2/config.py

```diff
@@ -52,14 +52,15 @@
     max_attention_size: int                     # Sequences will be processed in chunks to keep the size of the attention weights matrix <= this
     max_output_len: int | None                  # Maximum number of output tokens per forward pass
 
     scale_pos_emb: float                        # Factor by which to scale positional embeddings, e.g. for 4096-token sequence use a scaling factor of 2.0, requires finetuned model or LoRA
     scale_alpha_value: float                    # Alpha value for NTK RoPE scaling. Similar to compress_pos_emb but works without finetuned model
 
     no_flash_attn: bool                         # Implementation will automatically use flash-attn-2 when available
+    no_xformers: bool                           # Implementation will automatically use xformers for sm<80 when available, unless flash-attn-2 is available
     fasttensors: bool                           # Experimental, Linux only
     load_in_q4: bool                            # Load float linear layers in Q4 format (for test/dev purposes, not performant)
 
     max_dq_size: int                            # Max number of elements to dequantize at once
 
     # Loaded/set by .prepare():
 
@@ -113,14 +114,15 @@
         self.scale_pos_emb = 1.0
         self.scale_alpha_value = 1.0
         self.scale_long_factor = None
         self.scale_short_factor = None
         self.alt_rope_method = None
 
         self.no_flash_attn = False
+        self.no_xformers = False
         self.fasttensors = False
         self.load_in_q4 = False
 
         if model_dir is not None:
             self.model_dir = model_dir
             self.prepare()
         else:
```

## exllamav2/model.py

```diff
@@ -30,15 +30,15 @@
 import math
 from exllamav2.config import ExLlamaV2Config
 from exllamav2.cache import ExLlamaV2CacheBase
 from exllamav2.linear import ExLlamaV2Linear
 from exllamav2.module import ExLlamaV2Module
 from exllamav2.rmsnorm import ExLlamaV2RMSNorm
 from exllamav2.layernorm import ExLlamaV2LayerNorm
-from exllamav2.attn import ExLlamaV2Attention, has_flash_attn
+from exllamav2.attn import ExLlamaV2Attention, has_flash_attn, has_xformers
 from exllamav2.lora import ExLlamaV2Lora
 from exllamav2.mlp import ExLlamaV2MLP
 from exllamav2.moe_mlp import ExLlamaV2MoEMLP
 from exllamav2.parallel_decoder import ExLlamaV2ParallelDecoder
 from exllamav2.embedding import ExLlamaV2Embedding
 from exllamav2.pos_embedding import ExLlamaV2PosEmbedding
 from exllamav2.compat import safe_move_tensor
@@ -760,15 +760,16 @@
 
             # Limit chunk_size to max_input_len
 
             chunk_size = min(remaining_q_len, effective_max_input_len)
 
             # Limit chunk_size to keep size of attention operation <= max_attention_size
 
-            if has_flash_attn and not self.config.no_flash_attn:
+            if (has_flash_attn and not self.config.no_flash_attn) \
+                or (has_xformers and not self.config.no_xformers):
 
                 # Can't measure increase in VRAM usage with longer k_len, assume usage is constant
                 # for given chunk_size
                 pass
 
             else:
```

## exllamav2/model_init.py

```diff
@@ -11,14 +11,15 @@
 
     parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory")
     parser.add_argument("-gs", "--gpu_split", type = str, help = "\"auto\", or VRAM allocation per GPU in GB")
     parser.add_argument("-l", "--length", type = int, help = "Maximum sequence length")
     parser.add_argument("-rs", "--rope_scale", type = float, help = "RoPE scaling factor")
     parser.add_argument("-ra", "--rope_alpha", type = float, help = "RoPE alpha value (NTK)")
     parser.add_argument("-nfa", "--no_flash_attn", action = "store_true", help = "Disable Flash Attention")
+    parser.add_argument("-nxf", "--no_xformers", action = "store_true", help = "Disable xformers, an alternative plan of flash attn for older devices")
     parser.add_argument("-lm", "--low_mem", action = "store_true", help = "Enable VRAM optimizations, potentially trading off speed")
     parser.add_argument("-ept", "--experts_per_token", type = int, help = "Override MoE model's default number of experts per token")
     parser.add_argument("-lq4", "--load_q4", action = "store_true", help = "Load weights in Q4 mode")
     if os.name != "nt":
         parser.add_argument("-fst", "--fast_safetensors", action = "store_true", help = "Optimized safetensors loading with direct I/O (experimental!)")
 
 
@@ -83,14 +84,15 @@
 
     # Set config options
 
     if args.length: config.max_seq_len = args.length
     if args.rope_scale: config.scale_pos_emb = args.rope_scale
     if args.rope_alpha: config.scale_alpha_value = args.rope_alpha
     config.no_flash_attn = args.no_flash_attn
+    config.no_xformers = args.no_xformers
     if args.experts_per_token: config.num_experts_per_token = args.experts_per_token
 
     if max_batch_size: config.max_batch_size = max_batch_size
     config.max_output_len = max_output_len
 
     # Set low-mem options
```

## exllamav2/version.py

```diff
@@ -1 +1 @@
-__version__ = "0.1.0"
+__version__ = "0.1.1"
```

## exllamav2/exllamav2_ext/ext_sampling.cpp

```diff
@@ -327,24 +327,66 @@
         for (int j = 0; j < m; ++j)
             *a_ptr++ += *b_ptr_++;
     }
 
     Py_END_ALLOW_THREADS
 }
 
-void fast_copy_cpu(torch::Tensor a, torch::Tensor b)
+void fast_copy_cpu(torch::Tensor dst, torch::Tensor src)
 {
-    size_t size_a = a.numel() * torch::elementSize(torch::typeMetaToScalarType(a.dtype()));
-    size_t size_b = b.numel() * torch::elementSize(torch::typeMetaToScalarType(b.dtype()));
-    TORCH_CHECK(size_a == size_b, "a and b are not the same size");
+    TORCH_CHECK(dst.sizes() == src.sizes(), "Tensors must have the same shape");
+    TORCH_CHECK(dst.dtype() == src.dtype(), "Tensors must have the same dtype");
+
+    auto dst_strides = dst.strides();
+    auto src_strides = src.strides();
+    auto sizes = dst.sizes();
 
     Py_BEGIN_ALLOW_THREADS
 
-    memcpy(a.data_ptr(), b.data_ptr(), size_a);
+    if (dst.is_contiguous() && src.is_contiguous())
+    {
+        std::memcpy(dst.data_ptr(), src.data_ptr(), src.numel() * src.element_size());
+    }
+    else
+    {
+        auto copy_recursive = [&](auto& self, int64_t dst_offset, int64_t src_offset, int dim) -> void
+        {
+            if (dim == sizes.size())
+            {
+                std::memcpy(static_cast<char*>(dst.data_ptr()) + dst_offset * dst.element_size(),
+                            static_cast<char*>(src.data_ptr()) + src_offset * src.element_size(),
+                            dst.element_size());
+                return;
+            }
+
+            for (int64_t i = 0; i < sizes[dim]; ++i)
+            {
+                self(self, dst_offset + i * dst_strides[dim],
+                     src_offset + i * src_strides[dim],
+                     dim + 1);
+            }
+        };
+
+        copy_recursive(copy_recursive, 0, 0, 0);
+    }
 
     Py_END_ALLOW_THREADS
 }
 
+
+//void fast_copy_cpu(torch::Tensor a, torch::Tensor b)
+//{
+//    size_t size_a = a.numel() * torch::elementSize(torch::typeMetaToScalarType(a.dtype()));
+//    size_t size_b = b.numel() * torch::elementSize(torch::typeMetaToScalarType(b.dtype()));
+//    TORCH_CHECK(size_a == size_b, "a and b are not the same size");
+//
+//    Py_BEGIN_ALLOW_THREADS
+//
+//    memcpy(a.data_ptr(), b.data_ptr(), size_a);
+//
+//    Py_END_ALLOW_THREADS
+//}
+
 void dump_profile_results()
 {
     profile_results();
 }
```

## exllamav2/exllamav2_ext/cuda/cache.cu

```diff
@@ -1,15 +1,16 @@
 #include "cache.cuh"
 
 #include "quant/qdq_util.cuh"
 #include "util.cuh"
 #include "compat.cuh"
 
 #define THREADS 32
-#define BLOCKSIZE_Q 256
+#define BLOCKSIZE_Q 512
+#define SUPER_BLOCKSIZE_Q (128 * 1024)
 #define THREADS_Q (BLOCKSIZE_Q / 2)
 #define HADAMARD_Q4
 
 // The upper 8 bits of FP16 are equivalent to FP8 E5M2.
 //
 // The range of values typically cached seem to be in the range of +/- 16, with an exponent component (with bias) up to
 // about 20. Empirically, the MSE over the whole range of observed values in the K/V cache works out the same for E4M3
@@ -110,22 +111,22 @@
     gridDim.x = DIVIDE((max - min) / 8, THREADS);
     gridDim.y = height;
 
     fp8_to_fp16_kernel<<<gridDim, blockDim>>>(pIn, pOut, stride, height, min, max);
     // cuda_check( cudaPeekAtLastError() );
 }
 
-// Q4
+// -------------- FP16 -> Q4
 
-__device__ void fp16_to_q4
+inline __device__ void fp16_to_q4
 (
     int t,
-    const half* in,
-    unsigned char* out,
-    half* scales,
+    const half* __restrict__ in,
+    unsigned char* __restrict__ out,
+    half* __restrict__ scales,
     int block_offset
 )
 {
     const half2* in2 = (const half2*) (in + block_offset);
     __shared__ uint32_t q_buffer[BLOCKSIZE_Q / 8];
     __shared__ half s_buffer[BLOCKSIZE_Q / 32];
 
@@ -185,80 +186,14 @@
     int4* out_q = (int4*) (out + block_offset / 2);
     int4* out_s = (int4*) (scales + block_offset / 32);
 
     if (t < BLOCKSIZE_Q / 32) out_q[t] = pq[t];
     if (t < BLOCKSIZE_Q / 256) out_s[t] = ps[t];
 }
 
-__device__ void q4_to_fp16
-(
-    int t,
-    const unsigned char* in,
-    const half* scales,
-    half* out,
-    int block_offset
-)
-{
-    __shared__ uint32_t q_buffer[BLOCKSIZE_Q / 8];
-    __shared__ half s_buffer[BLOCKSIZE_Q / 32];
-
-    // Fetch
-
-    int4* in_q = (int4*) (in + block_offset / 2);
-    int4* in_s = (int4*) (scales + block_offset / 32);
-    int4* pq = (int4*) q_buffer;
-    int4* ps = (int4*) s_buffer;
-
-    if (t < BLOCKSIZE_Q / 32) pq[t] = in_q[t];
-    if (t < BLOCKSIZE_Q / 256) ps[t] = in_s[t];
-    __syncthreads();
-
-    // Get scale
-
-    half scale = s_buffer[t / 16];
-    half2 scale2 = __half2half2(scale);
-
-    // Dequantize
-
-    int shift0 = (t % 4) * 8;
-    int shift1 = shift0 + 4;
-    uint32_t q = q_buffer[t / 4];
-    int q0 = ((int) ((q >> shift0) & 0x0f)) - 8;
-    int q1 = ((int) ((q >> shift1) & 0x0f)) - 8;
-
-    half w0 = __int2half_rn(q0);
-    half w1 = __int2half_rn(q1);
-    half2 w2 = __halves2half2(w0, w1);
-    w2 = __hmul2(w2, scale2);
-
-    // Perform hadamard transform on two interleaved 32-element groups. Skipped scaling when quantizing, so result
-    // is scaled by 1/32 here
-
-    #ifdef HADAMARD_Q4
-
-        for (int i = 1; i < 32; i <<= 1)
-        {
-            half2 pw2 = __shfl_xor_sync(0xffffffff, w2, i);
-            uint32_t* w2i = reinterpret_cast<uint32_t*>(&w2);
-            int32_t sfm = -static_cast<int32_t>(t & i) >> 31;
-            *w2i ^= (sfm & 0x80008000);
-            w2 = __hadd2(w2, pw2);
-        }
-        w2 = __hmul2(w2, __float2half2_rn(1.0f/32.0f));
-
-    #endif
-
-    // Store
-
-    half2* out2 = (half2*) (out + block_offset);
-    out2[t] = w2;
-}
-
-// -------------- FP16 -> Q4
-
 __global__ void fp16_to_q4_kv_paged_kernel
 (
     const half* __restrict__ k_in,
     unsigned char* __restrict__ k_out,
     half* __restrict__ k_scales,
     const half* __restrict__ v_in,
     unsigned char* __restrict__ v_out,
@@ -268,36 +203,38 @@
     int pages_per_seq,
     int page_size,
     int dim,
     int q_len
 )
 {
     int t = threadIdx.x;
-    const half* in = blockIdx.z ? v_in : k_in;
-    half* scales = blockIdx.z ? v_scales : k_scales;
-    unsigned char* out = blockIdx.z ? v_out : k_out;
+    int kv = blockIdx.z & 1;
+    const half* in = kv ? v_in : k_in;
+    half* scales = kv ? v_scales : k_scales;
+    unsigned char* out = kv ? v_out : k_out;
 
     int x = blockIdx.x;
-    int y = blockIdx.y;
+    int y = blockIdx.z >> 1;
 
     int page = block_table[pages_per_seq * y + x];
     int seqlen = cache_seqlens[y];
     int vx_a = page_size * x;
     int px_a = seqlen - vx_a;
     int px_b = px_a + q_len;
     px_a = max(px_a, 0);
     px_b = min(px_b, page_size);
 
     int block_a = (page * page_size + px_a) * dim;
     int block_b = (page * page_size + px_b) * dim;
 
-    for (int i = block_a; i < block_b; i += BLOCKSIZE_Q)
+    for (int i = block_a; i < block_b; i += SUPER_BLOCKSIZE_Q)
     {
-//        if (!t) DBGI2(y, i);
-        fp16_to_q4(t, in, out, scales, i);
+        int j = i + blockIdx.y * BLOCKSIZE_Q;
+        if (j >= block_b) continue;
+        fp16_to_q4(t, in, out, scales, j);
     }
 }
 
 __global__ void fp16_to_q4_kv_kernel
 (
     const half* __restrict__ k_in,
     unsigned char* __restrict__ k_out,
@@ -334,16 +271,16 @@
     int page_size,
     int q_len
 )
 {
     dim3 blockDim, gridDim;
     blockDim.x = THREADS_Q;
     gridDim.x = pages_per_seq;
-    gridDim.y = batch_size;
-    gridDim.z = v_in ? 2 : 1;
+    gridDim.y = SUPER_BLOCKSIZE_Q / BLOCKSIZE_Q;
+    gridDim.z = batch_size * 2;
 
     fp16_to_q4_kv_paged_kernel<<<gridDim, blockDim>>>
     (
         k_in,
         k_out,
         k_scales,
         v_in,
@@ -379,14 +316,84 @@
     gridDim.z = v_in ? 2 : 1;
 
     fp16_to_q4_kv_kernel<<<gridDim, blockDim>>>(k_in, k_out, k_scales, v_in, v_out, v_scales, offset, stride);
 }
 
 // --------------- Q4 -> FP16
 
+inline __device__ void q4_to_fp16
+(
+    int t,
+    const unsigned char* __restrict__ in,
+    const half* __restrict__ scales,
+    half* __restrict__ out,
+    int block_offset
+)
+{
+//    __shared__ uint32_t q_buffer[BLOCKSIZE_Q / 8];
+//    __shared__ half s_buffer[BLOCKSIZE_Q / 32];
+
+    // Fetch
+
+//    int4* in_q = (int4*) (in + block_offset / 2);
+//    int4* in_s = (int4*) (scales + block_offset / 32);
+//    int4* pq = (int4*) q_buffer;
+//    int4* ps = (int4*) s_buffer;
+//
+//    if (t < BLOCKSIZE_Q / 32) pq[t] = in_q[t];
+//    int t2 = t - BLOCKSIZE_Q / 32;
+//    if (t2 >= 0 && t2 < BLOCKSIZE_Q / 256) ps[t2] = in_s[t2];
+//    __syncthreads();
+
+    const uint32_t* in_q = (const uint32_t*) (in + block_offset / 2);
+    const half* in_s = (const half*) (scales + block_offset / 32);
+
+    // Get scale
+
+//    half scale = s_buffer[t / 16];
+    half scale = __ldg(in_s + t / 16);
+    half2 scale2 = __half2half2(scale);
+
+    // Dequantize
+
+    int shift0 = (t % 4) * 8;
+    int shift1 = shift0 + 4;
+//    uint32_t q = q_buffer[t / 4];
+    uint32_t q = __ldg(in_q + t / 4);
+    int q0 = ((int) ((q >> shift0) & 0x0f)) - 8;
+    int q1 = ((int) ((q >> shift1) & 0x0f)) - 8;
+
+    half w0 = __int2half_rn(q0);
+    half w1 = __int2half_rn(q1);
+    half2 w2 = __halves2half2(w0, w1);
+    w2 = __hmul2(w2, scale2);
+
+    // Perform hadamard transform on two interleaved 32-element groups. Skipped scaling when quantizing, so result
+    // is scaled by 1/32 here
+
+    #ifdef HADAMARD_Q4
+
+        for (int i = 1; i < 32; i <<= 1)
+        {
+            half2 pw2 = __shfl_xor_sync(0xffffffff, w2, i);
+            uint32_t* w2i = reinterpret_cast<uint32_t*>(&w2);
+            int32_t sfm = -static_cast<int32_t>(t & i) >> 31;
+            *w2i ^= (sfm & 0x80008000);
+            w2 = __hadd2(w2, pw2);
+        }
+        w2 = __hmul2(w2, __float2half2_rn(1.0f/32.0f));
+
+    #endif
+
+    // Store
+
+    half2* out2 = (half2*) (out + block_offset);
+    out2[t] = w2;
+}
+
 __global__ void q4_to_fp16_kv_paged_kernel
 (
     const unsigned char* __restrict__ k_in,
     const half* __restrict__ k_scales,
     half* __restrict__ k_out,
     const unsigned char* __restrict__ v_in,
     const half* __restrict__ v_scales,
@@ -395,32 +402,34 @@
     const int* __restrict__ block_table,
     int pages_per_seq,
     int page_size,
     int dim
 )
 {
     int t = threadIdx.x;
-    const unsigned char* in = blockIdx.z ? v_in : k_in;
-    const half* scales = blockIdx.z ? v_scales : k_scales;
-    half* out = blockIdx.z ? v_out : k_out;
+    int kv = blockIdx.z & 1;
+    const unsigned char* in = kv ? v_in : k_in;
+    const half* scales = kv ? v_scales : k_scales;
+    half* out = kv ? v_out : k_out;
 
     int x = blockIdx.x;
-    int y = blockIdx.y;
+    int y = blockIdx.z >> 1;
     int page = block_table[pages_per_seq * y + x];
     int seqlen = cache_seqlens[y];
     int vx_a = page_size * x;
     int vx_b = min(vx_a + page_size, seqlen);
     int vnum = max(vx_b - vx_a, 0);
     int block_a = (page * page_size) * dim;
     int block_b = (page * page_size + vnum) * dim;
 
-    for (int i = block_a; i < block_b; i += BLOCKSIZE_Q)
+    for (int i = block_a; i < block_b; i += SUPER_BLOCKSIZE_Q)
     {
-//        if (!t) DBGI2(y, i);
-        q4_to_fp16(t, in, scales, out, i);
+        int j = i + blockIdx.y * BLOCKSIZE_Q;
+        if (j >= block_b) continue;
+        q4_to_fp16(t, in, scales, out, j);
     }
 }
 
 __global__ void q4_to_fp16_kv_kernel
 (
     const unsigned char* __restrict__ k_in,
     const half* __restrict__ k_scales,
@@ -456,16 +465,16 @@
     const int* block_table,
     int page_size
 )
 {
     dim3 blockDim, gridDim;
     blockDim.x = THREADS_Q;
     gridDim.x = pages_per_seq;
-    gridDim.y = batch_size;
-    gridDim.z = v_in ? 2 : 1;
+    gridDim.y = SUPER_BLOCKSIZE_Q / BLOCKSIZE_Q;
+    gridDim.z = batch_size * 2;
 
     q4_to_fp16_kv_paged_kernel<<<gridDim, blockDim>>>
     (
         k_in,
         k_scales,
         k_out,
         v_in,
```

## exllamav2/exllamav2_ext/cuda/q_attn.cu

 * *Ordering differences only*

```diff
@@ -153,24 +153,24 @@
         norm_state = temp_state;
     }
 
     gemm_half_q_half_cuda(cublas_handle, norm_state, q_proj, temp_q, q_len * batch_size, q_proj->width, hidden_size, true, temp_dq);
     gemm_half_q_half_cuda(cublas_handle, norm_state, k_proj, temp_k, q_len * batch_size, k_proj->width, hidden_size, true, temp_dq);
     gemm_half_q_half_cuda(cublas_handle, norm_state, v_proj, temp_v, q_len * batch_size, v_proj->width, hidden_size, true, temp_dq);
 
+    apply_loras_cuda(cublas_handle, q_proj_lora, loras, q_proj, norm_state, temp_q, lora_temp, q_len * batch_size);
+    apply_loras_cuda(cublas_handle, k_proj_lora, loras, k_proj, norm_state, temp_k, lora_temp, q_len * batch_size);
+    apply_loras_cuda(cublas_handle, v_proj_lora, loras, v_proj, norm_state, temp_v, lora_temp, q_len * batch_size);
+
     if (q_norm)
         head_norm_cuda(temp_q, q_norm, NULL, temp_q, norm_epsilon, q_len * batch_size, num_heads, head_dim);
 
     if (k_norm)
         head_norm_cuda(temp_k, k_norm, NULL, temp_k, norm_epsilon, q_len * batch_size, num_kv_heads, head_dim);
 
-    apply_loras_cuda(cublas_handle, q_proj_lora, loras, q_proj, norm_state, temp_q, lora_temp, q_len * batch_size);
-    apply_loras_cuda(cublas_handle, k_proj_lora, loras, k_proj, norm_state, temp_k, lora_temp, q_len * batch_size);
-    apply_loras_cuda(cublas_handle, v_proj_lora, loras, v_proj, norm_state, temp_v, lora_temp, q_len * batch_size);
-
 //    rope_cuda(temp_q, sin, cos, batch_size, q_len * num_heads,    head_dim, num_heads,    past_len, past_lens);
 //    rope_cuda(temp_k, sin, cos, batch_size, q_len * num_kv_heads, head_dim, num_kv_heads, past_len, past_lens);
 
     if (rope_style != ROPE_STYLE_NONE)
     {
         rope_cuda_qk
         (
```

## exllamav2/generator/dynamic.py

```diff
@@ -292,14 +292,16 @@
             Enable paged mode, defaults to True. If this is False, the generator uses a fallback unpaged mode which
             does not require paged attention support, but in which the max supported batch size is 1. CFG also will
             not work in this mode.
 
         :param kwargs:
         """
 
+        # torch.set_num_threads(1)
+
         self.model = model
         self.cache = cache
         self.tokenizer = tokenizer
         cfg = self.model.config
         self.padded_vocab_size = ((cfg.vocab_size + 31) // 32) * 32
 
         self.draft_model = draft_model
@@ -1037,15 +1039,17 @@
             cache = self.cache,
             loras = self.current_loras,
         )
 
         # Pass logits to jobs for sampling
 
         batch_logits = self.logits_pinned[:device_logits.shape[0], :device_logits.shape[1], :]
-        batch_logits.copy_(device_logits)
+        batch_logits.copy_(device_logits, non_blocking = False)
+        # device_logits = device_logits.float().cpu()
+        # ext_c.fast_copy_cpu(batch_logits, device_logits)
         # torch.cuda.synchronize()
 
         if self.max_sampling_threads > 1 and len(self.active_jobs) >= self.min_sampling_threads:
             mt_sample = True
             futures = deque()
             for job, a, b in zip(self.active_jobs, logit_mapping[:-1], logit_mapping[1:]):
                 if a == b: continue
@@ -1197,14 +1201,15 @@
         input_ids: SeqTensor
         sequence_ids: SeqTensor
         kv_position: int
         page_hashes: list[bytes]
         new_unique_pages: int
         allocated_pages: list[CachePage] | None
         block_index_tensor: torch.Tensor | None
+        prefill_complete: bool
         live: bool
 
     sequences: list[Sequence]
     all_unique_hashes: list[bytes]
 
     max_new_tokens: int
     min_new_tokens: int
@@ -1385,14 +1390,15 @@
             seq.sequence_ids = SeqTensor.from_tensor(seq_ids, seq_dim = -1)
             seq.kv_position = 0
             seq.page_hashes = None
             seq.new_unique_pages = 0
             seq.allocated_pages = None
             seq.block_index_tensor = None
             seq.live = True
+            seq.prefill_complete = False
             self.sequences.append(seq)
 
         # Generation parameters
 
         self.max_new_tokens = max_new_tokens
         self.min_new_tokens = min_new_tokens
         self.new_tokens = 0 if self.prefix_token is None else -1
@@ -1538,15 +1544,16 @@
             self.sequences[0].sequence_ids.torch(),
             self.rng.random(),
             self.generator.tokenizer,
             self.prefix_token if self.new_tokens == -1 else None,
             self.return_top_tokens,
             blocked_tokens = blocked_tokens,
             filters = self.filters if self.new_tokens >= 0 else None,
-            filter_prefer_eos = self.filter_prefer_eos
+            filter_prefer_eos = self.filter_prefer_eos,
+            # sync = True
         )
 
         return next_token, next_k_tokens, next_k_probs, next_prob, filter_eos
 
 
     def receive_sample(
             self,
@@ -1900,14 +1907,16 @@
         page_size = self.generator.page_size
 
         if self.time_first_prefill is None:
             self.time_first_prefill = time.time()
 
         progress = 0
         for seq in self.sequences:
+            if seq.prefill_complete:
+                continue
 
             prefill_start = seq.kv_position
             prefill_end = seq.kv_position + self.generator.max_chunk_size
             if self.generator.paged:
                 prefill_end = (prefill_end // page_size) * page_size
             prefill_end = min(prefill_end, len(seq.sequence_ids) - 1)
 
@@ -2032,14 +2041,16 @@
                     pf_b = min(local_idx * page_size + page_size, prefill_end)
                     pfp_a = pf_a - local_idx * page_size
                     pfp_b = pf_b - local_idx * page_size
                     page.sequence[:, pfp_a:pfp_b].copy_(seq.sequence_ids.torch_slice(pf_a, pf_b))
                     page.can_revert = False
 
                 progress += prefill_end - prefill_start
+                if progress >= len(seq.sequence_ids) - 1:
+                    seq.prefill_complete = True
 
         if progress:
             r = {
                 "job": self,
                 "stage": "prefill",
                 "eos": False,
                 "curr_progress": sum(seq.kv_position for seq in self.sequences),
@@ -2143,14 +2154,15 @@
                 seq.allocated_pages.append(np)
 
             # Advance cache over prefilled pages
 
             for page in seq.allocated_pages:
                 if page.kv_position == page_size:
                     seq.kv_position += page_size
+                    self.cached_pages += 1
                 else:
                     break
 
         self.generator.validate_cache()
 
 
     def deallocate_pages(self):
```

## exllamav2/generator/sampler.py

```diff
@@ -3,14 +3,15 @@
 import torch
 import torch.nn.functional as F
 from exllamav2 import ExLlamaV2Tokenizer
 from exllamav2.generator.filters import ExLlamaV2Filter
 from exllamav2.generator.hooks import ExLlamaV2PostSamplingHook
 from exllamav2.ext import exllamav2_ext as ext_c, none_tensor
 from copy import copy
+# import line_profiler
 
 class ExLlamaV2Sampler:
 
     @dataclass
     class Settings:
 
         token_repetition_penalty: float = 1.025
@@ -87,25 +88,27 @@
                 padding = -tokenizer.config.vocab_size % 32
                 self.token_bias = torch.zeros((tokenizer.config.vocab_size + padding,), dtype = torch.float)
 
             self.token_bias[tokens] = float("-inf")
 
 
     @staticmethod
+    # @profile
     def sample(
         logits: torch.tensor,
         settings: Settings,
         sequence_ids: torch.tensor,
         random: float,
         tokenizer: ExLlamaV2Tokenizer,
         prefix_token: torch.Tensor | None = None,
         return_top_tokens: int = 0,
         blocked_tokens: list[int] | None = None,
         filters: list[ExLlamaV2Filter] | None = None,
-        filter_prefer_eos: bool = False
+        filter_prefer_eos: bool = False,
+        sync: bool = False
     ):
 
         """
         Sample tokens from (batched) logits tensor
 
         :param logits:
             Input logits, float tensor of shape (batch_size, 1, vocab_size)
@@ -135,14 +138,17 @@
         :param filters:
             List of ExLlamaV2Filters. Sampling will be constrained to the intersection of allowed tokens for all
             filters.
 
         :param filter_prefer_eos:
             If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters
 
+        :param sync:
+            Synchronize CUDA right before using the logits
+
         :return:
             Tuple of:
             - Sampled tokens, tensor of shape (batch_size, 1)
             - Top candidates per token (batch_size, 1, return_top_tokens), or meta tensor if return_top_tokens = 0
             - Top probs per token (batch_size, 1, return_top_tokens), or meta tensor if return_top_tokens = 0
             - Probabilities per token, shape (batch_size, 1)
             - True if the current filter has reached a stop condition
@@ -158,27 +164,32 @@
         if settings.cfg_scale is not None:
             assert batch_size == 2, "CFG requires logits to be bsz 2"
         else:
             assert batch_size == 1 or len(filters) == 0, "Filters not implemented for batch size > 1"
 
         logits = logits.squeeze(1)
 
+        # Prepare filter
+
+        logit_filter = torch.empty((batch_size, vocab_size), dtype = torch.bool)
+        ext_c.fast_fill_cpu_ones_bool(logit_filter)
+
+        # Sync
+
+        if sync:
+            torch.cuda.synchronize()
+
         # CFG
 
         if settings.cfg_scale is not None:
             logits = F.log_softmax(logits, dim = -1)
             logits = settings.cfg_scale * logits[0] + (1 - settings.cfg_scale) * logits[1]
             logits = logits.unsqueeze(0)
             batch_size = 1
 
-        # Prepare filter
-
-        logit_filter = torch.empty((batch_size, vocab_size), dtype = torch.bool)
-        ext_c.fast_fill_cpu_ones_bool(logit_filter)
-
         # Repetition penalty
 
         if settings.token_repetition_penalty != 1.0 or \
             settings.token_frequency_penalty != 0.0 or \
             settings.token_presence_penalty != 0.0:
 
             ext_c.apply_rep_penalty(sequence_ids[:, :],
```

## Comparing `exllamav2-0.1.0.dist-info/LICENSE` & `exllamav2-0.1.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `exllamav2-0.1.0.dist-info/RECORD` & `exllamav2-0.1.1.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 exllamav2/__init__.py,sha256=DMqLKtqqEDBywO-Coct6oOI3dKZ2h5sXBgV_5QmNPfA,482
 exllamav2/architecture.py,sha256=Tppq2SDuNazhJAt8ZYLjLNPT7kLRqsDMDgXaeeFvhAY,23883
-exllamav2/attn.py,sha256=UgKgL8b4T94j10Dqp2FI9taZI8DS1u48k4bNyXvuyR4,34762
+exllamav2/attn.py,sha256=Ep_tCyZ-BUEXWPlN2vUJAIvhe7ibDZd_qBzc_tPMIYw,39138
 exllamav2/cache.py,sha256=qrv2e2LKrJs7HrkDOzV59BKqzPiflYtPz0gUkICrEX0,16389
 exllamav2/compat.py,sha256=6uS0zlyU2vP7Lm9fKuKFRB2Tnoz2bH0otQjYSFSs0j4,2001
-exllamav2/config.py,sha256=cJsK0qSkYjQIgIelCMgCXuKPfFdEgDnHDxL7O3iYjCQ,12385
+exllamav2/config.py,sha256=F_LgDI1xcRRr9HYGNXM009pxGTpc3vuoP918jj255o4,12574
 exllamav2/embedding.py,sha256=jP7-FxDSl5m3qR_XG2Pb90d9h1hlEe8kj6di2axnQXM,4545
 exllamav2/ext.py,sha256=T9PQnqgkDli8bPVKZJb5lvx4_-P4e0Wn8qOZ4xdFcCQ,12302
 exllamav2/fasttensors.py,sha256=48uGS8ZvRINWDZKBGY9CyvBXtAIGvig8aqhiG0NCzPE,6399
 exllamav2/headnorm.py,sha256=MujxZqqBOc6Nh-1supwtbTZAp0gJqZsV3CXVtIRKGsg,3969
 exllamav2/layernorm.py,sha256=FWZLmRp_HmrpsabTGtkMvTB8K3tzYVyrvxVRRpsbxR0,3502
 exllamav2/linear.py,sha256=Lko2aQ06axN7NB_NDERoBo9sxwLgAxh44ZjU53qW8cQ,11159
 exllamav2/lora.py,sha256=0diPvhoQZtu04XgmkKVnybWiJcvJwboNAd534jLnytE,6408
 exllamav2/mlp.py,sha256=8OIowovzn0DgSpkheBi4_0DGVDd9Reu8oImxxwengwI,12405
-exllamav2/model.py,sha256=faMnwaHwsAGJ6JtB0w8pGNM3xT-xeS4yVxIjsO0qbw8,31792
-exllamav2/model_init.py,sha256=FA4_dE1Q9BVk1O7KDt0QbRg7FXdmd79jBM4sBX2Iwx8,4737
+exllamav2/model.py,sha256=HSM_ktlQHFYLzmGfJ2TAgXLkXajziH22QPA8IdI0AkU,31876
+exllamav2/model_init.py,sha256=8TnYA9J9CubITMGFInBDGLYouY9S2zHTMgxHm_ORIXg,4931
 exllamav2/module.py,sha256=xJ-BcES9OVXHv56BEy4VVRxyr7NzqXWV1S2FfPXqh3o,7100
 exllamav2/moe_mlp.py,sha256=MbBmpKSqxrx_J4iRuwQ5tY9W7BZYV9UCFOs7lxkLs7E,14288
 exllamav2/parallel_decoder.py,sha256=kuUEUnDTITez_S2z3RRofg_DeZT75VhbLhoQvEY1G48,5262
 exllamav2/pos_embedding.py,sha256=FOIr10SIBgzbTDeLXmNTJ9PTfYFDemYHT1gOzAmjiBs,3172
 exllamav2/rmsnorm.py,sha256=AnHr1_NHYSgMZqZEq3kV7HCM4sf981rek1Ok1xF8zEY,3646
 exllamav2/util.py,sha256=Nxs_As0K7toR8cNuZeicxfhmdJ9PbWjxEaaRaud-L7E,6187
-exllamav2/version.py,sha256=Pru0BlFBASFCFo7McHdohtKkUtgMPDwbGfyUZlE2_Vw,21
+exllamav2/version.py,sha256=8oAxKUG747GUokmxjkrWejyJa5yPNEsoJDlXxoedxTw,21
 exllamav2/exllamav2_ext/config.h,sha256=EDy0nMntf9sCDb37o8Hz156uxGmDdmIersSaOyj_BnE,337
 exllamav2/exllamav2_ext/ext_bindings.cpp,sha256=NN2asv7tDnlbnUsgEpLA1W9VQh2yU--wL1q1HtAwNnY,4378
 exllamav2/exllamav2_ext/ext_cache.cpp,sha256=q39bqJCgW9WoGn4tOPmbT0LQBKgQlCzof1g-cLkgpvs,8037
 exllamav2/exllamav2_ext/ext_cache.h,sha256=PRXrgDyAIyjvnkOT0jdGBypgjXHr7cYHoRFMBl3qZ3M,1105
 exllamav2/exllamav2_ext/ext_gemm.cpp,sha256=YAM7OXhVugvhI7U4YUZtGxTLoDjzhpaqKA6lff-RidA,1455
 exllamav2/exllamav2_ext/ext_gemm.h,sha256=ttylcKe8XcitDFhd-6FgRyck9rPVYzy9xt5GHXggfuA,161
 exllamav2/exllamav2_ext/ext_hadamard.cpp,sha256=wUgHLHANiHRNy_5hXddvZ1gfYcbu1apj2rjWAZo2_x4,2505
@@ -39,15 +39,15 @@
 exllamav2/exllamav2_ext/ext_qmlp.h,sha256=1lo5MQjMq0NYY6SibvND7qD4NTb3dR-zwpEcbP89OLg,2274
 exllamav2/exllamav2_ext/ext_quant.cpp,sha256=I9chP7pWNE1-6M-YjRx7C1DjvAY540lTKyuxSjMCMcU,5533
 exllamav2/exllamav2_ext/ext_quant.h,sha256=qRPzLbnS7h4dOm2OjGSrXQI7-tCJr7YZIHhkqLRQQX0,895
 exllamav2/exllamav2_ext/ext_rope.cpp,sha256=eJ25NuXpgVapd52t-7atVMc7Quf8VIzFl3E6lZaJMTU,1339
 exllamav2/exllamav2_ext/ext_rope.h,sha256=-AqqKgRX39ewzwH7AveM-IRaRFajxw--TbkQz39kAW0,186
 exllamav2/exllamav2_ext/ext_safetensors.cpp,sha256=PjNjpfj0xx7SoKCa3ImQTqfh_DzEemTsEg0CUJ7Ree8,344
 exllamav2/exllamav2_ext/ext_safetensors.h,sha256=BKSzjcE6xnnPwo5rRhCMQv18qzVhpqZIVrxFmUkI-4c,29
-exllamav2/exllamav2_ext/ext_sampling.cpp,sha256=shtmNpSlQFbyANWb2dIswTZFJa4g_K5XTh6rVOMphPo,9654
+exllamav2/exllamav2_ext/ext_sampling.cpp,sha256=8WtGjhmzL5Se7i1782FuMdHXi8nVZ5oyUrvUdptU3Uo,10976
 exllamav2/exllamav2_ext/ext_sampling.h,sha256=OjQgOl5uVjhNR5lkZm951_T6r0mdbNSdWjOZXKX6lEs,1379
 exllamav2/exllamav2_ext/cpp/avx2_target.h,sha256=EoPC3-FO1Q08LCZKwGLQVfAeZ2WRAVlijHbkN8s_OGA,1275
 exllamav2/exllamav2_ext/cpp/avx_mathfun.h,sha256=c_UOsXO0A0lZ4rudKAH1K-7Z_q3vMuzds8ool5LEiFM,24414
 exllamav2/exllamav2_ext/cpp/generator.cpp,sha256=WLfsqasiP06AA79PQ6-XmYSjv5ypPWfNVHwPce6mzSw,1299
 exllamav2/exllamav2_ext/cpp/generator.h,sha256=VSqCjJRcqMtkSnuGc-1jZCNeEHpGcuwXiXNl24p98Hg,274
 exllamav2/exllamav2_ext/cpp/profiling.cpp,sha256=OTfVrKKc6ihxogBPtxg5czpGOmi6WOxRJky91bmLWO0,1627
 exllamav2/exllamav2_ext/cpp/profiling.h,sha256=UpXs6r43A_EAAhYJxsPrn92lNYPXY2S3Xc6ve5iSUv8,362
@@ -56,15 +56,15 @@
 exllamav2/exllamav2_ext/cpp/safetensors.cpp,sha256=wJkfd7_Su4jKUhNjBH2ZOKznmrGsgy_05X_8LxosqZ0,8308
 exllamav2/exllamav2_ext/cpp/safetensors.h,sha256=jl2f5bcdt5GySYCuAAlbowquck68tlMnSZ1HU2dH9Gg,801
 exllamav2/exllamav2_ext/cpp/sampling.cpp,sha256=e6aXc_JyMorrPv9bYQw3Y0x-VuQCE-FChkGabsYy5_s,20446
 exllamav2/exllamav2_ext/cpp/sampling.h,sha256=Nh1FZ-l2HWe5f2a-03IpJjmaEMcP6FKdVfvQrDCIOqc,2283
 exllamav2/exllamav2_ext/cpp/sampling_avx2.cpp,sha256=ArDeIEjdG3wf6syC8RHtq2CQ5xnZxelPbxFcm1mydI4,3089
 exllamav2/exllamav2_ext/cpp/sampling_avx2.h,sha256=8bZHIcrY7jC0ca2EanR6va46fiAGIT98JKdsUY1QsW4,334
 exllamav2/exllamav2_ext/cpp/util.h,sha256=M7CBgwNltaa7H_dKN0WpVbE1jtnbxDQPg9M3W908av0,2204
-exllamav2/exllamav2_ext/cuda/cache.cu,sha256=3xEh7VKrYjUkYKckJTCmGKFC9BgD0CJrpy9rh1uAh5Y,12970
+exllamav2/exllamav2_ext/cuda/cache.cu,sha256=ozgfDUaDRUusRLTR0K060tP76bhUPUm5KNsnvpWDC8A,13564
 exllamav2/exllamav2_ext/cuda/cache.cuh,sha256=YSq6j6AeLbpVcggkcjhacq-DrWlKYkix3HFN3WO6YOY,1658
 exllamav2/exllamav2_ext/cuda/compat.cuh,sha256=zua49pQI0UnYKF3wO7POzz7L29PD6o55DM0awWSK-oY,2831
 exllamav2/exllamav2_ext/cuda/h_add.cu,sha256=XXS6F2abr9oQUBYbAKWG5ddE2XsQlCSZ1je7_AKEyic,2604
 exllamav2/exllamav2_ext/cuda/h_add.cuh,sha256=5-6EliRD6dBScpmlO-DrrP7Wm3Xk5Bm4EEs_a6-iUcw,265
 exllamav2/exllamav2_ext/cuda/h_gemm.cu,sha256=zamYBBUKcdyUzlp9V8RtbFyb-pOhpfB-lECaX5Z8TzU,7202
 exllamav2/exllamav2_ext/cuda/h_gemm.cuh,sha256=-nlLG5p7dfyUltraxSv6BL6_mrATaolbMHjj0fU5Chc,667
 exllamav2/exllamav2_ext/cuda/head_norm.cu,sha256=hQ46TxdScwtm6dwlwLHpXzpfpmwXHYQftoRkmS_zK58,3160
@@ -72,33 +72,31 @@
 exllamav2/exllamav2_ext/cuda/layer_norm.cu,sha256=-qUYrvblZeIYma0AmeK3cdwBxzYitGB3f3FlrQCeGt4,4970
 exllamav2/exllamav2_ext/cuda/layer_norm.cuh,sha256=Jdx5aQTltyTf4amX-Uzya5hNqyEUHT9E3yg9LOcpElI,302
 exllamav2/exllamav2_ext/cuda/lora.cu,sha256=cJJR32u8mzfBqv2SJ-LxDQaUuacvPIrQ7M0BCsxRo-U,935
 exllamav2/exllamav2_ext/cuda/lora.cuh,sha256=g4OOTYnDUFrgtBvF5TVUVZvx8FGnpHN46BH0F1IhPoQ,463
 exllamav2/exllamav2_ext/cuda/matrix_view.cuh,sha256=80YkwkT_BUc4Tocn-gj8EZX4YYf1TvACWX9IB7E38Cc,4293
 exllamav2/exllamav2_ext/cuda/pack_tensor.cu,sha256=wHmL9YEiumozrGP5At5mJCa0Hb3jM8hTdtFQEJo6dM8,7287
 exllamav2/exllamav2_ext/cuda/pack_tensor.cuh,sha256=_nTRIj9DIcEO-DtZKq8DzzvuAEXRiXz4u4qAR46v7oQ,549
-exllamav2/exllamav2_ext/cuda/q_attn.cu,sha256=2ICcrKRKpAC3Zt2FvwgB_v7HYGswU6qFjW3yghukwHo,6642
+exllamav2/exllamav2_ext/cuda/q_attn.cu,sha256=WRrBzmUrfXOFhP94eYKncNeNnJh-Mkb-xBjUTzgU6nc,6642
 exllamav2/exllamav2_ext/cuda/q_attn.cuh,sha256=dS8kNyqA38lu0CJ7D1CvCR-srUSJj_p_hr25LG4umpI,2392
 exllamav2/exllamav2_ext/cuda/q_gemm.cu,sha256=SJatE9RJk_KbpJTxUehH0ahGMt_EKYUPR-qOB-Dblb8,8809
 exllamav2/exllamav2_ext/cuda/q_gemm.cuh,sha256=nDoPy-Ar6YLGi0oBHO3EsvT2qHMhq308mxntk-_ObQw,614
 exllamav2/exllamav2_ext/cuda/q_gemm_autotune.cuh,sha256=ZFjiYdpKdulTPsQFF2hK0C3SXMQtRj0fZ8gKMZ56zbE,1705
 exllamav2/exllamav2_ext/cuda/q_gemm_kernel.cuh,sha256=q3qW5FrzuhdMBo8CBYWq7Bc0iHx69HKB524qFn1zD6E,19672
 exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq.cuh,sha256=6ef79hp_33pTsWX9DrKp-nFx7w2buQGmvflYY4ZybvQ,7516
-exllamav2/exllamav2_ext/cuda/q_gemm_kernel_gptq_old.cuh,sha256=6ef79hp_33pTsWX9DrKp-nFx7w2buQGmvflYY4ZybvQ,7516
 exllamav2/exllamav2_ext/cuda/q_matrix.cu,sha256=Zwu0-WzaLruIbsC0n3dzg5KQIvMr3o0yVEuN8dtt_VI,21853
 exllamav2/exllamav2_ext/cuda/q_matrix.cuh,sha256=01lgKsWjMgbeEK0k8Pcx8Ry6VQrk-EV5U1QPVID2CeQ,1910
 exllamav2/exllamav2_ext/cuda/q_mlp.cu,sha256=kMKpCtWAaLvwaXRZ6vTbOaMEjKbMrZihb-NrQx6OUZo,8195
 exllamav2/exllamav2_ext/cuda/q_mlp.cuh,sha256=2VZe1bdaZHtHo5sSE8oaEM-A2l3lSPUrApmOcOjghjo,2924
 exllamav2/exllamav2_ext/cuda/q_mlp_activation.cuh,sha256=FKzgQL0EzE7Y8FKP0J0-HsNVsM7w3BNk5ipb0qB2e0I,5762
 exllamav2/exllamav2_ext/cuda/q_mlp_softmax.cuh,sha256=7P4Ed9-1HDB7fkDrHA4XxiXD_qNj6J9K279X4T47Eas,7296
 exllamav2/exllamav2_ext/cuda/quantize.cu,sha256=0fvnVhD8tPH-hSH7HdEcQqzrpfauem1BcN9h9A6q8gI,9975
 exllamav2/exllamav2_ext/cuda/quantize.cuh,sha256=IcuAmGnqAxVCdEMx6y_29AFIK32ILJB6hKwD89AO-Ig,1303
 exllamav2/exllamav2_ext/cuda/rms_norm.cu,sha256=xVbcwuJBPAXWO2sl1aA2vkcDAl_cbG1h9LvonFJIN6U,3834
 exllamav2/exllamav2_ext/cuda/rms_norm.cuh,sha256=DWLKIRFV-IBm4qv7dRPsgBKsWjY79aVdPdrRYMBtV7M,277
-exllamav2/exllamav2_ext/cuda/rms_norm_.cu,sha256=H8e2r9fP50M7rayrjHMt2l9fy63BeM5VpO9NgQdRVnM,5085
 exllamav2/exllamav2_ext/cuda/rope.cu,sha256=OjTEK5V7IIKYDsJJp1bDZAQLlGxMDYy08L_zyso-tk4,7215
 exllamav2/exllamav2_ext/cuda/rope.cuh,sha256=TY4eRuW4deBoZGE2ch5dScup-rohBwecRx2w9KlbPSU,736
 exllamav2/exllamav2_ext/cuda/util.cu,sha256=tmZMQTx8lqbDoKIrtM4JIsnXS_3TYETpRui9g0OGZwg,624
 exllamav2/exllamav2_ext/cuda/util.cuh,sha256=Y_Tq6rSujFSMmlgt48BXwj9OzcnNrPzj3tneOFc1iVY,3345
 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cu,sha256=z4a9-5OS3HxxRbhjtW3iJuYb7gcj7nGpKGlOvT8t1Wc,1313
 exllamav2/exllamav2_ext/cuda/comp_units/kernel_select.cuh,sha256=4qUEwDAtNJxahck_AAAz1eBwQqgOuDDiYFKlbDhzGHw,8080
 exllamav2/exllamav2_ext/cuda/comp_units/unit_exl2_1a.cu,sha256=54UiJMwsoyypM9r4ulTAehJqaTdWzD-vWPzl_Y0jKgk,694
@@ -115,19 +113,19 @@
 exllamav2/exllamav2_ext/cuda/quant/qdq_4.cuh,sha256=2TP9bFuf6ZN1eWA398JgdLipnUfDhZOoo7d1WkJIpaM,5755
 exllamav2/exllamav2_ext/cuda/quant/qdq_5.cuh,sha256=lc4Hjp-vTdbOw02TlDy9YmkAgd8YRTKwhUtu4anUewI,7342
 exllamav2/exllamav2_ext/cuda/quant/qdq_6.cuh,sha256=jX7EYGh2EELTteVaqWkhQ6O5nvWIHpbRyw0hwpAc4no,4530
 exllamav2/exllamav2_ext/cuda/quant/qdq_8.cuh,sha256=YvJHf4kOHMOoXbI-hoRZt-Q0I36P7n7FCsKn--4dWQE,643
 exllamav2/exllamav2_ext/cuda/quant/qdq_util.cuh,sha256=_U-x5oEmtKL5ulsdcg2-CsZsYiCISH2hRD-qgLPYtB4,1367
 exllamav2/generator/__init__.py,sha256=0fXrRbpo5aYJB3oTY_ORJNev03nqynVuGeI2kfZsFMQ,513
 exllamav2/generator/base.py,sha256=OQCbX28f4ZB-sW1E_asL2hARpAq4uW8wfYlaf_jFqSQ,12853
-exllamav2/generator/dynamic.py,sha256=dETTLXTIrvaKbP6ZApvJpOJaDvCGpwnwoO87MhAxOxw,80386
+exllamav2/generator/dynamic.py,sha256=pSEzS2OLJfnXZWTxJWhTq-KY-swwNqwjU1Qa0WRk-NA,80867
 exllamav2/generator/dynamic_async.py,sha256=qXAlWhbMvgHt5CnE3wUKt2HkPJr2jQE0ccKSoJQDs6A,2687
 exllamav2/generator/hooks.py,sha256=oxkTKys3iUtpQby97fb8ktlwm1LBPT6wz9GAGEF4yhU,497
 exllamav2/generator/ngram.py,sha256=jp94y816c3d0Ac4RuGQmNhfOOaaf46uZSA_OZccsPFA,2370
-exllamav2/generator/sampler.py,sha256=nl8xd7vVYUtbiMxZtw6lxTMU37OrzhMBMu7DVaqmsMk,10162
+exllamav2/generator/sampler.py,sha256=rJuFIyaQnh6hwEvJscucWCOsCiPdejKXFY4h5zFg3Xs,10380
 exllamav2/generator/streaming.py,sha256=4t86BBBE86TBlTmE_Ii2dfxbZx_TX3J9lbrS1AhIS5M,39172
 exllamav2/generator/filters/__init__.py,sha256=O-_cLgKh9gN-n8wboZbOmwbmruU6jRolz7wNg85CUBY,242
 exllamav2/generator/filters/base.py,sha256=eBFcaYvBlHThqqClRZaKyl0tLwXrrSrXyj-kggz1m84,756
 exllamav2/generator/filters/prefix.py,sha256=5hQX7fxAtAniZEGYolYUB7m1iI8Nh1OMMop81IA9Jbg,1866
 exllamav2/generator/filters/select.py,sha256=XLhU1MZA1_PLbJj-uupIsPH_NJ_JhM93rgcUBMUGWhI,3965
 exllamav2/hadamard/hadamard.py,sha256=ZBNmxT5951EnrSjh27590tkHdIoLqq7Q4F_qwIbbPbc,4346
 exllamav2/hadamard/hadamard_1.txt,sha256=oxjCQhbe_iBv7rc-9b4AAz-pxKdNC5Z_ZTKibKWQbTs,1
@@ -146,12 +144,12 @@
 exllamav2/server/websocket.py,sha256=v3dxGQr6Hhtt5UhNd2GuSQzAhicwWGO61uYA9Q9854Q,1623
 exllamav2/server/websocket_actions.py,sha256=gcYNpfmnMvFYeQQwxf6G67TtcPMJfV4GZHzDJm5kqhA,10260
 exllamav2/tokenizer/__init__.py,sha256=1-nke-DAloiKZbcuiVqkJGB0hiURq3V184pXza-1ep4,217
 exllamav2/tokenizer/base.py,sha256=63BtYpnPK8NpD4QKSRTcV5eVDXlb06_N6rraxh5CrbM,2157
 exllamav2/tokenizer/hf.py,sha256=HaWxaxzpzKRzKENCGyIwNpkvkSAEZ7Q29mHNssouHm4,3028
 exllamav2/tokenizer/spm.py,sha256=WkmFxy7--Ex-h2Xs5NaV5QnkuMSebQN7XYOu7xb4EmE,1972
 exllamav2/tokenizer/tokenizer.py,sha256=pvM8k90d1gFnWX9roMbTcKbE8LjxG-5_JetRZUTWKew,23709
-exllamav2-0.1.0.dist-info/LICENSE,sha256=GEfg4GmBQu1DR8FEGp-oHI-93USx2LvNXjZH-ZF1nX8,1035
-exllamav2-0.1.0.dist-info/METADATA,sha256=4UiyX9Xn0X7-cuIw65vDuOpzZ-akuJjG4P854okREAk,427
-exllamav2-0.1.0.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-exllamav2-0.1.0.dist-info/top_level.txt,sha256=zHcNv9GI63f6FSZ1gBE6rZUoFLz6jTBuX2RjL8_6H0g,10
-exllamav2-0.1.0.dist-info/RECORD,,
+exllamav2-0.1.1.dist-info/LICENSE,sha256=GEfg4GmBQu1DR8FEGp-oHI-93USx2LvNXjZH-ZF1nX8,1035
+exllamav2-0.1.1.dist-info/METADATA,sha256=DpUwDcaXXy080blEBl_Ad3iGFrBbrAazdLW5x1vY2EQ,421
+exllamav2-0.1.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+exllamav2-0.1.1.dist-info/top_level.txt,sha256=zHcNv9GI63f6FSZ1gBE6rZUoFLz6jTBuX2RjL8_6H0g,10
+exllamav2-0.1.1.dist-info/RECORD,,
```

