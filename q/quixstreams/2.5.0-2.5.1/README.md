# Comparing `tmp/quixstreams-2.5.0-py3-none-any.whl.zip` & `tmp/quixstreams-2.5.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,90 +1,90 @@
-Zip file size: 130226 bytes, number of entries: 88
--rw-r--r--  2.0 unx      163 b- defN 24-May-16 05:51 quixstreams/__init__.py
--rw-r--r--  2.0 unx    36819 b- defN 24-May-16 05:51 quixstreams/app.py
--rw-r--r--  2.0 unx     2559 b- defN 24-May-16 05:51 quixstreams/context.py
--rw-r--r--  2.0 unx     1561 b- defN 24-May-16 05:51 quixstreams/error_callbacks.py
--rw-r--r--  2.0 unx     1409 b- defN 24-May-16 05:51 quixstreams/logging.py
--rw-r--r--  2.0 unx     2512 b- defN 24-May-16 05:51 quixstreams/processing_context.py
--rw-r--r--  2.0 unx     6262 b- defN 24-May-16 05:51 quixstreams/rowconsumer.py
--rw-r--r--  2.0 unx     5610 b- defN 24-May-16 05:51 quixstreams/rowproducer.py
--rw-r--r--  2.0 unx      112 b- defN 24-May-16 05:51 quixstreams/types.py
--rw-r--r--  2.0 unx      116 b- defN 24-May-16 05:51 quixstreams/checkpointing/__init__.py
--rw-r--r--  2.0 unx     6080 b- defN 24-May-16 05:51 quixstreams/checkpointing/checkpoint.py
--rw-r--r--  2.0 unx       97 b- defN 24-May-16 05:51 quixstreams/checkpointing/exceptions.py
--rw-r--r--  2.0 unx        0 b- defN 24-May-16 05:51 quixstreams/core/__init__.py
--rw-r--r--  2.0 unx       47 b- defN 24-May-16 05:51 quixstreams/core/stream/__init__.py
--rw-r--r--  2.0 unx     7678 b- defN 24-May-16 05:51 quixstreams/core/stream/functions.py
--rw-r--r--  2.0 unx     8070 b- defN 24-May-16 05:51 quixstreams/core/stream/stream.py
--rw-r--r--  2.0 unx       90 b- defN 24-May-16 05:51 quixstreams/dataframe/__init__.py
--rw-r--r--  2.0 unx      455 b- defN 24-May-16 05:51 quixstreams/dataframe/base.py
--rw-r--r--  2.0 unx    26879 b- defN 24-May-16 05:51 quixstreams/dataframe/dataframe.py
--rw-r--r--  2.0 unx      370 b- defN 24-May-16 05:51 quixstreams/dataframe/exceptions.py
--rw-r--r--  2.0 unx    16317 b- defN 24-May-16 05:51 quixstreams/dataframe/series.py
--rw-r--r--  2.0 unx      754 b- defN 24-May-16 05:51 quixstreams/dataframe/utils.py
--rw-r--r--  2.0 unx      106 b- defN 24-May-16 05:51 quixstreams/dataframe/windows/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-May-16 05:51 quixstreams/dataframe/windows/base.py
--rw-r--r--  2.0 unx    10516 b- defN 24-May-16 05:51 quixstreams/dataframe/windows/definitions.py
--rw-r--r--  2.0 unx     7163 b- defN 24-May-16 05:51 quixstreams/dataframe/windows/time_based.py
--rw-r--r--  2.0 unx       46 b- defN 24-May-16 05:51 quixstreams/exceptions/__init__.py
--rw-r--r--  2.0 unx      322 b- defN 24-May-16 05:51 quixstreams/exceptions/assignment.py
--rw-r--r--  2.0 unx       41 b- defN 24-May-16 05:51 quixstreams/exceptions/base.py
--rw-r--r--  2.0 unx       48 b- defN 24-May-16 05:51 quixstreams/kafka/__init__.py
--rw-r--r--  2.0 unx    21948 b- defN 24-May-16 05:51 quixstreams/kafka/consumer.py
--rw-r--r--  2.0 unx      695 b- defN 24-May-16 05:51 quixstreams/kafka/exceptions.py
--rw-r--r--  2.0 unx     6019 b- defN 24-May-16 05:51 quixstreams/kafka/producer.py
--rw-r--r--  2.0 unx      146 b- defN 24-May-16 05:51 quixstreams/models/__init__.py
--rw-r--r--  2.0 unx     1949 b- defN 24-May-16 05:51 quixstreams/models/messagecontext.py
--rw-r--r--  2.0 unx      485 b- defN 24-May-16 05:51 quixstreams/models/messages.py
--rw-r--r--  2.0 unx     2229 b- defN 24-May-16 05:51 quixstreams/models/rows.py
--rw-r--r--  2.0 unx     1813 b- defN 24-May-16 05:51 quixstreams/models/timestamps.py
--rw-r--r--  2.0 unx     1309 b- defN 24-May-16 05:51 quixstreams/models/types.py
--rw-r--r--  2.0 unx     1016 b- defN 24-May-16 05:51 quixstreams/models/serializers/__init__.py
--rw-r--r--  2.0 unx     3157 b- defN 24-May-16 05:51 quixstreams/models/serializers/base.py
--rw-r--r--  2.0 unx     1370 b- defN 24-May-16 05:51 quixstreams/models/serializers/exceptions.py
--rw-r--r--  2.0 unx     1936 b- defN 24-May-16 05:51 quixstreams/models/serializers/json.py
--rw-r--r--  2.0 unx    16878 b- defN 24-May-16 05:51 quixstreams/models/serializers/quix.py
--rw-r--r--  2.0 unx     4516 b- defN 24-May-16 05:51 quixstreams/models/serializers/simple_types.py
--rw-r--r--  2.0 unx       65 b- defN 24-May-16 05:51 quixstreams/models/topics/__init__.py
--rw-r--r--  2.0 unx     6566 b- defN 24-May-16 05:51 quixstreams/models/topics/admin.py
--rw-r--r--  2.0 unx      297 b- defN 24-May-16 05:51 quixstreams/models/topics/exceptions.py
--rw-r--r--  2.0 unx    15725 b- defN 24-May-16 05:51 quixstreams/models/topics/manager.py
--rw-r--r--  2.0 unx    10343 b- defN 24-May-16 05:51 quixstreams/models/topics/topic.py
--rw-r--r--  2.0 unx        0 b- defN 24-May-16 05:51 quixstreams/platforms/__init__.py
--rw-r--r--  2.0 unx      118 b- defN 24-May-16 05:51 quixstreams/platforms/quix/__init__.py
--rw-r--r--  2.0 unx     6306 b- defN 24-May-16 05:51 quixstreams/platforms/quix/api.py
--rw-r--r--  2.0 unx     1531 b- defN 24-May-16 05:51 quixstreams/platforms/quix/checks.py
--rw-r--r--  2.0 unx    22216 b- defN 24-May-16 05:51 quixstreams/platforms/quix/config.py
--rw-r--r--  2.0 unx     1773 b- defN 24-May-16 05:51 quixstreams/platforms/quix/env.py
--rw-r--r--  2.0 unx     1035 b- defN 24-May-16 05:51 quixstreams/platforms/quix/exceptions.py
--rw-r--r--  2.0 unx     3329 b- defN 24-May-16 05:51 quixstreams/platforms/quix/topic_manager.py
--rw-r--r--  2.0 unx       68 b- defN 24-May-16 05:51 quixstreams/state/__init__.py
--rw-r--r--  2.0 unx      381 b- defN 24-May-16 05:51 quixstreams/state/exceptions.py
--rw-r--r--  2.0 unx     8556 b- defN 24-May-16 05:51 quixstreams/state/manager.py
--rw-r--r--  2.0 unx    14923 b- defN 24-May-16 05:51 quixstreams/state/recovery.py
--rw-r--r--  2.0 unx     1672 b- defN 24-May-16 05:51 quixstreams/state/state.py
--rw-r--r--  2.0 unx    15308 b- defN 24-May-16 05:51 quixstreams/state/types.py
--rw-r--r--  2.0 unx      143 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/__init__.py
--rw-r--r--  2.0 unx      587 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/exceptions.py
--rw-r--r--  2.0 unx      323 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/metadata.py
--rw-r--r--  2.0 unx     2959 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/options.py
--rw-r--r--  2.0 unx    13877 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/partition.py
--rw-r--r--  2.0 unx      948 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/serialization.py
--rw-r--r--  2.0 unx     5423 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/store.py
--rw-r--r--  2.0 unx    16846 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/transaction.py
--rw-r--r--  2.0 unx      640 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/types.py
--rw-r--r--  2.0 unx        0 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/__init__.py
--rw-r--r--  2.0 unx      118 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/metadata.py
--rw-r--r--  2.0 unx     2524 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/partition.py
--rw-r--r--  2.0 unx     2014 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/serialization.py
--rw-r--r--  2.0 unx     3057 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/state.py
--rw-r--r--  2.0 unx     2107 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/store.py
--rw-r--r--  2.0 unx     8294 b- defN 24-May-16 05:51 quixstreams/state/rocksdb/windowed/transaction.py
--rw-r--r--  2.0 unx        0 b- defN 24-May-16 05:51 quixstreams/utils/__init__.py
--rw-r--r--  2.0 unx      607 b- defN 24-May-16 05:51 quixstreams/utils/dicts.py
--rw-r--r--  2.0 unx      636 b- defN 24-May-16 05:51 quixstreams/utils/json.py
--rw-r--r--  2.0 unx    11353 b- defN 24-May-16 05:52 quixstreams-2.5.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    22419 b- defN 24-May-16 05:52 quixstreams-2.5.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-16 05:52 quixstreams-2.5.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 24-May-16 05:52 quixstreams-2.5.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     7954 b- defN 24-May-16 05:52 quixstreams-2.5.0.dist-info/RECORD
-88 files, 422127 bytes uncompressed, 117454 bytes compressed:  72.2%
+Zip file size: 131041 bytes, number of entries: 88
+-rw-r--r--  2.0 unx      163 b- defN 24-May-24 15:21 quixstreams/__init__.py
+-rw-r--r--  2.0 unx    37557 b- defN 24-May-24 14:54 quixstreams/app.py
+-rw-r--r--  2.0 unx     2559 b- defN 24-May-24 14:54 quixstreams/context.py
+-rw-r--r--  2.0 unx     1561 b- defN 24-May-24 14:54 quixstreams/error_callbacks.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-May-24 14:54 quixstreams/logging.py
+-rw-r--r--  2.0 unx     2512 b- defN 24-May-24 14:54 quixstreams/processing_context.py
+-rw-r--r--  2.0 unx     6262 b- defN 24-May-24 14:54 quixstreams/rowconsumer.py
+-rw-r--r--  2.0 unx     5610 b- defN 24-May-24 14:54 quixstreams/rowproducer.py
+-rw-r--r--  2.0 unx      112 b- defN 24-May-24 14:54 quixstreams/types.py
+-rw-r--r--  2.0 unx      116 b- defN 24-May-24 14:54 quixstreams/checkpointing/__init__.py
+-rw-r--r--  2.0 unx     6080 b- defN 24-May-24 14:54 quixstreams/checkpointing/checkpoint.py
+-rw-r--r--  2.0 unx       97 b- defN 24-May-24 14:54 quixstreams/checkpointing/exceptions.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-24 14:54 quixstreams/core/__init__.py
+-rw-r--r--  2.0 unx       47 b- defN 24-May-24 14:54 quixstreams/core/stream/__init__.py
+-rw-r--r--  2.0 unx     7678 b- defN 24-May-24 14:54 quixstreams/core/stream/functions.py
+-rw-r--r--  2.0 unx     8070 b- defN 24-May-24 14:54 quixstreams/core/stream/stream.py
+-rw-r--r--  2.0 unx       90 b- defN 24-May-24 14:54 quixstreams/dataframe/__init__.py
+-rw-r--r--  2.0 unx      455 b- defN 24-May-24 14:54 quixstreams/dataframe/base.py
+-rw-r--r--  2.0 unx    26879 b- defN 24-May-24 14:54 quixstreams/dataframe/dataframe.py
+-rw-r--r--  2.0 unx      370 b- defN 24-May-24 14:54 quixstreams/dataframe/exceptions.py
+-rw-r--r--  2.0 unx    16317 b- defN 24-May-24 14:54 quixstreams/dataframe/series.py
+-rw-r--r--  2.0 unx      754 b- defN 24-May-24 14:54 quixstreams/dataframe/utils.py
+-rw-r--r--  2.0 unx      106 b- defN 24-May-24 14:54 quixstreams/dataframe/windows/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-24 14:54 quixstreams/dataframe/windows/base.py
+-rw-r--r--  2.0 unx    10516 b- defN 24-May-24 14:54 quixstreams/dataframe/windows/definitions.py
+-rw-r--r--  2.0 unx     7163 b- defN 24-May-24 14:54 quixstreams/dataframe/windows/time_based.py
+-rw-r--r--  2.0 unx       46 b- defN 24-May-24 14:54 quixstreams/exceptions/__init__.py
+-rw-r--r--  2.0 unx      322 b- defN 24-May-24 14:54 quixstreams/exceptions/assignment.py
+-rw-r--r--  2.0 unx       41 b- defN 24-May-24 14:54 quixstreams/exceptions/base.py
+-rw-r--r--  2.0 unx       48 b- defN 24-May-24 14:54 quixstreams/kafka/__init__.py
+-rw-r--r--  2.0 unx    21948 b- defN 24-May-24 14:54 quixstreams/kafka/consumer.py
+-rw-r--r--  2.0 unx      695 b- defN 24-May-24 14:54 quixstreams/kafka/exceptions.py
+-rw-r--r--  2.0 unx     6019 b- defN 24-May-24 14:54 quixstreams/kafka/producer.py
+-rw-r--r--  2.0 unx      146 b- defN 24-May-24 14:54 quixstreams/models/__init__.py
+-rw-r--r--  2.0 unx     1949 b- defN 24-May-24 14:54 quixstreams/models/messagecontext.py
+-rw-r--r--  2.0 unx      485 b- defN 24-May-24 14:54 quixstreams/models/messages.py
+-rw-r--r--  2.0 unx     2229 b- defN 24-May-24 14:54 quixstreams/models/rows.py
+-rw-r--r--  2.0 unx     1813 b- defN 24-May-24 14:54 quixstreams/models/timestamps.py
+-rw-r--r--  2.0 unx     1309 b- defN 24-May-24 14:54 quixstreams/models/types.py
+-rw-r--r--  2.0 unx     1016 b- defN 24-May-24 14:54 quixstreams/models/serializers/__init__.py
+-rw-r--r--  2.0 unx     3157 b- defN 24-May-24 14:54 quixstreams/models/serializers/base.py
+-rw-r--r--  2.0 unx     1370 b- defN 24-May-24 14:54 quixstreams/models/serializers/exceptions.py
+-rw-r--r--  2.0 unx     1936 b- defN 24-May-24 14:54 quixstreams/models/serializers/json.py
+-rw-r--r--  2.0 unx    16878 b- defN 24-May-24 14:54 quixstreams/models/serializers/quix.py
+-rw-r--r--  2.0 unx     4516 b- defN 24-May-24 14:54 quixstreams/models/serializers/simple_types.py
+-rw-r--r--  2.0 unx       65 b- defN 24-May-24 14:54 quixstreams/models/topics/__init__.py
+-rw-r--r--  2.0 unx     7042 b- defN 24-May-24 14:54 quixstreams/models/topics/admin.py
+-rw-r--r--  2.0 unx      297 b- defN 24-May-24 14:54 quixstreams/models/topics/exceptions.py
+-rw-r--r--  2.0 unx    17593 b- defN 24-May-24 14:54 quixstreams/models/topics/manager.py
+-rw-r--r--  2.0 unx    10334 b- defN 24-May-24 14:54 quixstreams/models/topics/topic.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-24 14:54 quixstreams/platforms/__init__.py
+-rw-r--r--  2.0 unx      118 b- defN 24-May-24 14:54 quixstreams/platforms/quix/__init__.py
+-rw-r--r--  2.0 unx     6845 b- defN 24-May-24 14:54 quixstreams/platforms/quix/api.py
+-rw-r--r--  2.0 unx     1531 b- defN 24-May-24 14:54 quixstreams/platforms/quix/checks.py
+-rw-r--r--  2.0 unx    24474 b- defN 24-May-24 14:54 quixstreams/platforms/quix/config.py
+-rw-r--r--  2.0 unx     1773 b- defN 24-May-24 14:54 quixstreams/platforms/quix/env.py
+-rw-r--r--  2.0 unx     1035 b- defN 24-May-24 14:54 quixstreams/platforms/quix/exceptions.py
+-rw-r--r--  2.0 unx     3743 b- defN 24-May-24 14:54 quixstreams/platforms/quix/topic_manager.py
+-rw-r--r--  2.0 unx       68 b- defN 24-May-24 14:54 quixstreams/state/__init__.py
+-rw-r--r--  2.0 unx      381 b- defN 24-May-24 14:54 quixstreams/state/exceptions.py
+-rw-r--r--  2.0 unx     8556 b- defN 24-May-24 14:54 quixstreams/state/manager.py
+-rw-r--r--  2.0 unx    14923 b- defN 24-May-24 14:54 quixstreams/state/recovery.py
+-rw-r--r--  2.0 unx     1672 b- defN 24-May-24 14:54 quixstreams/state/state.py
+-rw-r--r--  2.0 unx    15308 b- defN 24-May-24 14:54 quixstreams/state/types.py
+-rw-r--r--  2.0 unx      143 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/__init__.py
+-rw-r--r--  2.0 unx      587 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/exceptions.py
+-rw-r--r--  2.0 unx      323 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/metadata.py
+-rw-r--r--  2.0 unx     2959 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/options.py
+-rw-r--r--  2.0 unx    13877 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/partition.py
+-rw-r--r--  2.0 unx      948 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/serialization.py
+-rw-r--r--  2.0 unx     5423 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/store.py
+-rw-r--r--  2.0 unx    16846 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/transaction.py
+-rw-r--r--  2.0 unx      640 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/types.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/__init__.py
+-rw-r--r--  2.0 unx      118 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/metadata.py
+-rw-r--r--  2.0 unx     2524 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/partition.py
+-rw-r--r--  2.0 unx     2014 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/serialization.py
+-rw-r--r--  2.0 unx     3057 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/state.py
+-rw-r--r--  2.0 unx     2107 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/store.py
+-rw-r--r--  2.0 unx     8294 b- defN 24-May-24 14:54 quixstreams/state/rocksdb/windowed/transaction.py
+-rw-r--r--  2.0 unx        0 b- defN 24-May-24 14:54 quixstreams/utils/__init__.py
+-rw-r--r--  2.0 unx      607 b- defN 24-May-24 14:54 quixstreams/utils/dicts.py
+-rw-r--r--  2.0 unx      636 b- defN 24-May-24 14:54 quixstreams/utils/json.py
+-rw-r--r--  2.0 unx    11353 b- defN 24-May-24 15:21 quixstreams-2.5.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    22385 b- defN 24-May-24 15:21 quixstreams-2.5.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-24 15:21 quixstreams-2.5.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 24-May-24 15:21 quixstreams-2.5.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     7954 b- defN 24-May-24 15:21 quixstreams-2.5.1.dist-info/RECORD
+88 files, 428377 bytes uncompressed, 118269 bytes compressed:  72.4%
```

## zipnote {}

```diff
@@ -243,23 +243,23 @@
 
 Filename: quixstreams/utils/dicts.py
 Comment: 
 
 Filename: quixstreams/utils/json.py
 Comment: 
 
-Filename: quixstreams-2.5.0.dist-info/LICENSE
+Filename: quixstreams-2.5.1.dist-info/LICENSE
 Comment: 
 
-Filename: quixstreams-2.5.0.dist-info/METADATA
+Filename: quixstreams-2.5.1.dist-info/METADATA
 Comment: 
 
-Filename: quixstreams-2.5.0.dist-info/WHEEL
+Filename: quixstreams-2.5.1.dist-info/WHEEL
 Comment: 
 
-Filename: quixstreams-2.5.0.dist-info/top_level.txt
+Filename: quixstreams-2.5.1.dist-info/top_level.txt
 Comment: 
 
-Filename: quixstreams-2.5.0.dist-info/RECORD
+Filename: quixstreams-2.5.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## quixstreams/__init__.py

```diff
@@ -1,7 +1,7 @@
 from .app import Application
 from .context import message_context, message_key
 from .models import MessageContext
 from .state import State
 
 
-__version__ = "2.5.0"
+__version__ = "2.5.1"
```

## quixstreams/app.py

```diff
@@ -113,14 +113,16 @@
         consumer_poll_timeout: float = 1.0,
         producer_poll_timeout: float = 0.0,
         loglevel: Optional[LogLevel] = "INFO",
         auto_create_topics: bool = True,
         use_changelog_topics: bool = True,
         quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,
         topic_manager: Optional[TopicManager] = None,
+        request_timeout: float = 30,
+        topic_create_timeout: float = 60,
     ):
         """
         :param broker_address: Kafka broker host and port in format `<host>:<port>`.
             Passed as `bootstrap.servers` to `confluent_kafka.Consumer`.
             Either this OR `quix_sdk_token` must be set to use `Application` (not both).
             Linked Environment Variable: `Quix__Broker__Address`.
             Default: `None`
@@ -158,14 +160,16 @@
             externally using `logging` library.
             Default - `"INFO"`.
         :param auto_create_topics: Create all `Topic`s made via Application.topic()
             Default - `True`
         :param use_changelog_topics: Use changelog topics to back stateful operations
             Default - `True`
         :param topic_manager: A `TopicManager` instance
+        :param request_timeout: timeout (seconds) for REST-based requests
+        :param topic_create_timeout: timeout (seconds) for topic create finalization
 
         <br><br>***Error Handlers***<br>
         To handle errors, `Application` accepts callbacks triggered when
             exceptions occur on different stages of stream processing. If the callback
             returns `True`, the exception will be ignored. Otherwise, the exception
             will be propagated and the processing will eventually stop.
         :param on_consumer_error: triggered when internal `RowConsumer` fails
@@ -208,17 +212,21 @@
             )
 
         if quix_sdk_token and not quix_config_builder:
             quix_app_source = "Quix SDK Token"
             quix_config_builder = QuixKafkaConfigsBuilder(quix_sdk_token=quix_sdk_token)
 
         if broker_address and quix_config_builder:
-            raise ValueError("Cannot provide both broker address and Quix SDK Token")
+            raise ValueError(
+                'Cannot provide both "broker_address" and "quix_sdk_token"'
+            )
         elif not (broker_address or quix_config_builder):
-            raise ValueError("Either broker address or Quix SDK Token must be provided")
+            raise ValueError(
+                'Either "broker_address" or "quix_sdk_token" must be provided'
+            )
         elif quix_config_builder:
             # SDK Token or QuixKafkaConfigsBuilder were provided
             logger.info(
                 f"{quix_app_source} detected; "
                 f"the application will connect to Quix Cloud brokers"
             )
             topic_manager_factory = functools.partial(
@@ -272,14 +280,16 @@
         if not topic_manager:
             topic_manager = topic_manager_factory(
                 topic_admin=TopicAdmin(
                     broker_address=broker_address,
                     extra_config=producer_extra_config,
                 ),
                 consumer_group=consumer_group,
+                timeout=request_timeout,
+                create_timeout=topic_create_timeout,
             )
         self._topic_manager = topic_manager
         self._state_manager = StateStoreManager(
             group_id=consumer_group,
             state_dir=state_dir,
             rocksdb_options=rocksdb_options,
             producer=self._producer if use_changelog_topics else None,
@@ -316,14 +326,16 @@
         consumer_poll_timeout: float = 1.0,
         producer_poll_timeout: float = 0.0,
         loglevel: Optional[LogLevel] = "INFO",
         quix_config_builder: Optional[QuixKafkaConfigsBuilder] = None,
         auto_create_topics: bool = True,
         use_changelog_topics: bool = True,
         topic_manager: Optional[QuixTopicManager] = None,
+        request_timeout: float = 30,
+        topic_create_timeout: float = 60,
     ) -> Self:
         """
         >***NOTE:*** DEPRECATED: use Application with `quix_sdk_token` argument instead.
 
         Initialize an Application to work with Quix Cloud,
         assuming environment is properly configured (by default in Quix Cloud).
 
@@ -383,14 +395,16 @@
             externally using `logging` library.
             Default - `"INFO"`.
         :param auto_create_topics: Create all `Topic`s made via `Application.topic()`
             Default - `True`
         :param use_changelog_topics: Use changelog topics to back stateful operations
             Default - `True`
         :param topic_manager: A `QuixTopicManager` instance
+        :param request_timeout: timeout (seconds) for REST-based requests
+        :param topic_create_timeout: timeout (seconds) for topic create finalization
 
         <br><br>***Error Handlers***<br>
         To handle errors, `Application` accepts callbacks triggered when
             exceptions occur on different stages of stream processing. If the callback
             returns `True`, the exception will be ignored. Otherwise, the exception
             will be propagated and the processing will eventually stop.
         :param on_consumer_error: triggered when internal `RowConsumer` fails to poll
@@ -428,14 +442,16 @@
             producer_poll_timeout=producer_poll_timeout,
             loglevel=loglevel,
             state_dir=state_dir,
             rocksdb_options=rocksdb_options,
             auto_create_topics=auto_create_topics,
             use_changelog_topics=use_changelog_topics,
             topic_manager=topic_manager,
+            request_timeout=request_timeout,
+            topic_create_timeout=topic_create_timeout,
             quix_config_builder=quix_config_builder,
         )
         return app
 
     def topic(
         self,
         name: str,
```

## quixstreams/models/topics/admin.py

```diff
@@ -63,45 +63,54 @@
         self._config = {
             "bootstrap.servers": broker_address,
             **(extra_config or {}),
             "logger": logger,
         }
 
     @property
-    def _admin_client(self) -> AdminClient:
+    def admin_client(self) -> AdminClient:
         if not self._inner_admin:
             self._inner_admin = AdminClient(self._config)
         return self._inner_admin
 
-    def list_topics(self) -> Dict[str, ConfluentTopicMetadata]:
+    def list_topics(self, timeout: float = -1) -> Dict[str, ConfluentTopicMetadata]:
         """
         Get a list of topics and their metadata from a Kafka cluster
 
+        :param timeout: response timeout (seconds); Default infinite (-1)
+
         :return: a dict of topic names and their metadata objects
         """
         # TODO: allow filtering based on a prefix ignore list?
-        return self._admin_client.list_topics().topics
+        return self.admin_client.list_topics(timeout=timeout).topics
 
     def inspect_topics(
-        self, topic_names: List[str]
+        self,
+        topic_names: List[str],
+        timeout: float = 30,
     ) -> Dict[str, Optional[TopicConfig]]:
         """
         A simplified way of getting the topic configurations of the provided topics
         from the cluster (if they exist).
 
         :param topic_names: a list of topic names
+        :param timeout: response timeout (seconds)
+
+        >***NOTE***: `timeout` must be >0 here (expects non-neg, and 0 != inf).
+
         :return: a dict with topic names and their respective `TopicConfig`
         """
         futures_dict = {}
-        cluster_topics = self.list_topics()
+        cluster_topics = self.list_topics(timeout=timeout)
         if existing_topics := [
             topic for topic in topic_names if topic in cluster_topics
         ]:
-            futures_dict = self._admin_client.describe_configs(
-                [confluent_topic_config(topic) for topic in existing_topics]
+            futures_dict = self.admin_client.describe_configs(
+                [confluent_topic_config(topic) for topic in existing_topics],
+                request_timeout=timeout,
             )
         configs = {
             config_resource.name: {c.name: c.value for c in config.result().values()}
             for config_resource, config in futures_dict.items()
         }
         return {
             topic: (
@@ -114,24 +123,26 @@
                 )
                 if topic in existing_topics
                 else None
             )
             for topic in topic_names
         }
 
-    def _finalize_create(self, futures: Dict[str, Future], timeout: int):
+    def _finalize_create(
+        self, futures: Dict[str, Future], finalize_timeout: float = 60
+    ):
         """
         The confirmation step for topic creation.
 
         :param futures: a dict of futures as generated by Confluent's
             `AdminClient.create_topics()`
-        :param timeout: How long to confirm topics before raising exception
+        :param finalize_timeout: topic finalization timeout (seconds)
         """
         exceptions = {}
-        stop_time = time.time() + timeout
+        stop_time = time.time() + finalize_timeout
         while futures and time.time() < stop_time:
             time.sleep(1)
             for topic_name in list(futures.keys()):
                 future = futures[topic_name]
                 if future.done():
                     try:
                         future.result()
@@ -151,40 +162,42 @@
         if futures:
             raise CreateTopicTimeout(
                 f"Timed out waiting for creation status for topics:\n"
                 f"{pprint.pformat([topic_name for topic_name in futures])}"
             )
 
     def create_topics(
-        self, topics: List[Topic], timeout: int = 10, finalize_timeout: int = 60
+        self, topics: List[Topic], timeout: float = 30, finalize_timeout: float = 60
     ):
         """
         Create the given list of topics and confirm they are ready.
 
         Also raises an exception with detailed printout should the creation
         fail (it ignores issues for a topic already existing).
 
         :param topics: a list of `Topic`
-        :param timeout: timeout of the creation broker request
-        :param finalize_timeout: the timeout of the topic finalizing ("ready")
+        :param timeout: creation acknowledge timeout (seconds)
+        :param finalize_timeout: topic finalization timeout (seconds)
+
+        >***NOTE***: `timeout` must be >0 here (expects non-neg, and 0 != inf).
         """
 
-        existing_topics = self.list_topics()
+        existing_topics = self.list_topics(timeout=timeout)
         topics_to_create = [
             topic for topic in topics if topic.name not in existing_topics
         ]
         if not topics_to_create:
             return
 
         for topic in topics_to_create:
             logger.info(
                 f'Creating a new topic "{topic.name}" '
                 f'with config: "{topic.config.as_dict()}"'
             )
 
         self._finalize_create(
-            self._admin_client.create_topics(
+            self.admin_client.create_topics(
                 convert_topic_list(topics_to_create),
                 request_timeout=timeout,
             ),
-            finalize_timeout,
+            finalize_timeout=finalize_timeout,
         )
```

## quixstreams/models/topics/manager.py

```diff
@@ -34,39 +34,45 @@
     Generally initialized and managed automatically by an `Application`,
     but allows a user to work with it directly when needed, such as using it alongside
     a plain `Producer` to create its topics.
 
     See methods for details.
     """
 
-    _topic_partitions = 1
-    _topic_replication = 1
+    # Default topic params
+    default_num_partitions = 1
+    default_replication_factor = 1
+    default_extra_config = {}
+
+    # Max topic name length for the new topics
     _max_topic_name_len = 255
 
-    _topic_extra_config_defaults = {}
     _groupby_extra_config_imports_defaults = {"retention.bytes", "retention.ms"}
     _changelog_extra_config_defaults = {"cleanup.policy": "compact"}
     _changelog_extra_config_imports_defaults = {"retention.bytes", "retention.ms"}
 
     def __init__(
         self,
         topic_admin: TopicAdmin,
         consumer_group: str,
-        create_timeout: int = 60,
+        timeout: float = 30,
+        create_timeout: float = 60,
     ):
         """
         :param topic_admin: an `Admin` instance (required for some functionality)
         :param consumer_group: the consumer group (of the `Application`)
+        :param timeout: response timeout (seconds)
         :param create_timeout: timeout for topic creation
         """
         self._admin = topic_admin
         self._consumer_group = consumer_group
         self._topics: Dict[str, Topic] = {}
         self._repartition_topics: Dict[str, Topic] = {}
         self._changelog_topics: Dict[str, Dict[str, Topic]] = {}
+        self._timeout = timeout
         self._create_timeout = create_timeout
 
     @property
     def _topics_list(self) -> List[Topic]:
         return list(self._topics.values())
 
     @property
@@ -101,15 +107,15 @@
         returns: the changelog topic dict, {topic_name: {suffix: Topic}}
         """
         return self._changelog_topics
 
     @property
     def all_topics(self) -> Dict[str, Topic]:
         """
-        Every registered topic name mapped to it's respective `Topic`.
+        Every registered topic name mapped to its respective `Topic`.
 
         returns: full topic dict, {topic_name: Topic}
         """
         return {topic.name: topic for topic in self._all_topics_list}
 
     def _resolve_topic_name(self, name: str) -> str:
         """
@@ -164,37 +170,47 @@
         :return: formatted topic name
         """
         nested_name = self._format_nested_name(topic_name)
         return self._resolve_topic_name(
             f"{topic_type}__{'--'.join([self._consumer_group, nested_name, suffix])}"
         )
 
-    def _create_topics(self, topics: List[Topic]):
+    def _create_topics(
+        self, topics: List[Topic], timeout: float, create_timeout: float
+    ):
         """
         Method that actually creates the topics in Kafka via an `Admin` instance.
 
         :param topics: list of `Topic`s
+        :param timeout: creation acknowledge timeout (seconds)
+        :param create_timeout: topic finalization timeout (seconds)
         """
-        self._admin.create_topics(topics, timeout=self._create_timeout)
+        self._admin.create_topics(
+            topics, timeout=timeout, finalize_timeout=create_timeout
+        )
 
     def _get_source_topic_config(
-        self, topic_name: str, extras_imports: Optional[Set[str]] = None
+        self,
+        topic_name: str,
+        timeout: float,
+        extras_imports: Optional[Set[str]] = None,
     ) -> TopicConfig:
         """
         Retrieve configs for a topic, defaulting to stored Topic objects if topic does
         not exist in Kafka.
 
         :param topic_name: name of the topic to get configs from
+        :param timeout: config lookup timeout (seconds); Default 30
         :param extras_imports: set of extra configs that should be imported from topic
 
         :return: a TopicConfig
         """
-        topic_config = self._admin.inspect_topics([topic_name])[topic_name] or deepcopy(
-            self._non_changelog_topics[topic_name].config
-        )
+        topic_config = self._admin.inspect_topics([topic_name], timeout=timeout)[
+            topic_name
+        ] or deepcopy(self._non_changelog_topics[topic_name].config)
 
         # Copy only certain configuration values from original topic
         if extras_imports:
             topic_config.extra_config = {
                 k: v
                 for k, v in topic_config.extra_config.items()
                 if k in extras_imports
@@ -213,17 +229,17 @@
         :param num_partitions: the number of topic partitions
         :param replication_factor: the topic replication factor
         :param extra_config: other optional configuration settings
 
         :return: a TopicConfig object
         """
         return TopicConfig(
-            num_partitions=num_partitions or self._topic_partitions,
-            replication_factor=replication_factor or self._topic_replication,
-            extra_config=extra_config or self._topic_extra_config_defaults,
+            num_partitions=num_partitions or self.default_num_partitions,
+            replication_factor=replication_factor or self.default_replication_factor,
+            extra_config=extra_config or self.default_extra_config,
         )
 
     def topic(
         self,
         name: str,
         value_deserializer: Optional[DeserializerType] = None,
         key_deserializer: Optional[DeserializerType] = "bytes",
@@ -247,17 +263,17 @@
 
         :return: Topic object with creation configs
         """
         name = self._resolve_topic_name(name)
 
         if not config:
             config = TopicConfig(
-                num_partitions=self._topic_partitions,
-                replication_factor=self._topic_replication,
-                extra_config=self._topic_extra_config_defaults,
+                num_partitions=self.default_num_partitions,
+                replication_factor=self.default_replication_factor,
+                extra_config=self.default_extra_config,
             )
         topic = Topic(
             name=name,
             value_serializer=value_serializer,
             value_deserializer=value_deserializer,
             key_serializer=key_serializer,
             key_deserializer=key_deserializer,
@@ -271,46 +287,51 @@
         self,
         operation: str,
         topic_name: str,
         value_deserializer: Optional[DeserializerType] = "json",
         key_deserializer: Optional[DeserializerType] = "json",
         value_serializer: Optional[SerializerType] = "json",
         key_serializer: Optional[SerializerType] = "json",
+        timeout: Optional[float] = None,
     ) -> Topic:
         """
         Create an internal repartition topic.
 
         :param operation: name of the GroupBy operation (column name or user-defined).
         :param topic_name: name of the topic the GroupBy is sourced from.
         :param value_deserializer: a deserializer type for values; default - JSON
         :param key_deserializer: a deserializer type for keys; default - JSON
         :param value_serializer: a serializer type for values; default - JSON
         :param key_serializer: a serializer type for keys; default - JSON
+        :param timeout: config lookup timeout (seconds); Default 30
 
+        :return: `Topic` object (which is also stored on the TopicManager)
         """
         name = self._internal_name(f"repartition", topic_name, operation)
 
         topic = Topic(
             name=name,
             value_deserializer=value_deserializer,
             key_deserializer=key_deserializer,
             value_serializer=value_serializer,
             key_serializer=key_serializer,
             config=self._get_source_topic_config(
                 topic_name,
                 extras_imports=self._groupby_extra_config_imports_defaults,
+                timeout=timeout if timeout is not None else self._timeout,
             ),
         )
         self._repartition_topics[name] = topic
         return topic
 
     def changelog_topic(
         self,
         topic_name: str,
         store_name: str,
+        timeout: Optional[float] = None,
     ) -> Topic:
         """
         Performs all the logic necessary to generate a changelog topic based on a
         "source topic" (aka input/consumed topic).
 
         Its main goal is to ensure partition counts of the to-be generated changelog
         match the source topic, and ensure the changelog topic is compacted. Also
@@ -325,21 +346,24 @@
         generate changelog topics. To turn off changelogs, init an Application with
         "use_changelog_topics"=`False`.
 
         :param topic_name: name of consumed topic (app input topic)
             > NOTE: normally contain any prefixes added by TopicManager.topic()
         :param store_name: name of the store this changelog belongs to
             (default, rolling10s, etc.)
+        :param timeout: config lookup timeout (seconds); Default 30
 
         :return: `Topic` object (which is also stored on the TopicManager)
         """
 
         topic_name = self._resolve_topic_name(topic_name)
         source_topic_config = self._get_source_topic_config(
-            topic_name, extras_imports=self._changelog_extra_config_imports_defaults
+            topic_name,
+            extras_imports=self._changelog_extra_config_imports_defaults,
+            timeout=timeout if timeout is not None else self._timeout,
         )
         source_topic_config.extra_config.update(self._changelog_extra_config_defaults)
 
         changelog_config = self.topic_config(
             num_partitions=source_topic_config.num_partitions,
             replication_factor=source_topic_config.replication_factor,
             extra_config=source_topic_config.extra_config,
@@ -352,69 +376,87 @@
             key_deserializer="bytes",
             value_deserializer="bytes",
             config=changelog_config,
         )
         self._changelog_topics.setdefault(topic_name, {})[store_name] = topic
         return topic
 
-    def create_topics(self, topics: List[Topic]):
+    def create_topics(
+        self,
+        topics: List[Topic],
+        timeout: Optional[float] = None,
+        create_timeout: Optional[float] = None,
+    ):
         """
         Creates topics via an explicit list of provided `Topics`.
 
         Exists as a way to manually specify what topics to create; otherwise,
         `create_all_topics()` is generally simpler.
 
         :param topics: list of `Topic`s
+        :param timeout: creation acknowledge timeout (seconds); Default 30
+        :param create_timeout: topic finalization timeout (seconds); Default 60
         """
         if not topics:
             logger.debug("No topics provided for creation...skipping!")
             return
         affirm_ready_for_create(topics)
-        self._create_topics(topics)
+        self._create_topics(
+            topics,
+            timeout=timeout if timeout is not None else self._timeout,
+            create_timeout=(
+                create_timeout if create_timeout is not None else self._create_timeout
+            ),
+        )
 
-    def create_all_topics(self):
+    def create_all_topics(
+        self, timeout: Optional[float] = None, create_timeout: Optional[float] = None
+    ):
         """
         A convenience method to create all Topic objects stored on this TopicManager.
+
+        :param timeout: creation acknowledge timeout (seconds); Default 30
+        :param create_timeout: topic finalization timeout (seconds); Default 60
         """
-        self.create_topics(self._all_topics_list)
+        self.create_topics(
+            self._all_topics_list, timeout=timeout, create_timeout=create_timeout
+        )
 
-    def validate_all_topics(self):
+    def validate_all_topics(self, timeout: Optional[float] = None):
         """
         Validates all topics exist and changelogs have correct topic and rep factor.
 
         Issues are pooled and raised as an Exception once inspections are complete.
         """
         logger.info(f"Validating Kafka topics exist and are configured correctly...")
-        topics = self._all_topics_list
-        changelog_names = [topic.name for topic in self._changelog_topics_list]
-        actual_configs = self._admin.inspect_topics([t.name for t in topics])
-
-        for topic in topics:
-            # Validate that topic exists
-            actual_topic_config = actual_configs[topic.name]
-            if actual_topic_config is None:
-                raise TopicNotFoundError(
-                    f'Topic "{topic.name}" is not found on the broker'
-                )
-
-            # For changelog topics, validate the amount of partitions and
-            # a replication factor match with the source topic
-            if topic.name in changelog_names:
-                # Ensure that changelog topic has the same amount of partitions and
-                # replication factor as the source topic
-                if topic.config.num_partitions != actual_topic_config.num_partitions:
+        all_topic_names = [t.name for t in self._all_topics_list]
+        actual_configs = self._admin.inspect_topics(
+            all_topic_names,
+            timeout=timeout if timeout is not None else self._timeout,
+        )
+
+        if missing := [t for t in all_topic_names if actual_configs[t] is None]:
+            raise TopicNotFoundError(f"Topics {missing} not found on the broker")
+
+        for source_name in self._non_changelog_topics.keys():
+            source_cfg = actual_configs[source_name]
+            # For any changelog topics, validate the amount of partitions and
+            # replication factor match with the source topic
+            for changelog in self.changelog_topics.get(source_name, {}).values():
+                changelog_cfg = actual_configs[changelog.name]
+
+                if changelog_cfg.num_partitions != source_cfg.num_partitions:
                     raise TopicConfigurationMismatch(
-                        f'Invalid partition count for the topic "{topic.name}": '
-                        f"expected {topic.config.num_partitions}, "
-                        f"got {actual_topic_config.num_partitions}"
+                        f'changelog topic "{changelog.name}" partition count '
+                        f'does not match its source topic "{source_name}": '
+                        f"expected {source_cfg.num_partitions}, "
+                        f'got {changelog_cfg.num_partitions}"'
                     )
-
-                if (
-                    topic.config.replication_factor
-                    != actual_topic_config.replication_factor
-                ):
+                if changelog_cfg.replication_factor != source_cfg.replication_factor:
                     raise TopicConfigurationMismatch(
-                        f'Invalid replication factor for the topic "{topic.name}": '
-                        f"expected {topic.config.replication_factor}, "
-                        f"got {actual_topic_config.replication_factor}"
+                        f'changelog topic "{changelog.name}" replication factor '
+                        f'does not match its source topic "{source_name}": '
+                        f"expected {source_cfg.replication_factor}, "
+                        f'got {changelog_cfg.replication_factor}"'
                     )
+
         logger.info(f"Kafka topics validation complete")
```

## quixstreams/models/topics/topic.py

```diff
@@ -97,19 +97,19 @@
         key_deserializer: Optional[DeserializerType] = BytesDeserializer(),
         value_serializer: Optional[SerializerType] = None,
         key_serializer: Optional[SerializerType] = BytesSerializer(),
         timestamp_extractor: Optional[TimestampExtractor] = None,
     ):
         """
         :param name: topic name
+        :param config: topic configs via `TopicConfig` (creation/validation)
         :param value_deserializer: a deserializer type for values
         :param key_deserializer: a deserializer type for keys
         :param value_serializer: a serializer type for values
         :param key_serializer: a serializer type for keys
-        :param config: optional topic configs via `TopicConfig` (creation/validation)
         :param timestamp_extractor: a callable that returns a timestamp in
             milliseconds from a deserialized message.
         """
         self._name = name
         self._config = config
         self._key_serializer = _get_serializer(key_serializer)
         self._key_deserializer = _get_deserializer(key_deserializer)
```

## quixstreams/platforms/quix/api.py

```diff
@@ -54,15 +54,16 @@
 
     class SessionWithUrlBase(requests.Session):
         def __init__(self, url_base: str):
             self.url_base = url_base
             super().__init__()
 
         def request(self, method, url, **kwargs):
-            timeout = kwargs.pop("timeout", 10)
+            # provided methods have timeout defined, but keep a default for adhoc calls
+            timeout = kwargs.pop("timeout", 30)
             return super().request(
                 method, urljoin(base=self.url_base, url=url), timeout=timeout, **kwargs
             )
 
     @property
     def default_workspace_id(self) -> str:
         if not self._default_workspace_id:
@@ -106,67 +107,83 @@
                 "X-Version": self.api_version,
                 "Authorization": f"Bearer {self._auth_token}",
             }
         )
         return s
 
     def get_workspace_certificate(
-        self, workspace_id: Optional[str] = None
+        self, workspace_id: Optional[str] = None, timeout: float = 30
     ) -> Optional[bytes]:
         """
         Get a workspace TLS certificate if available.
 
         Returns `None` if certificate is not specified.
 
         :param workspace_id: workspace id, optional
+        :param timeout: request timeout; Default 30
         :return: certificate as bytes if present, or None
         """
         workspace_id = workspace_id or self.default_workspace_id
-        content = self.session.get(f"/workspaces/{workspace_id}/certificates").content
+        content = self.session.get(
+            f"/workspaces/{workspace_id}/certificates", timeout=timeout
+        ).content
         if not content:
             return
 
         with ZipFile(BytesIO(content)) as z:
             with z.open("ca.cert") as f:
                 return f.read()
 
-    def get_auth_token_details(self) -> dict:
-        return self.session.get(f"/auth/token/details").json()
+    def get_auth_token_details(self, timeout: float = 30) -> dict:
+        return self.session.get(f"/auth/token/details", timeout=timeout).json()
 
-    def get_workspace(self, workspace_id: Optional[str] = None) -> dict:
+    def get_workspace(
+        self, workspace_id: Optional[str] = None, timeout: float = 30
+    ) -> dict:
         workspace_id = workspace_id or self.default_workspace_id
-        return self.session.get(f"/workspaces/{workspace_id}").json()
+        return self.session.get(f"/workspaces/{workspace_id}", timeout=timeout).json()
 
-    def get_workspaces(self) -> List[dict]:
+    def get_workspaces(self, timeout: float = 30) -> List[dict]:
         # TODO: This seems only return [] with Personal Access Tokens as of Sept 7 '23
-        return self.session.get("/workspaces").json()
+        return self.session.get("/workspaces", timeout=timeout).json()
 
-    def get_topic(self, topic_name: str, workspace_id: Optional[str] = None) -> dict:
+    def get_topic(
+        self, topic_name: str, workspace_id: Optional[str] = None, timeout: float = 30
+    ) -> dict:
         workspace_id = workspace_id or self.default_workspace_id
-        return self.session.get(f"/{workspace_id}/topics/{topic_name}").json()
+        return self.session.get(
+            f"/{workspace_id}/topics/{topic_name}", timeout=timeout
+        ).json()
 
-    def get_topics(self, workspace_id: Optional[str] = None) -> List[dict]:
+    def get_topics(
+        self,
+        workspace_id: Optional[str] = None,
+        timeout: float = 30,
+    ) -> List[dict]:
         workspace_id = workspace_id or self.default_workspace_id
-        return self.session.get(f"/{workspace_id}/topics").json()
+        return self.session.get(f"/{workspace_id}/topics", timeout=timeout).json()
 
     def post_topic(
         self,
         topic_name: str,
         topic_partitions: Optional[int] = None,
         topic_rep_factor: Optional[int] = None,
         topic_ret_minutes: Optional[int] = None,
         topic_ret_bytes: Optional[int] = None,
         cleanup_policy: Optional[Literal["compact", "delete"]] = None,
         workspace_id: Optional[str] = None,
+        timeout: float = 30,
     ) -> dict:
         workspace_id = workspace_id or self.default_workspace_id
         d = {
             "name": topic_name,
             "configuration": {
                 "partitions": topic_partitions,
                 "replicationFactor": topic_rep_factor,
                 "retentionInMinutes": topic_ret_minutes,
                 "retentionInBytes": topic_ret_bytes,
                 "cleanupPolicy": cleanup_policy,
             },
         }
-        return self.session.post(f"/{workspace_id}/topics", json=d).json()
+        return self.session.post(
+            f"/{workspace_id}/topics", json=d, timeout=timeout
+        ).json()
```

## quixstreams/platforms/quix/config.py

```diff
@@ -107,14 +107,16 @@
     # TODO: Consider a workspace class?
     def __init__(
         self,
         quix_sdk_token: Optional[str] = None,
         workspace_id: Optional[str] = None,
         workspace_cert_path: Optional[str] = None,
         quix_portal_api_service: Optional[QuixPortalApiService] = None,
+        timeout: float = 30,
+        topic_create_timeout: float = 60,
     ):
         """
         :param quix_portal_api_service: A QuixPortalApiService instance (else generated)
         :param workspace_id: A valid Quix Workspace ID (else searched for)
         :param workspace_cert_path: path to an existing workspace cert (else retrieved)
         """
         if quix_sdk_token:
@@ -140,14 +142,16 @@
                 "provide a known topic name later on to help find the applicable ID."
             )
         self._workspace_cert_path = workspace_cert_path
         self._confluent_broker_config = None
         self._quix_broker_config = None
         self._quix_broker_settings = None
         self._workspace_meta = None
+        self._timeout = timeout
+        self._topic_create_timeout = topic_create_timeout
 
     @property
     def workspace_id(self) -> str:
         if not self._workspace_id:
             self.get_workspace_info()
         return self._workspace_id
 
@@ -198,48 +202,64 @@
 
         :param s: the string to append to
         :return: the string with workspace_id prepended
         """
         return prepend_workspace_id(self.workspace_id, s)
 
     def search_for_workspace(
-        self, workspace_name_or_id: Optional[str] = None
+        self,
+        workspace_name_or_id: Optional[str] = None,
+        timeout: Optional[float] = None,
     ) -> Optional[dict]:
         # TODO: there is more to do here to accommodate the new "environments" in v2
         # as it stands now, the search won't work with Quix v2 platform correctly if
         # it's not a workspace_id
         """
         Search for a workspace given an expected workspace name or id.
 
         :param workspace_name_or_id: the expected name or id of a workspace
+        :param timeout: response timeout (seconds); Default 30
+
         :return: the workspace data dict if search success, else None
         """
+        timeout = timeout if timeout is not None else self._timeout
         if not workspace_name_or_id:
             workspace_name_or_id = self._workspace_id
         try:
-            return self.api.get_workspace(workspace_id=workspace_name_or_id)
+            return self.api.get_workspace(
+                workspace_id=workspace_name_or_id, timeout=timeout
+            )
         except HTTPError:
             # check to see if they provided the workspace name instead
-            ws_list = self.api.get_workspaces()
+            ws_list = self.api.get_workspaces(timeout=timeout)
             for ws in ws_list:
                 if ws["name"] == workspace_name_or_id:
                     return ws
 
-    def get_workspace_info(self, known_workspace_topic: Optional[str] = None):
+    def get_workspace_info(
+        self,
+        known_workspace_topic: Optional[str] = None,
+        timeout: Optional[float] = None,
+    ):
         """
         Queries for workspace data from the Quix API, regardless of instance cache,
         and updates instance attributes from query result.
 
         :param known_workspace_topic: a topic you know to exist in some workspace
+        :param timeout: response timeout (seconds); Default 30
         """
         # TODO: more error handling with the wrong combo of ws_id and topic
         if self._workspace_id:
-            ws_data = self.search_for_workspace(workspace_name_or_id=self._workspace_id)
+            ws_data = self.search_for_workspace(
+                workspace_name_or_id=self._workspace_id, timeout=timeout
+            )
         else:
-            ws_data = self.search_for_topic_workspace(known_workspace_topic)
+            ws_data = self.search_for_topic_workspace(
+                known_workspace_topic, timeout=timeout
+            )
         if not ws_data:
             raise NoWorkspaceFound(
                 "No workspace was found for the given workspace/auth-token/topic combo"
             )
         self._workspace_id = ws_data.pop("workspaceId")
         self._quix_broker_config = ws_data.pop("broker")
         try:
@@ -248,76 +268,92 @@
             self._quix_broker_settings = {
                 "brokerType": ws_data["brokerType"],
                 "syncTopics": False,
             }
         self._workspace_meta = ws_data
 
     def search_workspace_for_topic(
-        self, workspace_id: str, topic: str
+        self, workspace_id: str, topic: str, timeout: Optional[float] = None
     ) -> Optional[str]:
         """
         Search through all the topics in the given workspace id to see if there is a
         match with the provided topic.
 
         :param workspace_id: the workspace to search in
         :param topic: the topic to search for
+        :param timeout: response timeout (seconds); Default 30
+
         :return: the workspace_id if success, else None
         """
-        topics = self.api.get_topics(workspace_id=workspace_id)
+        topics = self.api.get_topics(
+            workspace_id=workspace_id,
+            timeout=timeout if timeout is not None else self._timeout,
+        )
         for t in topics:
             if t["name"] == topic or t["id"] == topic:
                 return workspace_id
 
-    def search_for_topic_workspace(self, topic: str) -> Optional[dict]:
+    def search_for_topic_workspace(
+        self, topic: str, timeout: Optional[float] = None
+    ) -> Optional[dict]:
         """
         Find what workspace a topic belongs to.
         If there is only one workspace altogether, it is assumed to be the workspace.
         More than one means each workspace will be searched until the first hit.
 
         :param topic: the topic to search for
+        :param timeout: response timeout (seconds); Default 30
+
         :return: workspace data dict if topic search success, else None
         """
-        ws_list = self.api.get_workspaces()
+        ws_list = self.api.get_workspaces(
+            timeout=timeout if timeout is not None else self._timeout
+        )
         if len(ws_list) == 1:
             return ws_list[0]
         if topic is None:
             raise MultipleWorkspaces(
                 "More than 1 workspace was found, so you must provide a topic name "
                 "to find the correct workspace"
             )
         for ws in ws_list:
-            if self.search_workspace_for_topic(ws["workspaceId"], topic):
+            if self.search_workspace_for_topic(
+                ws["workspaceId"], topic, timeout=timeout
+            ):
                 return ws
 
     def get_workspace_ssl_cert(
-        self, extract_to_folder: Optional[Path] = None
+        self, extract_to_folder: Optional[Path] = None, timeout: Optional[float] = None
     ) -> Optional[str]:
         """
         Gets and extracts zipped certificate from the API to provided folder if the
         SSL certificate is specified in broker configuration.
 
         If no path was provided, will dump to /tmp. Expects cert named 'ca.cert'.
 
         :param extract_to_folder: path to folder to dump zipped cert file to
+        :param timeout: response timeout (seconds); Default 30
+
         :return: full cert filepath as string or `None` if certificate is not specified
         """
         certificate_bytes = self.api.get_workspace_certificate(
-            workspace_id=self._workspace_id
+            workspace_id=self._workspace_id,
+            timeout=timeout if timeout is not None else self._timeout,
         )
         if certificate_bytes is None:
             return
         extract_to_folder = extract_to_folder or Path(gettempdir())
         full_path = extract_to_folder / "ca.cert"
         if not full_path.is_file():
             extract_to_folder.mkdir(parents=True, exist_ok=True)
             with open(full_path, "wb") as f:
                 f.write(certificate_bytes)
         return full_path.as_posix()
 
-    def _create_topic(self, topic: Topic):
+    def _create_topic(self, topic: Topic, timeout: Optional[float] = None):
         """
         The actual API call to create the topic
 
         :param topic: a Topic instance
         """
         topic_name = self.strip_workspace_id_prefix(topic.name)
         cfg = topic.config
@@ -331,36 +367,46 @@
             topic_name=topic_name,
             workspace_id=self.workspace_id,
             topic_partitions=cfg.num_partitions,
             topic_rep_factor=cfg.replication_factor,
             topic_ret_bytes=ret_bytes if ret_bytes is None else int(ret_bytes),
             topic_ret_minutes=ret_ms if ret_ms is None else int(ret_ms) // 60000,
             cleanup_policy=cfg.extra_config.get("cleanup.policy"),
+            timeout=timeout if timeout is not None else self._timeout,
         )
         logger.info(
             f"Creation of topic {topic_name} acknowledged by broker. Must wait "
             f"for 'Ready' status before topic is actually available"
         )
 
-    def _finalize_create(self, topics: Set[str], timeout: Optional[int] = None):
+    def _finalize_create(
+        self,
+        topics: Set[str],
+        timeout: Optional[float] = None,
+        finalize_timeout: Optional[float] = None,
+    ):
         """
         After the broker acknowledges the topics are created, they will be in a
         "Creating", and will not be ready to consume from/produce to until they are
         set to a status of "Ready". This will block until all topics passed are marked
         as "Ready" or the timeout is hit.
 
         :param topics: set of topic names
-        :param timeout: amount of seconds allowed to finalize, else raise exception
+        :param finalize_timeout: topic finalization timeout (seconds); Default 60
         """
         exceptions = {}
-        stop_time = time.time() + (timeout or len(topics) * 30)
+        stop_time = time.time() + (
+            finalize_timeout if finalize_timeout else self._topic_create_timeout
+        )
         while topics and time.time() < stop_time:
             # Each topic seems to take 10-15 seconds each to finalize (at least in dev)
             time.sleep(1)
-            for topic in [t for t in self.get_topics() if t["id"] in topics.copy()]:
+            for topic in [
+                t for t in self.get_topics(timeout=timeout) if t["id"] in topics.copy()
+            ]:
                 if topic["status"] == "Ready":
                     logger.debug(f"Topic {topic['name']} creation finalized")
                     topics.remove(topic["id"])
                 elif topic["status"] == "Error":
                     logger.debug(f"Topic {topic['name']} encountered an error")
                     exceptions[topic["name"]] = topic["lastError"]
                     topics.remove(topic["id"])
@@ -371,38 +417,40 @@
                 f"Creation succeeded, but waiting for 'Ready' status timed out "
                 f"for topics: {[self.strip_workspace_id_prefix(t) for t in topics]}"
             )
 
     def create_topics(
         self,
         topics: List[Topic],
-        finalize_timeout_seconds: Optional[int] = None,
+        timeout: Optional[float] = None,
+        finalize_timeout: Optional[float] = None,
     ):
         """
         Create topics in a Quix cluster.
 
         :param topics: a list of `Topic` objects
-        :param finalize_timeout_seconds: How long to wait for the topics to be
+        :param timeout: response timeout (seconds); Default 30
+        :param finalize_timeout: topic finalization timeout (seconds); Default 60
         marked as "Ready" (and thus ready to produce to/consume from).
         """
         logger.info("Attempting to create topics...")
-        current_topics = {t["id"]: t for t in self.get_topics()}
+        current_topics = {t["id"]: t for t in self.get_topics(timeout=timeout)}
         finalize = set()
         for topic in topics:
             topic_name = self.prepend_workspace_id(topic.name)
             exists = self.prepend_workspace_id(topic_name) in current_topics
             if not exists or current_topics[topic_name]["status"] != "Ready":
                 if exists:
                     logger.debug(
                         f"Topic {self.strip_workspace_id_prefix(topic_name)} exists but does "
                         f"not have 'Ready' status. Added to finalize check."
                     )
                 else:
                     try:
-                        self._create_topic(topic)
+                        self._create_topic(topic, timeout=timeout)
                     # TODO: more robust error handling to better identify issues
                     # See how it's handled in the admin client and maybe consolidate
                     # logic via TopicManager
                     except HTTPError as e:
                         # Topic was maybe created by another instance
                         if "already exists" not in e.response.text:
                             raise
@@ -412,15 +460,17 @@
                     f"Topic {self.strip_workspace_id_prefix(topic_name)} exists and is Ready"
                 )
         logger.info(
             "Topic creations acknowledged; waiting for 'Ready' statuses..."
             if finalize
             else "No topic creations required!"
         )
-        self._finalize_create(finalize, timeout=finalize_timeout_seconds)
+        self._finalize_create(
+            finalize, timeout=timeout, finalize_timeout=finalize_timeout
+        )
 
     def get_topic(self, topic_name: str) -> Optional[dict]:
         """
         return the topic ID (the actual cluster topic name) if it exists, else None
 
         >***NOTE***: if the name registered in Quix is instead the workspace-prefixed
         version, this returns None unless that exact name was created WITHOUT the
@@ -433,37 +483,43 @@
         try:
             return self.api.get_topic(topic_name, workspace_id=self.workspace_id)
         except QuixApiRequestFailure as e:
             if e.status_code == 404:
                 return
             raise
 
-    def get_topics(self) -> List[dict]:
-        return self.api.get_topics(workspace_id=self.workspace_id)
+    def get_topics(self, timeout: Optional[float] = None) -> List[dict]:
+        return self.api.get_topics(
+            workspace_id=self.workspace_id,
+            timeout=timeout if timeout is not None else self._timeout,
+        )
 
-    def confirm_topics_exist(self, topics: Union[List[Topic], List[str]]):
+    def confirm_topics_exist(
+        self, topics: Union[List[Topic], List[str]], timeout: Optional[float] = None
+    ):
         """
         Confirm whether the desired set of topics exists in the Quix workspace.
 
         :param topics: a list of `Topic` or topic names
+        :param timeout: response timeout (seconds); Default 30
         """
         if isinstance(topics[0], Topic):
             topics = [topic.name for topic in topics]
         logger.info("Confirming required topics exist...")
-        current_topics = [t["id"] for t in self.get_topics()]
+        current_topics = [t["id"] for t in self.get_topics(timeout=timeout)]
         missing_topics = []
         for name in topics:
             if name not in current_topics:
                 missing_topics.append(self.strip_workspace_id_prefix(name))
             else:
                 logger.debug(f"Topic {self.strip_workspace_id_prefix(name)} confirmed!")
         if missing_topics:
             raise MissingQuixTopics(f"Topics do no exist: {missing_topics}")
 
-    def _set_workspace_cert(self) -> str:
+    def _set_workspace_cert(self, timeout: Optional[float] = None) -> str:
         """
         Will create a cert and assigns it to the workspace_cert_path property.
         If there was no path provided at init, one is generated based on the cwd and
         workspace_id.
         Used by this class when generating configs (does some extra folder and class
         config stuff that's not needed if you just want to get the cert only)
 
@@ -472,31 +528,35 @@
         if self._workspace_cert_path:
             folder = Path(self._workspace_cert_path)
             if folder.name.endswith("cert"):
                 folder = folder.parent
         else:
             folder = Path(getcwd()) / "certificates" / self.workspace_id
         self._workspace_cert_path = self.get_workspace_ssl_cert(
-            extract_to_folder=folder
+            extract_to_folder=folder, timeout=timeout
         )
         return self._workspace_cert_path
 
-    def get_confluent_broker_config(self, known_topic: Optional[str] = None) -> dict:
+    def get_confluent_broker_config(
+        self, known_topic: Optional[str] = None, timeout: Optional[float] = None
+    ) -> dict:
         """
         Get the full client config dictionary required to authenticate a confluent-kafka
         client to a Quix platform broker/workspace.
 
         The returned config can be used directly by any confluent-kafka-python consumer/
         producer (add your producer/consumer-specific configs afterward).
 
         :param known_topic: a topic known to exist in some workspace
+        :param timeout: response timeout (seconds); Default 30
+
         :return: a dict of confluent-kafka-python client settings (see librdkafka
         config for more details)
         """
-        self.get_workspace_info(known_workspace_topic=known_topic)
+        self.get_workspace_info(known_workspace_topic=known_topic, timeout=timeout)
         cfg_out = {}
         for quix_param_name, rdkafka_param_name in _QUIX_PARAMS_NAMES_MAP.items():
             # Map broker config received from Quix to librdkafka format
             param_value = self.quix_broker_config[quix_param_name]
 
             # Also map values of "security.protocol" and "sasl.mechanisms" from Quix
             # to librdkafka format
@@ -522,28 +582,33 @@
         # `extra_consumer_config` or `extra_producer_config` parameters.
         cfg_out["connections.max.idle.ms"] = QUIX_CONNECTIONS_MAX_IDLE_MS
         cfg_out["metadata.max.age.ms"] = QUIX_METADATA_MAX_AGE_MS
         self._confluent_broker_config = cfg_out
         return self._confluent_broker_config
 
     def get_confluent_client_configs(
-        self, topics: list, consumer_group_id: Optional[str] = None
+        self,
+        topics: list,
+        consumer_group_id: Optional[str] = None,
+        timeout: Optional[float] = None,
     ) -> Tuple[dict, List[str], Optional[str]]:
         """
         Get all the values you need in order to use a confluent_kafka-based client
         with a topic on a Quix platform broker/workspace.
 
         The returned config can be used directly by any confluent-kafka-python consumer/
         producer (add your producer/consumer-specific configs afterward).
 
         The topics and consumer group are appended with any necessary values.
 
         :param topics: list of topics
         :param consumer_group_id: consumer group id, if needed
+        :param timeout: response timeout (seconds); Default 30
+
         :return: a tuple with configs and altered versions of the topics
         and consumer group name
         """
         return (
-            self.get_confluent_broker_config(topics[0]),
+            self.get_confluent_broker_config(topics[0], timeout=timeout),
             [self.prepend_workspace_id(t) for t in topics],
             self.prepend_workspace_id(consumer_group_id) if consumer_group_id else None,
         )
```

## quixstreams/platforms/quix/topic_manager.py

```diff
@@ -15,49 +15,60 @@
     Generally initialized and managed automatically by an `Application.Quix`,
     but allows a user to work with it directly when needed, such as using it alongside
     a plain `Producer` to create its topics.
 
     See methods for details.
     """
 
-    # Setting it to None to use defaults defined in Quix Cloud
-    _topic_replication = None
+    # Default topic params
+    # Set these to None to use defaults defined in Quix Cloud
+    default_num_partitions = None
+    default_replication_factor = None
+
+    # Max topic name length for the new topics
     _max_topic_name_len = 249
 
     def __init__(
         self,
         topic_admin: TopicAdmin,
         consumer_group: str,
         quix_config_builder: QuixKafkaConfigsBuilder,
-        create_timeout: int = 60,
+        timeout: float = 30,
+        create_timeout: float = 60,
     ):
         """
         :param topic_admin: an `Admin` instance
-        :param create_timeout: timeout for topic creation
         :param quix_config_builder: A QuixKafkaConfigsBuilder instance, else one is
             generated for you.
+        :param timeout: response timeout (seconds)
+        :param create_timeout: timeout for topic creation
         """
         super().__init__(
             topic_admin=topic_admin,
             consumer_group=quix_config_builder.strip_workspace_id_prefix(
                 consumer_group
             ),
+            timeout=timeout,
             create_timeout=create_timeout,
         )
         self._quix_config_builder = quix_config_builder
 
-    def _create_topics(self, topics: List[Topic]):
+    def _create_topics(
+        self, topics: List[Topic], timeout: float, create_timeout: float
+    ):
         """
         Method that actually creates the topics in Kafka via the
         QuixConfigBuilder instance.
 
         :param topics: list of `Topic`s
+        :param timeout: creation acknowledge timeout (seconds)
+        :param create_timeout: topic finalization timeout (seconds)
         """
         self._quix_config_builder.create_topics(
-            topics, finalize_timeout_seconds=self._create_timeout
+            topics, timeout=timeout, finalize_timeout=create_timeout
         )
 
     def _resolve_topic_name(self, name: str) -> str:
         """
         Checks if provided topic name is registered via Quix API; if yes,
         return its corresponding topic ID (AKA the actual topic name, usually
         just has prepended workspace ID).
```

## Comparing `quixstreams-2.5.0.dist-info/LICENSE` & `quixstreams-2.5.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `quixstreams-2.5.0.dist-info/METADATA` & `quixstreams-2.5.1.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: quixstreams
-Version: 2.5.0
+Version: 2.5.1
 Summary: Python library for building stream processing applications with Apache Kafka
 Author-email: Quix Analytics Ltd <devs@quix.io>
 License: Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -214,15 +214,15 @@
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Programming Language :: Python :: 3
 Requires-Python: <4,>=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: confluent-kafka <2.4,>=2.2
-Requires-Dist: requests >=2.28
+Requires-Dist: requests <2.32,>=2.28
 Requires-Dist: rocksdict <0.4,>=0.3
 Requires-Dist: typing-extensions <4.9,>=4.8
 Requires-Dist: orjson <4,>=3.9
 
 ![Quix - React to data, fast](https://github.com/quixio/quix-streams/blob/main/images/quixstreams-banner.jpg)
 
 [![Docs](https://img.shields.io/badge/-Docs-red?logo=read-the-docs)](https://quix.io/docs/quix-streams/introduction.html)
@@ -405,17 +405,15 @@
 
 ## Need help?
 
 If you run into any problems, please create an [issue](https://github.com/quixio/quix-streams/issues) or ask in #quix-help in our [Quix Community on Slack](https://quix.io/slack-invite).  
 
 ## Community 👭
 
-Join other software engineers in [The Stream](https://quix.io/slack-invite), an online community of people interested in all things data streaming. This is a space to both listen to and share learnings.
-
-🙌  [Join our Slack community!](https://quix.io/slack-invite)
+Join the [Quix Community on Slack](https://quix.io/slack-invite), a vibrant group of Python developers, data enthusiasts and newcomers to Apache Kafka, who are learning and leveraging Quix Streams for real-time data processing.
 
 ## License
 
 Quix Streams is licensed under the Apache 2.0 license. View a copy of the License file [here](https://github.com/quixio/quix-streams/blob/main/LICENSE).
 
 ## Stay in touch 👋
```

## Comparing `quixstreams-2.5.0.dist-info/RECORD` & `quixstreams-2.5.1.dist-info/RECORD`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-quixstreams/__init__.py,sha256=xq0M3dLpKlemc8wozgeV67uFUMAvD4XphsG9UBtqdAE,163
-quixstreams/app.py,sha256=P2SJWq8ePzO0DdfG_73TX6qJj7Moq9HgKGRsSELJBfo,36819
+quixstreams/__init__.py,sha256=ji1HyYGAu90sSLpW3mDwHTbRd-Qd7EYx7MKzkvej21k,163
+quixstreams/app.py,sha256=G8Ef1yoSdKBAbF0nTVrOCQTC-O4OQXFISzklsUlgTZ0,37557
 quixstreams/context.py,sha256=Q7SnL6bKMrcDzE-FQfpcchuheQFZ2HlBmqGCVgiWrDU,2559
 quixstreams/error_callbacks.py,sha256=RLvvlEK0hq_XtI-irJiNJf9sEFx9K1QCFZXx_3CMbWU,1561
 quixstreams/logging.py,sha256=b_M0GPJd3vjXWquqK0tsZkn69j-RcSikAzV1yTDFFIo,1409
 quixstreams/processing_context.py,sha256=U57NMvOIJU1zkwJlluDHCknIHAUDENAQAVLLs5hIIwg,2512
 quixstreams/rowconsumer.py,sha256=hwez_sZ3DPlgtglbTOCMP0WhjcrPOQxXiNnmVERamkU,6262
 quixstreams/rowproducer.py,sha256=UcV6tFgKBc7Uexy1P1eXaGWizYdJyC5FuSPqUY8dqPs,5610
 quixstreams/types.py,sha256=xX1rTm6VaaPABGo1KkDlaaw-t45ssEHX3oGkW9X8Tp4,112
@@ -40,26 +40,26 @@
 quixstreams/models/serializers/__init__.py,sha256=CUQhMRYQQZj1vCPJPnbO98vdxJbTLTc7YTXuNGaoCp4,1016
 quixstreams/models/serializers/base.py,sha256=_6J2Cgj9TF_TMTmNua4H2H5PFREd6RYlCt9ZrL_yXKI,3157
 quixstreams/models/serializers/exceptions.py,sha256=Zd3LNu0bMJNBKSrGX0bniy-Sh5Mist6n5jijalP2joU,1370
 quixstreams/models/serializers/json.py,sha256=z3LkGVVNJ7QdTwWCRFwYdXiDbggwMBhLDRQHG85-YbE,1936
 quixstreams/models/serializers/quix.py,sha256=pmuHIU_TzEOX2wEcGypwNJeMY7c0IWpauw84Ua_2wFo,16878
 quixstreams/models/serializers/simple_types.py,sha256=6mlxn4F-7S8KcDiiECyICykm-Zi2I5m7CqzH8JPAAa8,4516
 quixstreams/models/topics/__init__.py,sha256=6bi-O9GMdc08ouk5RCOPClwg8WCIeXwPo0tTmoDC00c,65
-quixstreams/models/topics/admin.py,sha256=XRCU3L9_6uex0CzHHhCUYZbZV7-BAtWaWfLC65WGkFM,6566
+quixstreams/models/topics/admin.py,sha256=r5mZobqdv-iWKgK0XR1njOg1H84VZqzvojIZ88s58uw,7042
 quixstreams/models/topics/exceptions.py,sha256=BmJQiWvxIdhr15IKov_oDB1uKtxGP3Rarrqc2Lgnjwk,297
-quixstreams/models/topics/manager.py,sha256=BumRZtCcLdSgcHvEZ2fGpYJ7PP3WyVqFkqHKjklCMhk,15725
-quixstreams/models/topics/topic.py,sha256=usXf5cQlVQhCG9vgxLlVwZkx1WZ0obeltGMhsKzXr5Q,10343
+quixstreams/models/topics/manager.py,sha256=pWfIAAph0NoZsaKZMWLoCNow1f9pf4kM39k6zpDAB5E,17593
+quixstreams/models/topics/topic.py,sha256=Bd4odTuKJYi6kyZ6guHN40ZtU5RMhKMjQ65cxk5NmxU,10334
 quixstreams/platforms/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 quixstreams/platforms/quix/__init__.py,sha256=pbNOUeJoyKUhv59fXCsQanwoEXTHcVtfduvVOLT3W3A,118
-quixstreams/platforms/quix/api.py,sha256=ITLLr9yRnU9ZL5sWQvVzQ3XY4ZLCly5LCJ_rYldw3zI,6306
+quixstreams/platforms/quix/api.py,sha256=wJKKwSVE6U5uo_lbhsmWsoPNK1AwlIyrEUR5G2EhRQM,6845
 quixstreams/platforms/quix/checks.py,sha256=2aYCmnDaGr57jjBarrDVRIfNbZ0ufbjxXdQw5qwqZgE,1531
-quixstreams/platforms/quix/config.py,sha256=HJGlpZWrlZspxnGPQgJ0ONivVgDnPbCP0K9esRruspg,22216
+quixstreams/platforms/quix/config.py,sha256=XAjiWL-nyPmfyOXaqklX4tq4Q3q5DpU0qrPeyp4gPOw,24474
 quixstreams/platforms/quix/env.py,sha256=A8vdwz1E5obuUp4c9qCCuLRDixI9LnXj_SsdmFSUbjQ,1773
 quixstreams/platforms/quix/exceptions.py,sha256=Modb_xU2PegEYpmxz47gRxqqfJ38H3oUFHzhOyEE9XU,1035
-quixstreams/platforms/quix/topic_manager.py,sha256=vJhQstGhrmLtQVHkT--WB6IJWY-F7AB3mtUzGgY80Uw,3329
+quixstreams/platforms/quix/topic_manager.py,sha256=JfpyblWp9EzCFnxNsfTuwgxUR9QXkNjL7Wbyich-w9E,3743
 quixstreams/state/__init__.py,sha256=sXQzET5v9-J9nI-1WLwtX2_N52nE9YANsYnSRMbHgLM,68
 quixstreams/state/exceptions.py,sha256=dOuwOU3siwFvMAQQo8RAr8TzeyL62ugpgQfBj2jlvvk,381
 quixstreams/state/manager.py,sha256=ONHGEDDAPvd3lfsq8rShyuBWyr7G_uEL6xE3cqUuZ1Y,8556
 quixstreams/state/recovery.py,sha256=oC8al25iSAONX2rLJ2_46-8NWeTpYyAs7qkNKl8sFM4,14923
 quixstreams/state/state.py,sha256=BPSiR8rm91TKRV2LbpakYZdqxM5_TabrJqFWzzIlakI,1672
 quixstreams/state/types.py,sha256=yEeK5B6623_7S0jpN7qXtnn0EGy2WDSgmMMo7WGDBNg,15308
 quixstreams/state/rocksdb/__init__.py,sha256=QtR97lmKpIo0PlgrnOit48tACX3rl5Aw68KfwpP9axc,143
@@ -77,12 +77,12 @@
 quixstreams/state/rocksdb/windowed/serialization.py,sha256=SznE2hw0DF3VXroXkM5m6xaFHsJKWvh-otbFhO22ILw,2014
 quixstreams/state/rocksdb/windowed/state.py,sha256=LHCqmbnMF7MzztJStfsNbj_rFsE7uCk2lZXs2ytvdfQ,3057
 quixstreams/state/rocksdb/windowed/store.py,sha256=zOabG0PwfkRhWWNjasrAI_tD4k_yLPrqD1qVbTPuD1M,2107
 quixstreams/state/rocksdb/windowed/transaction.py,sha256=kw26tmgbkSU8F8lTRi7lqAQtbIiIS49R-fQeBKRwhpA,8294
 quixstreams/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 quixstreams/utils/dicts.py,sha256=Yc8l-johZzEuWIm7vzmTwYXLj19MoK_0pqWMII_F11U,607
 quixstreams/utils/json.py,sha256=wBtRWEvo2nVfwFIsZVbNpr2hOoF4TuXnnNdcF24zow0,636
-quixstreams-2.5.0.dist-info/LICENSE,sha256=P12cSoW9dae7S1OZMTZLKVwDUvS8UYdAnVwSYrUmDRE,11353
-quixstreams-2.5.0.dist-info/METADATA,sha256=5L1ncRP7k5EDOHjcJZ3FFg3S2u8xTpYoJs31-DPVuhA,22419
-quixstreams-2.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-quixstreams-2.5.0.dist-info/top_level.txt,sha256=Vk4q2NfXCIP82ZaTX5k5e6e9ao3IEvYPzquAl-1C7vo,12
-quixstreams-2.5.0.dist-info/RECORD,,
+quixstreams-2.5.1.dist-info/LICENSE,sha256=P12cSoW9dae7S1OZMTZLKVwDUvS8UYdAnVwSYrUmDRE,11353
+quixstreams-2.5.1.dist-info/METADATA,sha256=XCc0iEksWbZFmYxiSk65EuNDeWHmkutU00u-beqYYcY,22385
+quixstreams-2.5.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+quixstreams-2.5.1.dist-info/top_level.txt,sha256=Vk4q2NfXCIP82ZaTX5k5e6e9ao3IEvYPzquAl-1C7vo,12
+quixstreams-2.5.1.dist-info/RECORD,,
```

