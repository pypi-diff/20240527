# Comparing `tmp/ragfmk-0.1.3.1-py3-none-any.whl.zip` & `tmp/ragfmk-0.1.4.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,38 +1,41 @@
-Zip file size: 29946 bytes, number of entries: 36
--rw-rw-rw-  2.0 fat     3622 b- defN 24-Apr-30 09:43 RagAdhocQueryDoc.py
--rw-rw-rw-  2.0 fat     1694 b- defN 24-Apr-30 09:42 RagPrompt.py
+Zip file size: 32679 bytes, number of entries: 39
+-rw-rw-rw-  2.0 fat     3665 b- defN 24-May-27 13:59 RagAdhocQueryDoc.py
+-rw-rw-rw-  2.0 fat     1735 b- defN 24-May-27 09:00 RagPrompt.py
+-rw-rw-rw-  2.0 fat     2233 b- defN 24-May-27 08:59 RagTest.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 __init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/__init__.py
--rw-rw-rw-  2.0 fat     9013 b- defN 24-Apr-30 09:43 ragfmk/rag.py
--rw-rw-rw-  2.0 fat     2641 b- defN 24-Apr-30 13:30 ragfmk/ragChromaDB.py
--rw-rw-rw-  2.0 fat     3720 b- defN 24-Apr-30 07:36 ragfmk/ragFAISS.py
+-rw-rw-rw-  2.0 fat     8274 b- defN 24-May-27 14:06 ragfmk/rag.py
+-rw-rw-rw-  2.0 fat     2645 b- defN 24-May-27 08:59 ragfmk/ragChromaDB.py
+-rw-rw-rw-  2.0 fat     3744 b- defN 24-May-27 08:59 ragfmk/ragFAISS.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/elements/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/embeddings/__init__.py
--rw-rw-rw-  2.0 fat     3623 b- defN 24-Apr-30 07:34 ragfmk/elements/embeddings/embeddings.py
--rw-rw-rw-  2.0 fat     1226 b- defN 24-Apr-29 15:51 ragfmk/elements/embeddings/stEmbeddings.py
+-rw-rw-rw-  2.0 fat     1586 b- defN 24-May-08 13:35 ragfmk/elements/embeddings/embedding.py
+-rw-rw-rw-  2.0 fat     3333 b- defN 24-May-08 16:07 ragfmk/elements/embeddings/embeddings.py
+-rw-rw-rw-  2.0 fat     2528 b- defN 24-May-08 15:44 ragfmk/elements/embeddings/ollamaEmbeddings.py
+-rw-rw-rw-  2.0 fat     1833 b- defN 24-May-08 14:39 ragfmk/elements/embeddings/stEmbeddings.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/llms/__init__.py
--rw-rw-rw-  2.0 fat     1245 b- defN 24-Apr-27 09:28 ragfmk/elements/llms/ollama.py
--rw-rw-rw-  2.0 fat     3400 b- defN 24-Apr-30 13:29 ragfmk/elements/simsearchengines/ChromaDBWrapper.py
--rw-rw-rw-  2.0 fat     4595 b- defN 24-Apr-29 15:26 ragfmk/elements/simsearchengines/FAISSWrapper.py
+-rw-rw-rw-  2.0 fat     1233 b- defN 24-May-27 13:48 ragfmk/elements/llms/ollama.py
+-rw-rw-rw-  2.0 fat     3390 b- defN 24-May-08 16:02 ragfmk/elements/simsearchengines/ChromaDBWrapper.py
+-rw-rw-rw-  2.0 fat     4814 b- defN 24-May-06 08:12 ragfmk/elements/simsearchengines/FAISSWrapper.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/elements/simsearchengines/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Dec-21 09:21 ragfmk/elements/wrappers/__init__.py
--rw-rw-rw-  2.0 fat     3237 b- defN 24-Apr-30 06:37 ragfmk/elements/wrappers/chunks.py
--rw-rw-rw-  2.0 fat     7605 b- defN 24-Apr-30 06:46 ragfmk/elements/wrappers/document.py
--rw-rw-rw-  2.0 fat     2816 b- defN 24-Apr-30 06:52 ragfmk/elements/wrappers/nearest.py
--rw-rw-rw-  2.0 fat     1199 b- defN 24-Apr-29 15:50 ragfmk/elements/wrappers/prompt.py
--rw-rw-rw-  2.0 fat      759 b- defN 24-Apr-30 06:50 ragfmk/interfaces/IChunks.py
--rw-rw-rw-  2.0 fat      432 b- defN 24-Apr-30 06:50 ragfmk/interfaces/IDocument.py
--rw-rw-rw-  2.0 fat      775 b- defN 24-Apr-30 07:33 ragfmk/interfaces/IEmbeddings.py
--rw-rw-rw-  2.0 fat      847 b- defN 24-Apr-30 06:52 ragfmk/interfaces/INearest.py
--rw-rw-rw-  2.0 fat     1244 b- defN 24-Apr-30 09:40 ragfmk/interfaces/IRag.py
--rw-rw-rw-  2.0 fat     1697 b- defN 24-Apr-30 09:30 ragfmk/utils/CONST.py
+-rw-rw-rw-  2.0 fat     3203 b- defN 24-May-08 13:09 ragfmk/elements/wrappers/chunks.py
+-rw-rw-rw-  2.0 fat     7519 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/document.py
+-rw-rw-rw-  2.0 fat     3128 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/nearest.py
+-rw-rw-rw-  2.0 fat     1850 b- defN 24-May-06 08:12 ragfmk/elements/wrappers/prompt.py
+-rw-rw-rw-  2.0 fat      740 b- defN 24-May-01 15:19 ragfmk/interfaces/IChunks.py
+-rw-rw-rw-  2.0 fat      425 b- defN 24-May-01 15:19 ragfmk/interfaces/IDocument.py
+-rw-rw-rw-  2.0 fat      916 b- defN 24-May-08 16:07 ragfmk/interfaces/IEmbeddings.py
+-rw-rw-rw-  2.0 fat      895 b- defN 24-May-02 09:04 ragfmk/interfaces/INearest.py
+-rw-rw-rw-  2.0 fat      581 b- defN 24-May-01 15:19 ragfmk/interfaces/IPrompt.py
+-rw-rw-rw-  2.0 fat     1061 b- defN 24-May-27 10:00 ragfmk/interfaces/IRag.py
+-rw-rw-rw-  2.0 fat     1842 b- defN 24-May-27 14:05 ragfmk/utils/CONST.py
 -rw-rw-rw-  2.0 fat        0 b- defN 24-Mar-25 16:16 ragfmk/utils/__init__.py
--rw-rw-rw-  2.0 fat     1864 b- defN 24-Apr-29 06:35 ragfmk/utils/log.py
--rw-rw-rw-  2.0 fat     1907 b- defN 24-Apr-22 13:53 ragfmk/utils/milestone.py
--rw-rw-rw-  2.0 fat     1091 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     8549 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       86 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       43 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3041 b- defN 24-Apr-30 13:31 ragfmk-0.1.3.1.dist-info/RECORD
-36 files, 72063 bytes uncompressed, 25034 bytes compressed:  65.3%
+-rw-rw-rw-  2.0 fat     2097 b- defN 24-May-27 08:57 ragfmk/utils/trace.py
+-rw-rw-rw-  2.0 fat     1091 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     8713 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       86 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       51 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     3311 b- defN 24-May-27 14:15 ragfmk-0.1.4.9.dist-info/RECORD
+39 files, 78614 bytes uncompressed, 27339 bytes compressed:  65.2%
```

## zipnote {}

```diff
@@ -1,13 +1,16 @@
 Filename: RagAdhocQueryDoc.py
 Comment: 
 
 Filename: RagPrompt.py
 Comment: 
 
+Filename: RagTest.py
+Comment: 
+
 Filename: __init__.py
 Comment: 
 
 Filename: ragfmk/__init__.py
 Comment: 
 
 Filename: ragfmk/rag.py
@@ -21,17 +24,23 @@
 
 Filename: ragfmk/elements/__init__.py
 Comment: 
 
 Filename: ragfmk/elements/embeddings/__init__.py
 Comment: 
 
+Filename: ragfmk/elements/embeddings/embedding.py
+Comment: 
+
 Filename: ragfmk/elements/embeddings/embeddings.py
 Comment: 
 
+Filename: ragfmk/elements/embeddings/ollamaEmbeddings.py
+Comment: 
+
 Filename: ragfmk/elements/embeddings/stEmbeddings.py
 Comment: 
 
 Filename: ragfmk/elements/llms/__init__.py
 Comment: 
 
 Filename: ragfmk/elements/llms/ollama.py
@@ -69,41 +78,41 @@
 
 Filename: ragfmk/interfaces/IEmbeddings.py
 Comment: 
 
 Filename: ragfmk/interfaces/INearest.py
 Comment: 
 
+Filename: ragfmk/interfaces/IPrompt.py
+Comment: 
+
 Filename: ragfmk/interfaces/IRag.py
 Comment: 
 
 Filename: ragfmk/utils/CONST.py
 Comment: 
 
 Filename: ragfmk/utils/__init__.py
 Comment: 
 
-Filename: ragfmk/utils/log.py
-Comment: 
-
-Filename: ragfmk/utils/milestone.py
+Filename: ragfmk/utils/trace.py
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/LICENSE
+Filename: ragfmk-0.1.4.9.dist-info/LICENSE
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/METADATA
+Filename: ragfmk-0.1.4.9.dist-info/METADATA
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/WHEEL
+Filename: ragfmk-0.1.4.9.dist-info/WHEEL
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/entry_points.txt
+Filename: ragfmk-0.1.4.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/top_level.txt
+Filename: ragfmk-0.1.4.9.dist-info/top_level.txt
 Comment: 
 
-Filename: ragfmk-0.1.3.1.dist-info/RECORD
+Filename: ragfmk-0.1.4.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## RagAdhocQueryDoc.py

```diff
@@ -1,10 +1,12 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
+import sys
+sys.path.append("./")
 
 import argparse
 from ragfmk.ragFAISS import ragFAISS
 import ragfmk.utils.CONST as C
 from ragfmk.elements.wrappers.chunks import chunks
 
 """
@@ -58,15 +60,15 @@
         myRag.addEmbeddings(vChunks)
         # 6 - Similarity Search
         myRag.initSearchEngine()
         similars = myRag.processSearch(args[ARG_NEAREST[0]], vPrompt)
         # 7 - Build prompt
         customPrompt = myRag.buildPrompt(args[ARG_PROMPT[0]], similars)
         # 8 - Ask to the LLM ...
-        resp = myRag.promptLLM(customPrompt, args[ARG_URL[0]], args[ARG_MODEL[0]], args[ARG_TEMP[0]])
+        resp, tokens = myRag.promptLLM(customPrompt, args[ARG_URL[0]], args[ARG_MODEL[0]], args[ARG_TEMP[0]])
     
         print(resp)
 
     except Exception as e:
         parser.print_help()
         print(e)
```

## RagPrompt.py

```diff
@@ -1,13 +1,16 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
+import sys
+sys.path.append("./")
+
 import argparse
-from ragfmk.rag import rag
+from src.ragfmk.rag import rag
 import ragfmk.utils.CONST as C
 from ragfmk.elements.wrappers.nearest import nearest
 
 ARG_PROMPT = ["prompt", "Prompt to send to the LLM"]
 ARG_PROMPT_TEMPLATE = ["template", "Prompt template to use when building the prompt (must contain {prompt} and {context}"]
 ARG_NEARESTFILE = ["nfile", "JSON file path which contains the nearest chunks/texts"]
```

## ragfmk/rag.py

```diff
@@ -1,38 +1,26 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 from ragfmk.elements.wrappers.document import document
 from ragfmk.elements.llms.ollama import ollama
 from ragfmk.elements.wrappers.prompt import prompt
-from ragfmk.utils.milestone import milestone
+from src.ragfmk.utils.trace import trace
+from ragfmk.elements.embeddings.embeddings import embeddings
 from ragfmk.elements.embeddings.stEmbeddings import stEmbeddings
 from ragfmk.elements.wrappers.chunks import chunks
-import ragfmk.utils.CONST as C
-from ragfmk.utils.log import log
 from ragfmk.interfaces.IRag import IRag
-import os
+import ragfmk.utils.CONST as C
 
 class rag(IRag):
     def __init__(self):
-        self.__milestones = milestone()
-        try:
-            self.__ragLogFileName = os.environ[C.RAGCLI_LOGFILE_ENV]
-        except:
-            self.__ragLogFileName = C.TRACE_FILENAME
-        self.__logLevel = C.TRACE_DEFAULT_LEVEL
-        self.__myLog = None
-        self.__milestones.start()
-
-    def setLogInfo(self, logFilename, level):
-        self.__ragLogFileName = logFilename
-        self.__logLevel = level
-        self.__myLog == None
-        
+        self.__trace = trace()
+        self.__trace.start()
+
     def initSearchEngine(self):
         # No search engine for the high level class
         pass
     def processSearch(self, k, vPrompt):
         # No search engine for the high level class
         pass
     def addEmbeddings(self, vChunks):
@@ -51,169 +39,159 @@
         dots = ""
         if (len(message) > limit):
             dots = " ..."
         logMsg = logMsg[:limit] + dots
         return logMsg
 
     @property
-    def milestones(self):
-        return self.__milestones
-    @property
-    def log(self):
-        if (self.__myLog == None):
-            self.__myLog = log(loggerName=C.TRACE_LOGGER, 
-                            logfilename=self.__ragLogFileName,
-                            level=self.__logLevel)
-        return self.__myLog
+    def trace(self):
+        return self.__trace
 
     def addMilestone(self, name, description, *others):
-        self.__milestones.add(name, description, others)
-        self.log.info("Step {} -> {}".format(name, self.__fmtMsgForLog(description)))
+        self.trace.add(name, description, others)
+        self.trace.addlog("INFO", "Step {} -> {}".format(name, self.__fmtMsgForLog(description)))
 
-    def readTXT(self, txtfile) -> str:
+    def readTXT(self, txtfile):
         """ Reads a txt file
         Args:
             txtfile (str): text file path
         Returns:
             str: text read
         """
         try:
             # Read and parse a pdf file
-            self.log.info("Read TXT file {} by using mode ...".format(txtfile))
+            self.trace.addlog("INFO", "Read TXT file {} by using mode ...".format(txtfile))
             doc = document()
             doc.load(txtfile)
             if (len(doc.content) <= 0):
                 raise Exception("Error while reading the TXT document")
             self.addMilestone("PDF2TXT", "TXT file successfully loaded. Text length : {}".format(len(doc.content)))
-            self.log.info("TXT file loaded successfully")
             return doc
         except Exception as e:
-            self.log.error("Error while reading the TXT file: {}".format(str(e)))
-            return ""
+            self.trace.addlog("ERROR", "Error while reading the TXT file: {}".format(str(e)))
+            return document()
 
-    def readPDF(self, pdffile, method = C.READER_VALPYPDF) -> str:
+    def readPDF(self, pdffile, method = C.READER_VALPYPDF):
         """ Reads a pdf file and converts it to Text
         Args:
             pdffile (str): pdf file path
             method (str, optional): Type of conversion. Defaults to C.READER_VALPYPDF.
         Returns:
             str: text converted
         """
         try:
             # Read and parse a pdf file
-            self.log.info("Read PDF file {} by using mode {}...".format(pdffile, method))
+            self.trace.addlog("INFO", "Read PDF file {} by using mode {}...".format(pdffile, method))
             pdf = document()
             if (method == C.READER_VALPYPDF):
                 pdf.pyMuPDFParseDocument(pdffile)
             else:
                 pdf.llamaParseDocument(pdffile)
             if (len(pdf.content) <= 0):
                 raise Exception("Error while converting the PDF document to text")
             self.addMilestone("PDF2TXT", "PDF converted to TEXT successfully. Text length : {}".format(len(pdf.content)))
-            self.log.info("PDF file opened successfully")
             return pdf
         except Exception as e:
-            self.log.error("Error while reading the PDF file: {}".format(str(e)))
-            return ""
+            self.trace.addlog("ERROR", "Error while reading the PDF file: {}".format(str(e)))
+            return document()
             
     def charChunk(self, doc, separator, chunk_size, chunk_overlap) -> chunks:
         """ Document character chunking process
         Args:
             doc (elements.document): Text / document to chunk
             separator (str): Chunks separator
             chunk_size (str): chunk size
             chunk_overlap (str): chunk overlap
         Returns:
             chunks: chunks object
         """
         try:
-            self.log.info("Character Chunking document processing ...")
+            self.trace.addlog("INFO", "Character Chunking document processing ...")
             cks =  doc.characterChunk(separator, chunk_size, chunk_overlap)
             if (cks == None):
                 raise Exception("Error while chunking the document")
             self.addMilestone("CHUNKING","Document (character) chunked successfully, Number of chunks : {}".format(cks.size), cks.size)
-            self.log.info("Document chunked successfully with {} chunks".format(cks.size))
             return cks
         except Exception as e:
-            self.log.error("Error while chunking the document: {}".format(str(e)))
+            self.trace.addlog("ERROR", "Error while chunking the document: {}".format(str(e)))
             return None
 
     def semChunk(self, doc) -> chunks:
         """ Document semantic chunking process
         Args:
             doc (elements.document): Text / document to chunk
         Returns:
             int: number of chunks
             list: List of chunks / JSON format -> {'chunks': ['Transcript of ...', ...] }
         """
         try:
-            self.log.info("Semantic Chunking document processing ...")
+            self.trace.addlog("INFO", "Semantic Chunking document processing ...")
             cks =  doc.semanticChunk()
             if (cks == None):
                 raise Exception("Error while chunking the document")
             self.addMilestone("CHUNKING","Document (character) chunked successfully, Number of chunks : {}".format(cks.size), cks.size)
-            self.log.info("Document chunked successfully with {} chunks".format(cks.size))
             return cks
         except Exception as e:
-            self.log.error("Error while chunking the document: {}".format(str(e)))
+            self.trace.addlog("ERROR", "Error while chunking the document: {}".format(str(e)))
             return None
         
     def buildPrompt(self, question, nr) -> str:
         """ Build smart prompt (for RAG)
         Args:
             question (str): initial question
             nr (nearest object): list of the nearest / most similar chunks
         Returns:
             str: new prompt
         """
         try:
-            self.log.info("Building RAG prompt ...")
+            self.trace.addlog("INFO", "Building RAG prompt ...")
             myPrompt = prompt(question, nr)
             customPrompt = myPrompt.build()
             if (len(customPrompt) == 0):
                 raise Exception("Error while creating the prompt")
             self.addMilestone("PROMPT", "Prompt built successfully", customPrompt)
-            self.log.info("RAG Prompt created successfully")
             return customPrompt
         except Exception as e:
-            self.log.error("Error while building the LLM prompt {}".format(str(e)))
+            self.trace.addlog("ERROR", "Error while building the LLM prompt {}".format(str(e)))
             return ""
 
     def promptLLM(self, question, urlOllama, model, temperature):
         """ send a prompt to the LLM
         Args:
             question (str): prompt
             urlOllama (str): Ollama URL
             model (str): Ollama Model
             temperature (str): Ollama Model LLM Temperature
         Returns:
             str: LLM response
         """
         try:
-            self.log.info("Send the prompt to the LLM ...")
+            self.trace.addlog("INFO", "Send the prompt to the LLM ...")
             myllm = ollama(urlOllama, model, temperature)
             resp = myllm.prompt(question)
-            self.addMilestone("LLMPT", "LLM Reponse\n {}\n".format(resp))
-            self.log.info("Prompt managed successfully by the LLM.")
-            return resp
+            try:
+                token_used = resp["prompt_eval_count"]
+            except:
+                token_used = 0
+            self.addMilestone("LLMPT", "LLM Reponse\n {}\n".format(resp["response"]))
+            return resp["response"], token_used
         except Exception as e:
-            self.log.error("Error while prompting the LLM {}".format(str(e)))
-            return ""
+            self.trace.addlog("ERROR", "Error while prompting the LLM {}".format(str(e)))
+            return "", 0
     
-    def createEmbeddings(self, cks) -> stEmbeddings:
+    def createEmbeddings(self, cks, embds = stEmbeddings()) -> embeddings:
         """ create embeddings for a list of chunks
         Args:
-            cks (chunks): Chunks object
+            cks (chunks): Chunks object (list of texts)
+            embds (embeddings): embeddings object Factory by default stEmbeddings (Sentence Transformer)
         Returns:
             json: data and embeddings
         """
         try:
-            self.log.info("Create embeddings for list of texts/chunks ...")
-            embds = stEmbeddings()
+            self.trace.addlog("INFO", "Create embeddings for list of texts/chunks ...")
             if (not embds.create(cks)):
                 raise Exception("Error while creating the chunks embeddings")
             self.addMilestone("DOCEMBEDDGS", "Embeddings created from chunks successfully")
-            self.log.info("Chunks Embeddings created successfully")
             return embds
         except Exception as e:
-            self.log.error("Error while creating the list of texts/chunks embeddings {}".format(str(e)))
+            self.trace.addlog("ERROR", "Error while creating the list of texts/chunks embeddings {}".format(str(e)))
             return None
```

## ragfmk/ragChromaDB.py

```diff
@@ -1,12 +1,12 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
-from ragfmk.rag import rag
+from src.ragfmk.rag import rag
 from ragfmk.elements.simsearchengines.ChromaDBWrapper import ChromaDBWrapper
 import ragfmk.utils.CONST as C
 
 class ragChromaDB(rag):
     def __init__(self):
         self.__mycdb = ChromaDBWrapper()
         self.__collectionName = "default"
```

## ragfmk/ragFAISS.py

```diff
@@ -1,13 +1,13 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 from ragfmk.elements.simsearchengines.FAISSWrapper import FAISSWrapper
-from ragfmk.rag import rag
+from src.ragfmk.rag import rag
 from ragfmk.elements.wrappers.nearest import nearest
 import ragfmk.utils.CONST as C
 
 """
     This FAISS implementation uses by default the sentence_transformer model (cf. C.EMBEDDING_MODEL) to create and manage embeddings
 """
 FAISS_INDEX_MEMORY = "memory"
@@ -75,15 +75,15 @@
         """
         try:
             self.addMilestone("FAISSSTORE", "Chunks embeddings indexed and stored successfully")
             if (self.indexName != FAISS_INDEX_MEMORY):
                 self.__myfaiss.save(self.storagePath, self.indexName)
             return True
         except Exception as e:
-            self.log.error("Error while storing FAISS index: {}".format(e))
+            self.trace.addlog("ERROR", "Error while storing FAISS index: {}".format(e))
             return False
 
     def initSearchEngine(self):
         """ Load the FAISS index on the disk
         Args:
             path (str): index path
         Returns:
@@ -91,10 +91,9 @@
         """
         try:
             self.addMilestone("FAISSLOAD", "Loading Similarity Search Engine (FAISS)")
             if (self.indexName != FAISS_INDEX_MEMORY):
                 self.__myfaiss.load(self.storagePath, self.indexName)
             return True
         except Exception as e:
-            self.log.error("Error while loading FAISS index: {}".format(e))
-            return False
-
+            self.trace.addlog("ERROR", "Error while loading FAISS index: {}".format(e))
+            return False
```

## ragfmk/elements/embeddings/embeddings.py

```diff
@@ -1,86 +1,80 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
-import ragfmk.utils.CONST as C
 import json
 from numpyencoder import NumpyEncoder
 from ragfmk.interfaces.IEmbeddings import IEmbeddings
+import ragfmk.utils.CONST as C
+from ragfmk.elements.embeddings.embedding import embedding
 
 """
         Embeddings and data are stored in Python list/JSON and used with the following format :
-        {0: {'text': 'The prompt or text', 
-             'embedding': array([-6.65125623e-02,  ..., -1.22626998e-01]) 
-            },
-        1: {'text': '...',  'embedding': array([...]) },
-        ...
+        {"0": {
+                'text': 'The prompt or text', 
+                'embedding': array([-6.65125623e-02,  
+                                    ..., 
+                                    -1.22626998e-01]) 
+              },
+        "1": {
+                'text': '...',  
+                'embedding': array([...]) 
+              },
+        ...,
+        "x" : { ... }
         }
 """
 
 class embeddings(IEmbeddings):
     def __init__(self):
-        self.__content = {}
-
-    @property
-    def content(self):
-        return self.__content
-    @content.setter
-    def content(self, q):
-        self.__content = q
-        
+        self.__embeddings = {}
+    
     @property
     def jsonContent(self) -> str: 
-        return json.dumps(self.__content, cls=NumpyEncoder)
+        return json.dumps(self.content, cls=NumpyEncoder)
+    @jsonContent.setter
+    def jsonContent(self, jsondata):
+        embsLoaded = json.loads(jsondata)
+        for key, value in embsLoaded.items():
+            emb = embedding()
+            emb.init(value[C.JST_TEXT], value[C.JST_EMBEDDINGS])
+            self.__embeddings[key] = emb
+
+    @property
+    def content(self): 
+        myJsonList = {}
+        for key, value in self.__embeddings.items():
+            myJsonList[key] = value.content
+        return myJsonList
     
     @property
-    def size(self):
-        return len(self.__content)
+    def items(self):
+        return self.__embeddings
+    @items.setter
+    def items(self, it):
+        self.__embeddings = it
         
-    def encode(self, cks):
-        """ to surcharge with the embeddings class
-        Args:
-            cks (chunks): list of chunks to embed
-        Returns:
-            json: vector embeddings
-        """
-        return None
-
-    def __wrap(self, vectAndData):
-        """ Wrap the Dataframe into a list (to have a json later)
-        The Dataframe contains 2 columns: 
-            1) text : with the data
-            2) embedding: with the vector/embeddings calculated (as a nparray)
+    def __getitem__(self, item):
+        """ Makes the Data column accessible via [] array
+            example: df['colName']
         Args:
-            vectAndData (Dataframe): Data and embeddings
+            item (str): attribute/column name
         Returns:
-            {}: list for a later JSON conversion
+            object: data
         """
-        self.__content = {}
-        for i, (chunk, vector) in enumerate(vectAndData):
-            line = {}
-            line[C.JST_TEXT] = chunk
-            line[C.JST_EMBEDDINGS] = vector
-            self.__content[i] = line
+        return self.__embeddings.__getitem__(item)
+    
+    @property
+    def size(self):
+        return len(self.__embeddings)
 
     def create(self, cks) -> bool:
-        """ Calculate the embeddings for list of chunks
-        Args:
-            cks (chunks):  chunks object
-        Returns:
-            str: json with data and embeddings for all chunks
-        """
-        try: 
-            vect = self.encode(cks)
-            vectAndData = zip(cks.items, vect)
-            self.__wrap(vectAndData)
-            return True
-        except Exception as e:
-            return False
-        
+        return False
+
     def save(self, filename) -> bool:
         """ Save the chunks in a file.
         Args:
             filename (_type_): JSON chunks file
         Returns:
             bool: True if ok
         """
@@ -91,24 +85,23 @@
         except Exception as e:
             return False
 
     def load(self, filename = "", content = "") -> bool:
         """ Load and build a chunk file (can be loaded from a json file or a json content). 
             Format required : Content = {"chunks": [..., ...] }
         Args:
-            filename (str, optional): JSON chunks file. Defaults to "".
-            content (str, optional): JSON chunks content. Defaults to "".
+            filename (str, optional): JSON embeddings file. Defaults to "".
+            content (str, optional): JSON embeddings content. Defaults to "".
         Returns:
             bool: True if ok
         """
         try:
-            self.content = {}
-            if (len(filename) >0):
+            if (len(filename) > 0):
                 with open(filename, "r", encoding=C.ENCODING) as f:
-                    self.content = json.load(f)
+                    self.jsonContent = f.read()
             elif (len(content) >0):
-                self.content = content
+                self.jsonContent = content
             else:
                 return False
             return True
         except Exception as e:
             return False
```

## ragfmk/elements/embeddings/stEmbeddings.py

```diff
@@ -1,33 +1,57 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 from sentence_transformers import SentenceTransformer
 import ragfmk.utils.CONST as C
 from ragfmk.elements.embeddings.embeddings import embeddings
+from ragfmk.elements.embeddings.embedding import embedding
 
 """
-        Embeddings and data are stored in JSON and used with the following format :
-        {0: {'text': 'How many jobs Joe Biden wants to create ?', 
-             'embedding': array([-6.65125623e-02,  4.26685601e-01, -1.22626998e-01, -1.14275487e-02,
-                                -1.76032424e-01, -2.55425069e-02,  3.19633447e-02,  1.10126780e-02,
-                                -1.75059751e-01,  2.00320985e-02,  3.28031659e-01,  1.18581623e-01,
-                                -9.89666581e-02,  1.68430805e-01,  1.19766712e-01, -7.14423656e-02, ...] 
-            },
-        1: {'text': '...', 
-            'embedding': array([...]
-            },
-        ...
+        Embeddings and data are stored in Python list/JSON and used with the following format :
+        {"0": {
+                'text': 'The prompt or text', 
+                'embedding': array([-6.65125623e-02,  
+                                    ..., 
+                                    -1.22626998e-01]) 
+              },
+        "1": {
+                'text': '...',  
+                'embedding': array([...]) 
+              },
+        ...,
+        "x" : { ... }
         }
 """
 
 class stEmbeddings(embeddings):
     def __init__(self):
+        self.__embeddingsModel = C.EMBEDDING_MODEL
         super().__init__()
-        
-    def encode(self, cks):
-        try:
-            encoder = SentenceTransformer(C.EMBEDDING_MODEL)
-            return encoder.encode(cks.items)
-        except:
-            return None
+
+    @property
+    def model(self) -> str: 
+        return self.__embeddingsModel
+    @model.setter
+    def model(self, model):
+        self.__embeddingsModel = model
+
+    def create(self, cks) -> bool:
+        """ Calculate the embeddings for list of chunks
+        Args:
+            cks (chunks):  chunks object
+        Returns:
+            str: json with data and embeddings for all chunks
+        """
+        try: 
+            encoder = SentenceTransformer(self.__embeddingsModel)
+            vect = encoder.encode(cks.items)
+            vectAndData = zip(cks.items, vect)
+            self.items = {}
+            for i, (chunk, vector) in enumerate(vectAndData):
+                emb = embedding()
+                emb.init(chunk, vector)
+                self.items[str(i)] = emb
+            return True
+        except Exception as e:
+            return False
```

## ragfmk/elements/llms/ollama.py

```diff
@@ -28,12 +28,12 @@
                       "prompt": prompt, 
                       "stream": False,
                       "temperature": self.temperature}
             response = requests.post(url, json=params)
             if (response.status_code == 200):
                 response_text = response.text
                 data = json.loads(response_text)
-                return data["response"]
+                return data
             else:
                 raise Exception("Error while reaching out to the Web Service: {}", str(response.status_code, response.text))
         except Exception as e:
             return str(e)
```

## ragfmk/elements/simsearchengines/ChromaDBWrapper.py

```diff
@@ -57,15 +57,15 @@
                 raise Exception ("Impossible to get the collection from Chroma DB")
             collection.add(documents=dfNewContent[C.JST_TEXT].tolist(),
                            embeddings=dfNewContent[C.JST_EMBEDDINGS].tolist(),
                            ids=["id_"+ str(i) for i in range(len(dfNewContent))],
                            metadatas=[{"md_id": i} for i in range(len(dfNewContent))])               
             return collection.count()
         except Exception as e:
-            return -1
+            raise
         
     def getNearest(self, vText, k, collectionName):
         """ Process the similarity search on the existing Chroma DB (and the given prompt)
                 --> k is set to the total number of vectors within the index
                 --> ann is the approximate nearest neighbour corresponding to those distances
         Args:
             vText (json): Prompt's embeddings
@@ -79,8 +79,8 @@
                 raise Exception ("Impossible to get the collection from Chroma DB")
             result = collection.query(query_embeddings=vText.content["0"]["embedding"], n_results=k)
             nr = nearest()
             nr.items = result["documents"][0]
             nr.distances = result["distances"][0]
             return nr
         except Exception as e:
-            return None
+            raise
```

## ragfmk/elements/simsearchengines/FAISSWrapper.py

```diff
@@ -11,72 +11,76 @@
 from ragfmk.elements.wrappers.nearest import nearest
 
 """ 
     Leverage Meta FAISS
 """
 class FAISSWrapper:
     def __init__(self):
-        self.index = None   # FAISS Index
-        self.dfContent = pd.DataFrame(columns = [C.JST_TEXT, C.JST_EMBEDDINGS]) # real data that are indexed
+        self.__index = None   # FAISS Index
+        self.__dfContent = pd.DataFrame(columns = [C.JST_TEXT, C.JST_EMBEDDINGS]) # real data that are indexed
 
     def save(self, filepath="./backup/", name="faissbackup"):
         """ Save the FAISS index and the data (chunks)
         Args:
             filepath (str, optional): _description_. Defaults to "./backup/".
             name (str, optional): _description_. Defaults to "faissbackup".
         """
         datafile = os.path.join(filepath, name + ".data")
         indexfile = os.path.join(filepath, name + ".index")
         with open(datafile, "wb") as f:
-            pickle.dump(self.dfContent, f)
-        faiss.write_index(self.index, indexfile)
+            pickle.dump(self.__dfContent, f)
+        faiss.write_index(self.__index, indexfile)
 
     def load(self, filepath="./backup/", name="faissbackup"):
         """ Read the FAISS index and the data (chunks) saved previously
 
         Args:
             filepath (str, optional): _description_. Defaults to "./backup/".
             name (str, optional): _description_. Defaults to "faissbackup".
         """
         datafile = os.path.join(filepath, name + ".data")
         indexfile = os.path.join(filepath, name + ".index")
         with open(datafile, "rb") as f:
-            self.dfContent = pickle.load(f)
-        self.index = faiss.read_index(indexfile)
+            self.__dfContent = pickle.load(f)
+        self.__index = faiss.read_index(indexfile)
         
     @property
+    def size(self) -> int:
+        return self.__index.ntotal
+    
+    @property
     def ready(self) -> bool:
         """check if ready for searching for the NN
         Returns:
             bool: True if index ready
         """
         try:
-            return self.index.is_trained #and not self.dfContent.empty
+            return self.__index.is_trained #and not self.dfContent.empty
         except:
             return False
 
     def add(self, item):
         """ Index a new item
         Args:
             item (stEmbeddings): embeddings object to add into the index
         """
         # Get source data and JSON -> DF
         dfNewContent = pd.DataFrame(item.content).T
         embeddings = [ np.asarray(v) for v in dfNewContent[C.JST_EMBEDDINGS] ]
         self.__addToIndexFlatL2(embeddings)
         # Concat the content with the existing DF
-        self.dfContent = pd.concat([self.dfContent, dfNewContent])
+        self.__dfContent = pd.concat([self.__dfContent, dfNewContent])
 
     def __addToIndexFlatL2(self, embeddings):
         """
             Build a Flat L2 index
         """
         vout = self.__prepareEmbeddings(embeddings)
-        self.index = faiss.IndexFlatL2(vout.shape[1])
-        self.index.add(vout)
+        self.__index = faiss.IndexFlatL2(vout.shape[1])
+        self.__index.add(vout)
 
     def __prepareEmbeddings(self, vects):
         """ Prepare the embeddings for indexing
         Args:
             vects (array/embedding): vector
         Returns:
             array/embedding: vector prepared
@@ -93,24 +97,26 @@
         Args:
             prompt (json): Prompt's embeddings
             k (int): Nb of nearest to return
         Returns:
             nearest object: List of the most nearest neighbors
         """
         try: 
+            if (not self.ready):
+                raise Exception("The FAISS Index is not loaded properly.")
             # Get prompt vector only and normalize it
             prompt = vPrompt.content
             idx = "0" if (str(type(list(prompt.keys())[0])) == "<class 'str'>") else 0
             vector = self.__prepareEmbeddings([ prompt[idx][C.JST_EMBEDDINGS] ])
             # process the Similarity search
-            ktotal = self.index.ntotal
-            distances, ann = self.index.search(vector, k=ktotal)
+            distances, ann = self.__index.search(vector, k=k)
             # Sort search results and return a DataFrame
-            results = pd.DataFrame({'distances': distances[0], 'ann': ann[0]})
-            self.dfContent.index = self.dfContent.index.astype(int)
-            merge = pd.merge(results, self.dfContent, left_on='ann', right_index=True)
+            results = pd.DataFrame({'distances': distances[0], 
+                                    'ann': ann[0]})
+            self.__dfContent.index = self.__dfContent.index.astype(int)
+            merge = pd.merge(results, self.__dfContent, left_on='ann', right_index=True)
             nr = nearest()
-            nr.items = merge[:k]["text"].to_list()
-            nr.distances = merge[:k]["distances"].to_list()
+            nr.items = merge["text"].to_list()
+            nr.distances = merge["distances"].to_list()
             return nr
-        except:
-            return None
+        except Exception as e:
+            raise
```

## ragfmk/elements/wrappers/chunks.py

```diff
@@ -38,14 +38,15 @@
     @jsonContent.setter
     def jsonContent(self, content):
         try:
             jsonEnv = json.loads(content)
             self.items = jsonEnv[C.JST_CHUNKS]
         except Exception as e:
             self.items = []
+            raise
 
     @property
     def size(self): 
         return len(self.items)
     
     def __createEnveloppe(self):
         jsonEnv = {}
@@ -59,19 +60,15 @@
         """ Wrap and store the document langchain
         Args:
             docs (document): langchain document
         Returns:
             int: Number of chunks
             str: json chunks -> {'chunks': ['Transcript of ...', ...] }
         """
-        try:
-            self.items = [ x.page_content for x in lcDoc ]
-            return True
-        except Exception as e:
-            return False
+        self.items = [ x.page_content for x in lcDoc ]
 
     def save(self, filename):
         """ Save the chunks in a file.
         Args:
             filename (_type_): JSON chunks file
         Returns:
             bool: True if ok
@@ -93,15 +90,16 @@
             bool: True if ok
         """
         try:
             jsonEnv = ""
             if (len(filename) >0):
                 with open(filename, "r", encoding=C.ENCODING) as f:
                     jsonEnv = json.load(f)
+                self.items = jsonEnv[C.JST_CHUNKS]
             elif (len(content) >0):
                 jsonEnv = content
+                self.items.append(jsonEnv)
             else:
                 return False
-            self.items = jsonEnv[C.JST_CHUNKS]
             return True
         except Exception as e:
             return False
```

## ragfmk/elements/wrappers/document.py

```diff
@@ -43,15 +43,15 @@
         try:
             with open(filename, "w", encoding=C.ENCODING) as f:
                 f.write(self.__content)
             return True
         except Exception as e:
             return False
     
-    def pyMuPDFParseDocument(self, filename, fromPage=0, toPage=0, heightToRemove=0) -> bool:
+    def pyMuPDFParseDocument(self, filename, fromPage=0, toPage=0, heightToRemove=0):
         """ Read a pdf file and add the content as text by using PyMuPDF
         Args:
             fromPage (int, optional): Starts from page Number. Defaults to 0.
             toPage (int, optional): Ends at page Number. Defaults to 0.
             heightToRemove (int, optional): Height in pixel to remove (header and footer). Defaults to 0.
 
         Returns:
@@ -64,18 +64,17 @@
                 if (numPage+1 >= fromPage and numPage+1 <= toPage):
                     pageBox = page.artbox
                     rect = fitz.Rect(pageBox[0], 
                                     pageBox[1] + heightToRemove, 
                                     pageBox[2], 
                                     pageBox[3] - heightToRemove)
                     self.__content = self.__content + page.get_textbox(rect) # get plain text encoded as UTF-8
-            return True
         except Exception as e:
             self.__content = ""
-            return False
+            raise
 
     def llamaParseDocument(self, filename, extractType="markdown"):
         """ Read a pdf file and add the content as text by using llamaparse
             the LLAMAINDEX_API_KEY environment variable must be set to the API Key
             Cf.
                 Login : https://cloud.llamaindex.ai/login
                 Docs : https://docs.llamaindex.ai/
@@ -115,19 +114,17 @@
                 time.sleep(C.LLAMAPARSE_API_WAITSEC)
                 if (iteration >= C.LLAMAPARSE_ITERATION_MAX):
                     raise Exception ("Llamaindex seems not responsive or not responsive enough, please retry again.")
                 iteration += 1
             # download the result
             result = response.json()
             self.content = result[extractType]
-            return True
-        
         except Exception as e:
             self.content = ""
-            return False
+            raise
 
     def characterChunk(self, separator, chunk_size, chunk_overlap) -> chunks:
         """ Chunks the document content into several pieces/chunks and returns a json text with the chunks
             format : {'chunks': ['Transcript of ...', ...] }
             Note: Leverage character langchain to manage the chunks
         Args:
             separator (str): Chunks separator
@@ -143,17 +140,17 @@
                                                 length_function = len, 
                                                 is_separator_regex = False)
             docs = text_splitter.create_documents([self.content])
             cks = chunks()
             cks.setLangchainDocument(docs)
             return cks
         except Exception as e:
-            return None
+            raise
     
-    def semanticChunk(self):
+    def semanticChunk(self) -> chunks:
         """Chunks the document content into several pieces/chunks and returns a json text with the chunks
             format : {'chunks': ['Transcript of ...', ...] }
             Note: Leverage semantic langchain to manage the chunks
         Returns:
             str: A JSON text which looks like this: {'chunks': ['Transcript of ...', ...] }
         """
         try: 
@@ -168,9 +165,9 @@
                     )
             text_splitter = SemanticChunker(hf_embeddings)
             docs = text_splitter.create_documents([self.content])
             cks = chunks()
             cks.setLangchainDocument(docs)
             return cks
         except Exception as e:
-            return -1, {}
+            raise
```

## ragfmk/elements/wrappers/nearest.py

```diff
@@ -37,15 +37,26 @@
         return self.__distances 
     @distances.setter
     def distances(self, q):
         self.__distances = q
         
     @property
     def jsonContent(self) -> str: 
-        return json.dumps(self.__createEnveloppe(), cls=NumpyEncoder)
+        try:
+            return json.dumps(self.__createEnveloppe(), cls=NumpyEncoder)
+        except:
+            return "{}"
+    @jsonContent.setter
+    def jsonContent(self, content):
+        try:
+            jsonEnv = json.loads(content)
+            self.__items = jsonEnv[C.JST_NEAREST]
+        except Exception as e:
+            self.__items = []
+            raise
 
     @property
     def size(self) -> items: 
         return len(self.items)
     
     def __createEnveloppe(self) -> str:
         jsonEnv = {}
```

## ragfmk/elements/wrappers/prompt.py

```diff
@@ -1,18 +1,20 @@
 __author__ = "Benoit CAYLA"
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import ragfmk.utils.CONST as C
+from ragfmk.interfaces.IPrompt import IPrompt
+from jinja2 import Template
 
-class prompt:
-    def __init__(self, myquestion, mycontext):
-        self.__question = myquestion
-        self.__context = mycontext  # list (nearest)
-        self.__template = C.PROMPT_RAG_TEMPLATE
+class prompt(IPrompt):
+    def __init__(self, question, similarItems):
+        self.__question = question
+        self.__similarItems = similarItems  # list (nearest)
+        self.__template = C.PROMPT_RAG_JINJA_TEMPLATE
 
     @property
     def template(self):
         return self.__template
     @template.setter
     def template(self, t):
         self.__template = t
@@ -21,22 +23,35 @@
     def question(self):
         return self.__question
     @question.setter
     def question(self, q):
         self.__question = q
     
     @property
-    def context(self):
-        return self.__context
-    @context.setter
-    def context(self, q):
-        self.__context = q
-    
+    def similarItems(self):
+        return self.__similarItems
+    @similarItems.setter
+    def similarItems(self, q):
+        self.__similarItems = q
+
+    def loadTemplate(self, filename):
+        try:
+            with open(filename, "r", encoding=C.ENCODING) as f:
+                self.__template = f.read()
+            return True
+        except Exception as e:
+            return False
+
     def build(self):
-        try: 
-            itemContext = ""
-            for i, item in enumerate(self.context.items):
-                itemContext = itemContext + C.ITEM_CONTEXT_TEMPLATE_LINE.format(i=i, contextItem=item) + "\n"
-            return self.template.format(prompt=self.question, 
-                                        context=itemContext)
-        except:
-            return ""
+        try:
+            if (len(self.template) == 0):
+                raise Exception ("A JINJA2 template must be specified.")
+            if (len(self.question) == 0):
+                raise Exception ("The RAG question for the prompt must be filled out.")
+            if (self.similarItems.size == 0):
+                raise Exception ("The number of context informations (chunks) cannot be empty.")
+            j2_template = Template(self.template)
+            data = { C.TPL_QUESTION: self.question, 
+                     C.TPL_NEAREST: self.similarItems }
+            return j2_template.render(data)
+        except Exception as e:
+            raise
```

## ragfmk/interfaces/IChunks.py

```diff
@@ -4,31 +4,31 @@
 
 from abc import ABC, abstractmethod
 
 class IChunks(ABC):
     @property
     @abstractmethod
     def items(self): 
-        return [] 
+        pass 
     @items.setter
     def items(self, q):
         pass
     
     @property
     @abstractmethod
     def jsonContent(self): 
-        return None
+        pass
     @jsonContent.setter
     def jsonContent(self, content):
         pass
     
     @property
     @abstractmethod
     def size(self): 
-        return None
+        pass
     
     @abstractmethod
     def add(self, chunk):
         pass
     
     @abstractmethod
     def save(self, filename):
```

## ragfmk/interfaces/IDocument.py

```diff
@@ -4,15 +4,15 @@
 
 from abc import ABC, abstractmethod
 
 class IDocument(ABC):
     @property
     @abstractmethod
     def content(self): 
-        return None
+        pass
     @content.setter
     def content(self, q):
         pass
     
     @abstractmethod
     def load(self, filename):
         pass
```

## ragfmk/interfaces/IEmbeddings.py

```diff
@@ -4,32 +4,40 @@
 
 from abc import ABC, abstractmethod
 
 class IEmbeddings(ABC):
     
     @property
     @abstractmethod
-    def content(self):
-        return None
-    @content.setter
-    def content(self, q):
+    def jsonContent(self):
+        pass
+    @jsonContent.setter
+    def jsonContent(self, content):
         pass
     
     @property
     @abstractmethod
-    def jsonContent(self):
-        return None
-    
+    def content(self): 
+        pass
+        
     @property
     @abstractmethod
-    def size(self):
-        return None
+    def items(self):
+        pass
+    @items.setter
+    def items(self, it):
+        pass
+    
+    @abstractmethod
+    def __getitem__(self, item):
+        pass
     
+    @property
     @abstractmethod
-    def encode(self, cks):
+    def size(self):
         pass
     
     @abstractmethod
     def create(self, cks):
         pass
     
     @abstractmethod
```

## ragfmk/interfaces/INearest.py

```diff
@@ -5,36 +5,39 @@
 from abc import ABC, abstractmethod
 
 class INearest(ABC):
 
     @property
     @abstractmethod
     def items(self): 
-        return None 
+        pass 
     @items.setter
     def items(self, q):
         pass
     
     @property
     @abstractmethod
     def distances(self): 
-        return None 
+        pass 
     @distances.setter
     def distances(self, q):
         pass
     
     @property
     @abstractmethod
     def jsonContent(self): 
-        return None
+        pass
+    @jsonContent.setter
+    def jsonContent(self, content):
+        pass
     
     @property
     @abstractmethod
     def size(self) -> items: 
-        return None
+        pass
     
     @abstractmethod
     def add(self, item):
         pass
 
     @abstractmethod
     def save(self, filename):
```

## ragfmk/interfaces/IRag.py

```diff
@@ -4,24 +4,15 @@
 
 from abc import ABC, abstractmethod
 
 class IRag(ABC):
 
     @property
     @abstractmethod
-    def milestones(self):
-        return None
-    
-    @property
-    @abstractmethod
-    def log(self):
-        return None
-    
-    @abstractmethod
-    def setLogInfo(self, logFilename, level):
+    def trace(self):
         pass
     
     @abstractmethod
     def readTXT(self, txtfile):
         pass
     
     @abstractmethod
```

## ragfmk/utils/CONST.py

```diff
@@ -21,29 +21,36 @@
 FAISS_DEFAULT_STORE = "./vstore"
 
 # LLM stuff
 SEMCHUNK_EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"
 EMBEDDING_MODEL = "paraphrase-mpnet-base-v2"
 OLLAMA_LOCAL_URL = "http://localhost:11434/api"
 OLLAMA_DEFAULT_LLM = "tinydolphin"
+OLLAMA_DEFAULT_EMB = "all-minilm"
 LLM_DEFAULT_TEMPERATURE = 0.9
 SM_DEFAULT_NEAREST = 3
 CHKS_DEFAULT_SIZE = 500
 CHKS_DEFAULT_OVERLAP = 50
 CHKS_DEFAULT_SEP = "."
 
 # JSON "tags" for chunks & embeddings
 JST_CHUNKS = "chunks"
 JST_TEXT = "text"
 JST_EMBEDDINGS = "embedding"
 JST_NEAREST = "nearest"
 
 # Prompts
-PROMPT_RAG_TEMPLATE = "Question: {prompt}\n Please answer the question based on the informations listed below: {context}"
-ITEM_CONTEXT_TEMPLATE_LINE = "Context {i}: {contextItem}"
+TPL_QUESTION = "question"
+TPL_NEAREST = "nearestItems"
+PROMPT_RAG_JINJA_TEMPLATE = "Question: {{question}}\n \
+    Please answer the question based on the informations listed below: \n \
+    {%- for item in nearestItems.items %} \
+    Item: \n \
+    {{ item }} \n \
+    {% endfor %}"
 
 # Output status
 OUT_ERROR = "ERROR"
 OUT_SUCCESS = "SUCCESS"
 
 # Llamaparse
 LLAMAPARSE_API_URL = "https://api.cloud.llamaindex.ai/api/parsing"
```

## Comparing `ragfmk/utils/milestone.py` & `ragfmk/utils/trace.py`

 * *Files 13% similar despite different names*

```diff
@@ -2,23 +2,27 @@
 __email__ = "benoit@datacorner.fr"
 __license__ = "MIT"
 
 import time
 from datetime import timedelta, datetime
 import json
 
-class milestone:
+class trace:
     def __init__(self):
         self.perfCounter = None
         self.startTime = None
         self.stopTime = None
         self.traceSteps = []
         self.msHeader = {}
         self.stepIdx = 1
         self.msHeader = {}
+        self.logs = []
+
+    def addlog(self, typelog, description):
+        self.logs.append("{} [{}] {}".format(datetime.now(), typelog, description))
 
     def initialize(self, args):
         for arg in args.keys():
             self.msHeader[arg] = args[arg]
 
     def start(self):
         if (self.perfCounter == None):
@@ -46,12 +50,13 @@
     def stop(self):
         self.stopTime = datetime.now()
 
     def getFullJSON(self):
         fullJson = {}
         fullJson["parameters"] = self.msHeader
         fullJson["steps"] = self.traceSteps
+        fullJson["logs"] = self.logs
         fullJson["start"] = str(self.startTime)
         self.stopTime = datetime.now() if self.stopTime == None else self.stopTime
         fullJson["stop"] = str(self.stopTime)
         fullJson["duration"] = str(self.stopTime - self.startTime)
         return json.dumps(fullJson)
```

## Comparing `ragfmk-0.1.3.1.dist-info/LICENSE` & `ragfmk-0.1.4.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `ragfmk-0.1.3.1.dist-info/RECORD` & `ragfmk-0.1.4.9.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,36 +1,39 @@
-RagAdhocQueryDoc.py,sha256=wkfGzr5lz1MFEyBjK1-ZT7hkGhuMrz0swHV2kWvyADk,3622
-RagPrompt.py,sha256=Rk2dMZq3lQ8RCnbm7sXeKwEjFH8ABX7YmUh6tgNkQ-A,1694
+RagAdhocQueryDoc.py,sha256=gzuNzXJBN1gIh_e6qd4tv5ntH6a1n3yYXmWkBg3ZfpE,3665
+RagPrompt.py,sha256=FYrRAVsU5TOPPL8aD9Nlr0KdjFJqGdCHxbszS1cgyqo,1735
+RagTest.py,sha256=0h0pmS1wZhyHC-yMkj-WJcLWo659pwRpqZTNPLSoORY,2233
 __init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/rag.py,sha256=JElXSEIa6NsETnzSkxTuILFGxH2KHTjfLZaO-56f6_Q,9013
-ragfmk/ragChromaDB.py,sha256=LSyPDjYeTyDsmEmHPKJ-8GT136qFJO5hz1wizr9CGDk,2641
-ragfmk/ragFAISS.py,sha256=sM9I0b7uhCn4AOwlmPwVwURgCSJTntVBLi3WVe_OkcQ,3720
+ragfmk/rag.py,sha256=mYJHC1gLlixRaLMklauSv2Gz0Iwoa7wkgJwHvEhqqUo,8274
+ragfmk/ragChromaDB.py,sha256=1HDZ7R4a3XwawND81yXNimJFZPmhwQz_AEy0_k_InxU,2645
+ragfmk/ragFAISS.py,sha256=c0CY_LoxM_Y9qT9smiehOerq2zcleuxRG5Z0TcR22F0,3744
 ragfmk/elements/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/embeddings/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/elements/embeddings/embeddings.py,sha256=DGqoSdYGkxAJYNjzfMq23Z0jFrhrGSim35e_nd-GA0Q,3623
-ragfmk/elements/embeddings/stEmbeddings.py,sha256=H-mRWBh3CNxQ3HqAylPLqsZHtVz7jq_rFKfwFmBZPUY,1226
+ragfmk/elements/embeddings/embedding.py,sha256=cmVbqR1-8dwcktghdctmTDhEn8QeiId2EkBrB8Jyl5E,1586
+ragfmk/elements/embeddings/embeddings.py,sha256=8qrLKBOfdXqbbeyWEl2SK7_Ug_dBISGwjM5rCxfR9Lo,3333
+ragfmk/elements/embeddings/ollamaEmbeddings.py,sha256=z-Bxq6zKHPkUoQUVY4oo98EKCM3Yrf37O2vWextDx_8,2528
+ragfmk/elements/embeddings/stEmbeddings.py,sha256=ckR-gL2mHMKId9I8DfGPoWrQeLhNctaZJpMq6iDnhQA,1833
 ragfmk/elements/llms/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/elements/llms/ollama.py,sha256=q78U-2bh4qk-JuO1lM8reMsrA1SG12k6KRyWkrsf8dg,1245
-ragfmk/elements/simsearchengines/ChromaDBWrapper.py,sha256=p18Qo_mz21qGqGnJlLlWfVfHaa336YpuFWQ4F_jwd-0,3400
-ragfmk/elements/simsearchengines/FAISSWrapper.py,sha256=0r_lVtK7Wgi6EF0XxHnoMlJiHkamiqMg_5CDiTc2Q6I,4595
+ragfmk/elements/llms/ollama.py,sha256=9FUkoHiWZk-KRe7QDXFtC828ghUHJrZmKbMiTrpWHE0,1233
+ragfmk/elements/simsearchengines/ChromaDBWrapper.py,sha256=mtNMIWED5V1aY36vIvGk4pnqo0gTQoMEN_dDzd3mv-Q,3390
+ragfmk/elements/simsearchengines/FAISSWrapper.py,sha256=CCRRVoFfturN4-QqC_vmBzZjrlkTj6GC1SBtx1ISKuc,4814
 ragfmk/elements/simsearchengines/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 ragfmk/elements/wrappers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/elements/wrappers/chunks.py,sha256=EEnRFzm2xNx-pEO0Jkam5UAM2r0dVPrTBjfVwkgfdaY,3237
-ragfmk/elements/wrappers/document.py,sha256=L-kbKr56X_XQSqhP-yFm-ANoYb_BAYcKPQwqdAug86Q,7605
-ragfmk/elements/wrappers/nearest.py,sha256=LMi96FEiBRGRgarlTrXHCsP8blR3YOiMhxSIzCRcuCY,2816
-ragfmk/elements/wrappers/prompt.py,sha256=sEKZ_hy0VPrIKlFAa5yAzq1bqPrS4bJOfPEURGx_ou4,1199
-ragfmk/interfaces/IChunks.py,sha256=md_sJ39Mg9iW4Bnn553WcYHqvUe-jf7GZ0RutIxtTjk,759
-ragfmk/interfaces/IDocument.py,sha256=UgivIp4Bc-qxPXtVO5u542wZUWDBaoX5Wmd4ncf1UOg,432
-ragfmk/interfaces/IEmbeddings.py,sha256=1zsQopimtSNdvvXO5g6BRNX8ahM0t8_9qEnOYnMR8IA,775
-ragfmk/interfaces/INearest.py,sha256=zZmcOp6t217sZU1O2TK6Jtip-JKB6_wxxFiATRGlFSE,847
-ragfmk/interfaces/IRag.py,sha256=X8_NYXJiD6-0z3ZMGMsXLnOWyjsS-wpYbiYdG55phqM,1244
-ragfmk/utils/CONST.py,sha256=bg_dOUfcDlmKBkvVLuODvC-ohfG7j10gKz42IpfkBsU,1697
+ragfmk/elements/wrappers/chunks.py,sha256=uBiTw2LP6waZe_uEpIGIfkuZUubwVf2vX6JbaCZC-3k,3203
+ragfmk/elements/wrappers/document.py,sha256=94to3cUwiypdcLwlDHwEtcy1d4W6EdTDkOf79juWRUE,7519
+ragfmk/elements/wrappers/nearest.py,sha256=ZDsjrOTFwkleFVMBnRe0vclaiypzG4pNyqCaICXmkBM,3128
+ragfmk/elements/wrappers/prompt.py,sha256=tsOH_Z8XRxuHhz7EmzEmfJdYxuYd99D4Eo0EgBM2-kE,1850
+ragfmk/interfaces/IChunks.py,sha256=1dBqYCEXgWiNQAiteQHcG-ZQ_uCluUPaGXCyBSMXZto,740
+ragfmk/interfaces/IDocument.py,sha256=0KrP-0szl8qHDH1TyC5opBqxTJV1dAIe9nTeNetiUR0,425
+ragfmk/interfaces/IEmbeddings.py,sha256=To1PGoNXFIplJf40CVrEUDoXcMRXkBNwtsYYlfI_nF4,916
+ragfmk/interfaces/INearest.py,sha256=DvPacdOW7lRzzX7MpdyvF3SlczjIlyxYrrQ-HcSV4bg,895
+ragfmk/interfaces/IPrompt.py,sha256=pZf8xQlKUw5s2Q4MSyxfpIQyNXGb6fjDwf9S1KAK9jg,581
+ragfmk/interfaces/IRag.py,sha256=unEm0bEoGOwuhgAwNdI4h-gF1vkxC4nGHTAaeUC-PR8,1061
+ragfmk/utils/CONST.py,sha256=ftYmVGuexCo7hF5_-llP2NBvZ_T_sFwpTPMiKhhPlYI,1842
 ragfmk/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-ragfmk/utils/log.py,sha256=39zWFAuL5vZ1RGFhEFPZ5rCmUg32AdlYuUae5lRGHEU,1864
-ragfmk/utils/milestone.py,sha256=dA5T_0QVWtHv3HaAFVZkKCEdRDS2SQSJsi7g0e_glGw,1907
-ragfmk-0.1.3.1.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
-ragfmk-0.1.3.1.dist-info/METADATA,sha256=06-mMEvo0DkNb218x8stNeCOOga251vOusoCsWlMF8U,8549
-ragfmk-0.1.3.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-ragfmk-0.1.3.1.dist-info/entry_points.txt,sha256=EDEdK3AmGTz0ZLfXx10aBUy50jxn6Uy3isBcmPkE3jg,86
-ragfmk-0.1.3.1.dist-info/top_level.txt,sha256=6NrWPkR5zCFIJnz9grec4DG39Znl2xhbQklrnns9kZ4,43
-ragfmk-0.1.3.1.dist-info/RECORD,,
+ragfmk/utils/trace.py,sha256=SvAuKCuXLsxs0jIgEjQtDAPWMYP0rXtbvKkK7EWGfMQ,2097
+ragfmk-0.1.4.9.dist-info/LICENSE,sha256=BvMmeujpLDerNqzsA1gOET7mphtef9ebt_u8-bp4bNA,1091
+ragfmk-0.1.4.9.dist-info/METADATA,sha256=pgLiTN7FpSWGT9yQKUGZ51LUWbWprH-2TLye2smA4Gs,8713
+ragfmk-0.1.4.9.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+ragfmk-0.1.4.9.dist-info/entry_points.txt,sha256=EDEdK3AmGTz0ZLfXx10aBUy50jxn6Uy3isBcmPkE3jg,86
+ragfmk-0.1.4.9.dist-info/top_level.txt,sha256=LlNlUm1ce3g4y9ARq9Q4C-19JnDcERKf-aDueiUfWG8,51
+ragfmk-0.1.4.9.dist-info/RECORD,,
```

