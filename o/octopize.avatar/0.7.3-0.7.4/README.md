# Comparing `tmp/octopize_avatar-0.7.3.tar.gz` & `tmp/octopize_avatar-0.7.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "octopize_avatar-0.7.3.tar", max compression
+gzip compressed data, was "octopize_avatar-0.7.4.tar", max compression
```

## Comparing `octopize_avatar-0.7.3.tar` & `octopize_avatar-0.7.4.tar`

### file list

```diff
@@ -1,51 +1,51 @@
--rw-r--r--   0        0        0     9861 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/LICENSE.txt
--rw-r--r--   0        0        0      420 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/README.md
--rw-r--r--   0        0        0       54 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/__init__.py
--rw-r--r--   0        0        0      673 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/_typing.py
--rw-r--r--   0        0        0    64564 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api.py
--rw-r--r--   0        0        0    34599 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_autogenerated.py
--rw-r--r--   0        0        0     2640 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_batch_test.py
--rw-r--r--   0        0        0    20298 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/api_custom_dataset_methods_test.py
--rw-r--r--   0        0        0    19921 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/base_client.py
--rw-r--r--   0        0        0     4423 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/client.py
--rw-r--r--   0        0        0     3845 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/client_test.py
--rw-r--r--   0        0        0     1005 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/conftest.py
--rw-r--r--   0        0        0       20 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/constants.py
--rw-r--r--   0        0        0      122 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/exceptions.py
--rw-r--r--   0        0        0        0 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/__init__.py
--rw-r--r--   0        0        0      450 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/continuous_threshold.py
--rw-r--r--   0        0        0      410 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/continuous_threshold_test.py
--rw-r--r--   0        0        0     6543 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/saferound.py
--rw-r--r--   0        0        0     3552 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/saferound_test.py
--rw-r--r--   0        0        0     3182 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split.py
--rw-r--r--   0        0        0     2253 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_column_types_test.py
--rw-r--r--   0        0        0      931 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_columns_types.py
--rw-r--r--   0        0        0     2624 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/lib/split_test.py
--rw-r--r--   0        0        0    80551 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/models.py
--rw-r--r--   0        0        0     1361 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/__init__.py
--rw-r--r--   0        0        0     1125 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/conftest.py
--rw-r--r--   0        0        0     2153 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/datetime.py
--rw-r--r--   0        0        0     1327 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/datetime_test.py
--rw-r--r--   0        0        0     8419 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/expected_mean.py
--rw-r--r--   0        0        0     6313 2024-04-29 15:40:22.278003 octopize_avatar-0.7.3/avatars/processors/expected_mean_test.py
--rw-r--r--   0        0        0     4691 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/group_modalities.py
--rw-r--r--   0        0        0     3839 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/group_modalities_test.py
--rw-r--r--   0        0        0    10381 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference.py
--rw-r--r--   0        0        0     7906 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference_test.py
--rw-r--r--   0        0        0    14028 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference.py
--rw-r--r--   0        0        0     6297 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference_test.py
--rw-r--r--   0        0        0     8633 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference.py
--rw-r--r--   0        0        0     9393 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference_test.py
--rw-r--r--   0        0        0    11673 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference.py
--rw-r--r--   0        0        0     8349 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference_test.py
--rw-r--r--   0        0        0     3309 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/perturbation.py
--rw-r--r--   0        0        0     3457 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/perturbation_test.py
--rw-r--r--   0        0        0     6497 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/proportions.py
--rw-r--r--   0        0        0     6894 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/proportions_test.py
--rw-r--r--   0        0        0     6945 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/relative_difference.py
--rw-r--r--   0        0        0     7089 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/relative_difference_test.py
--rw-r--r--   0        0        0     4632 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/to_categorical.py
--rw-r--r--   0        0        0     2979 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/processors/to_categorical_test.py
--rw-r--r--   0        0        0     1113 2024-04-29 15:40:22.282003 octopize_avatar-0.7.3/avatars/utils.py
--rw-r--r--   0        0        0     1912 2024-04-29 15:40:22.330004 octopize_avatar-0.7.3/pyproject.toml
--rw-r--r--   0        0        0     1183 1970-01-01 00:00:00.000000 octopize_avatar-0.7.3/PKG-INFO
+-rw-r--r--   0        0        0     9861 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/LICENSE.txt
+-rw-r--r--   0        0        0      420 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/README.md
+-rw-r--r--   0        0        0       54 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/__init__.py
+-rw-r--r--   0        0        0    49945 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/api.py
+-rw-r--r--   0        0        0    35707 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/api_autogenerated.py
+-rw-r--r--   0        0        0     2640 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/api_batch_test.py
+-rw-r--r--   0        0        0    19237 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/api_custom_dataset_methods_test.py
+-rw-r--r--   0        0        0     9874 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/arrow_utils.py
+-rw-r--r--   0        0        0    23251 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/base_client.py
+-rw-r--r--   0        0        0     4423 2024-05-27 16:52:45.168330 octopize_avatar-0.7.4/avatars/client.py
+-rw-r--r--   0        0        0     3845 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/client_test.py
+-rw-r--r--   0        0        0     1005 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/conftest.py
+-rw-r--r--   0        0        0       20 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/constants.py
+-rw-r--r--   0        0        0      122 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/exceptions.py
+-rw-r--r--   0        0        0        0 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/__init__.py
+-rw-r--r--   0        0        0      450 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/continuous_threshold.py
+-rw-r--r--   0        0        0      410 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/continuous_threshold_test.py
+-rw-r--r--   0        0        0     6543 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/saferound.py
+-rw-r--r--   0        0        0     3552 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/saferound_test.py
+-rw-r--r--   0        0        0     3182 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/split.py
+-rw-r--r--   0        0        0     2253 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/split_column_types_test.py
+-rw-r--r--   0        0        0      931 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/split_columns_types.py
+-rw-r--r--   0        0        0     2624 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/lib/split_test.py
+-rw-r--r--   0        0        0    81297 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/models.py
+-rw-r--r--   0        0        0     1361 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/__init__.py
+-rw-r--r--   0        0        0     1125 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/conftest.py
+-rw-r--r--   0        0        0     2153 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/datetime.py
+-rw-r--r--   0        0        0     1327 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/datetime_test.py
+-rw-r--r--   0        0        0     8419 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/expected_mean.py
+-rw-r--r--   0        0        0     6313 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/expected_mean_test.py
+-rw-r--r--   0        0        0     4691 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/group_modalities.py
+-rw-r--r--   0        0        0     3839 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/group_modalities_test.py
+-rw-r--r--   0        0        0    10381 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_cumulated_difference.py
+-rw-r--r--   0        0        0     7906 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_cumulated_difference_test.py
+-rw-r--r--   0        0        0    14028 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_range_difference.py
+-rw-r--r--   0        0        0     6297 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_range_difference_test.py
+-rw-r--r--   0        0        0     8633 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_cumulated_difference.py
+-rw-r--r--   0        0        0     9393 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_cumulated_difference_test.py
+-rw-r--r--   0        0        0    11673 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_range_difference.py
+-rw-r--r--   0        0        0     8349 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/inter_record_range_difference_test.py
+-rw-r--r--   0        0        0     3309 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/perturbation.py
+-rw-r--r--   0        0        0     3457 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/perturbation_test.py
+-rw-r--r--   0        0        0     6497 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/proportions.py
+-rw-r--r--   0        0        0     6894 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/proportions_test.py
+-rw-r--r--   0        0        0     6945 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/relative_difference.py
+-rw-r--r--   0        0        0     7089 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/relative_difference_test.py
+-rw-r--r--   0        0        0     4632 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/to_categorical.py
+-rw-r--r--   0        0        0     2979 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/processors/to_categorical_test.py
+-rw-r--r--   0        0        0     1513 2024-05-27 16:52:45.172330 octopize_avatar-0.7.4/avatars/utils.py
+-rw-r--r--   0        0        0     2316 2024-05-27 16:52:45.224329 octopize_avatar-0.7.4/pyproject.toml
+-rw-r--r--   0        0        0     1183 1970-01-01 00:00:00.000000 octopize_avatar-0.7.4/PKG-INFO
```

### Comparing `octopize_avatar-0.7.3/LICENSE.txt` & `octopize_avatar-0.7.4/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/api.py` & `octopize_avatar-0.7.4/avatars/api.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,38 +1,42 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
+# API Version : 2.2.1-dd82c574fc3448fe2f02c09f29b8136f63017c14
 
 
 import logging
-import os
-import shutil
 import tempfile
 import warnings
-from contextlib import ExitStack
-from io import BytesIO
 from pathlib import Path
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, TypeVar, Union
+from typing import Any, Dict, List, Optional, Tuple, TypeVar, Union
 from uuid import UUID
 
 import numpy as np
 import pandas as pd
-import pyarrow
-from pyarrow import parquet as pq
 
 from avatars.api_autogenerated import Auth as _Auth
 from avatars.api_autogenerated import Compatibility as _Compatibility
 from avatars.api_autogenerated import Datasets as _Datasets
 from avatars.api_autogenerated import Health as _Health
 from avatars.api_autogenerated import Jobs as _Jobs
 from avatars.api_autogenerated import Metrics as _Metrics
 from avatars.api_autogenerated import Reports as _Reports
 from avatars.api_autogenerated import Stats as _Stats
 from avatars.api_autogenerated import Users as _Users
+from avatars.arrow_utils import (
+    ArrowDatasetBuilder,
+    DataSourceItem,
+    DataSourceItems,
+    FileLike,
+    is_file_like,
+)
+from avatars.client import ApiClient
 from avatars.constants import DEFAULT_TIMEOUT
-from avatars.exceptions import InvalidFileType, Timeout
+from avatars.exceptions import Timeout
+from avatars.models import AdviceJob  # noqa: F401
+from avatars.models import AdviceJobCreate  # noqa: F401
 from avatars.models import AvatarizationBatchJob  # noqa: F401
 from avatars.models import AvatarizationBatchJobCreate  # noqa: F401
 from avatars.models import AvatarizationJob  # noqa: F401
 from avatars.models import AvatarizationJobCreate  # noqa: F401
 from avatars.models import AvatarizationMultiTableJob  # noqa: F401
 from avatars.models import AvatarizationMultiTableJobCreate  # noqa: F401
 from avatars.models import AvatarizationWithTimeSeriesJob  # noqa: F401
@@ -81,20 +85,14 @@
     ColumnType,
     FileType,
     JobStatus,
     PrivacyMetricsParameters,
     SignalMetricsParameters,
 )
 
-if TYPE_CHECKING:
-    from avatars.client import ApiClient
-    from avatars._typing import FileLikeInterface, HttpxFile
-
-from avatars._typing import is_file_like
-
 logger = logging.getLogger(__name__)
 logger.addHandler(logging.NullHandler())
 DEFAULT_RETRY_TIMEOUT = 60
 MAX_ROWS_PER_FILE = 1_000_000
 MAX_BYTES_PER_FILE = 100 * 1024 * 1024  # 100 MB
 
 PARQUET_MAGIC_BYTES = b"PAR1"
@@ -123,15 +121,15 @@
     else:
         return str(s.value)
 
     raise TypeError(f"Unknown column type: '{s}'")
 
 
 class Auth:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def login(
         self,
         request: Login,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
@@ -180,15 +178,15 @@
             request,
         ]
 
         return _Auth(self.client).reset_password(*args, **kwargs)
 
 
 class Compatibility:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def is_client_compatible(
         self,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> CompatibilityResponse:
@@ -200,25 +198,23 @@
 
         args: List[Any] = []
 
         return _Compatibility(self.client).is_client_compatible(*args, **kwargs)
 
 
 class Datasets:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def create_dataset_from_stream(
         self,
-        request: Optional[
-            Union["FileLikeInterface[bytes]", "FileLikeInterface[str]"]
-        ] = None,
+        request: Optional[FileLike] = None,
         name: Optional[str] = None,
         source: Optional[
-            Union[str, "FileLikeInterface[bytes]"]
+            DataSourceItem
         ] = None,  # optional because we still have to support the old way # TODO: Remove once deprecated
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Dataset:
         """Create a dataset by streaming chunks of the dataset.
 
         DEPRECATED: Please use create_dataset instead.
@@ -226,31 +222,23 @@
 
         warnings.warn(
             DeprecationWarning(
                 "create_dataset_from_stream is deprecated. Use create_dataset instead."
             )
         )
 
-        _source: Optional[
-            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
-        ] = (request or source)
-        return self.create_dataset(source=_source, name=name, timeout=timeout)  # type: ignore[arg-type]
+        _source: Optional[Union[str, FileLike]] = request or source
+        return self.create_dataset(source=_source, name=name, timeout=timeout)
 
     def create_dataset(
         self,
-        request: Optional[
-            Union["FileLikeInterface[str]", "FileLikeInterface[bytes]"]
-        ] = None,  # TODO: Remove once deprecated
+        request: Optional[FileLike] = None,  # TODO: Remove once deprecated
         name: Optional[str] = None,
         source: Optional[
-            Union[
-                str,
-                list[str],
-                "FileLikeInterface[bytes]",
-            ]
+            DataSourceItems
         ] = None,  # optional because we still have to support the old way
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Dataset:
         """Create a dataset from file upload."""
 
         if request:
@@ -265,425 +253,104 @@
             raise ValueError("You need to pass in a source.")
 
         _source = request or source
 
         if _source is None:
             raise ValueError("You need to pass in a source.")
 
-        with ExitStack() as stack:
-            file_arguments = self._create_httpx_file_argument(_source, stack)
+        ds, inferred_type = ArrowDatasetBuilder().to_dataset(_source)
+
+        filetype: Optional[FileType] = (
+            FileType[inferred_type] if inferred_type else None
+        )
+
+        kwargs = {
+            "method": "post",
+            "url": "/datasets/stream",
+            "timeout": timeout,
+            "dataset": ds,
+            "params": dict(name=name, filetype=filetype),
+        }
 
-            kwargs = {
-                "method": "post",
-                "url": "/datasets/stream",
-                "timeout": timeout,
-                "file": file_arguments,
-                "params": dict(
-                    name=name,
-                ),
-            }
+        result = self.client.request(**kwargs)
 
-            result = self.client.request(**kwargs)  # type: ignore[arg-type]
         return Dataset(**result)
 
     def download_dataset_as_stream(
         self,
         id: str,
-        destination: Optional[
-            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
-        ] = None,
+        destination: Optional[Union[str, FileLike]] = None,
         *,
         filetype: Optional[FileType] = None,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
-    ) -> BytesIO:
+    ) -> Any:
         """Download a dataset by streaming chunks of it.
 
         DEPRECATED: Please use download_dataset instead.
         """
         warnings.warn(
             DeprecationWarning(
                 "download_dataset_as_stream is deprecated. Use download_dataset instead."
             )
         )
 
         # Ignoring return type because download_dataset's logic makes sure
         # that the return type will be BytesIO
         # No point in going through the hassle of using typing.overload for something
         # that will be removed soon
-        return self.download_dataset(  # type: ignore[return-value]
+        return self.download_dataset(
             id,
             destination=destination,
             timeout=timeout,
             filetype=filetype,
             from_download_as_stream=True,
         )
 
     def download_dataset(
         self,
         id: str,
-        destination: Optional[
-            Union[str, "FileLikeInterface[bytes]", "FileLikeInterface[str]"]
-        ] = None,
+        destination: Optional[Union[str, FileLike]] = None,
         filetype: Optional[FileType] = None,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
         from_download_as_stream: bool = False,  # TODO: Remove once deprecated
-    ) -> Optional[Union[BytesIO, str]]:
+    ) -> Any:
         """Download a dataset."""
-
-        CHUNK_SIZE = 1024 * 1024  # 1MB
         # Download dataset metadata
         dataset_info = self.client.datasets.get_dataset(id, timeout=timeout)
 
-        if destination is None and not from_download_as_stream:
-            # Download as CSV was the old return value when using download_dataset
-            filetype = filetype or FileType.csv
-
-        elif destination is None and from_download_as_stream:
-            # Download as the filetype of the dataset on the server was the old
-            # return value when using download_dataset_as_stream
-            filetype = filetype or dataset_info.filetype
-
-        invalid_destination_error = TypeError(
-            f"Expected destination to be a string or a buffer, got {type(destination)} instead"
-        )
-
-        with tempfile.TemporaryDirectory() as download_dir:
-            download_path = Path(download_dir) / "downloaded"
-            download_path.touch()
-
-            with open(download_path, "wb") as file:
-                self.client.request(
-                    method="get",
-                    url=f"/datasets/{id}/download/stream",
-                    params={"filetype": filetype},
-                    should_stream=True,
-                    timeout=timeout,
-                    destination=file,
-                )
-
-            # The server returns the data in the same type as the filetype the dataset was saved in,
-            # if no filetype is specified in download_dataset
-            downloaded_filetype = filetype or dataset_info.filetype
-            parquet_dir = Path(download_dir) / "parquet"
-            if downloaded_filetype is FileType.parquet:
-                # If it's a parquet file, we have to open the file and split it into multiple files
-                # as the server sends us parquet fragments, which we saved in a single file upon
-                # receiving the stream.
-                with open(download_path, "rb") as input_file:
-                    count = 0
-                    parquet_dir = Path(download_dir) / "parquet"
-                    parquet_dir.mkdir()
-                    # Read the file in chunks of 1MB
-                    while content := input_file.read(CHUNK_SIZE):
-                        self._split_parquet_file(content, parquet_dir, count)
-
-                # Collect all filenames and store them into a single parquet file
-                concatenated_filename = Path(download_dir) / "concatenated.parquet"
-                filenames = self._collect_filenames(parquet_dir)
-                schema = pyarrow.parquet.ParquetFile(filenames[0]).schema_arrow
-                with pyarrow.parquet.ParquetWriter(
-                    str(concatenated_filename), schema=schema
-                ) as writer:
-                    for filename in filenames:
-                        writer.write_table(
-                            pyarrow.parquet.read_table(filename, schema=schema)
-                        )
-
-                if isinstance(destination, str):
-                    # We simply 'mv' the concatenated filename to the destination
-                    # desired by the user
-                    shutil.move(str(concatenated_filename), destination)
-                    return None
-
-                if destination and not is_file_like(destination):
-                    raise invalid_destination_error
-
-                output_buffer = BytesIO() if destination is None else destination
-
-                try:
-                    # We don't know the type of the buffer, so we try to write the bytes to it
-                    with open(concatenated_filename, "rb") as file:
-                        while as_bytes := file.read(CHUNK_SIZE):
-                            # error: Argument 1 to "write" of "SupportsWrite"
-                            # has incompatible type "bytes"; expected "str"
-                            output_buffer.write(as_bytes)  # type: ignore[arg-type]
-
-                except TypeError as e:
-                    message = str(e.args[0])
-                    messages_inducing_raise = [
-                        "string argument expected",
-                        "write() argument must be str, not bytes",
-                    ]
-                    if any(m in message for m in messages_inducing_raise):
-                        raise InvalidFileType(
-                            "Can't download parquet files as string. Please use a different destination."
-                        )
-                    raise e
-
-                output_buffer.seek(0)
-                # Destination = None is the deprecated case
-                # TODO: Remove once deprecated
-                if destination is None:
-                    warnings.warn(
-                        DeprecationWarning(
-                            "Please specify the destination argument. The return type will change in the future."
-                        )
-                    )
-
-                    output_buffer.seek(0)
-                    if from_download_as_stream:
-                        # Old return value when using download_dataset_as_stream
-
-                        # Old return value when using download_dataset_as_stream
-                        # Incompatible return value type (got
-                        # "Union[BytesIO, FileLikeInterface[bytes], FileLikeInterface[str]]", expected
-                        # "Union[BytesIO, str, None]")  [return-value]
-
-                        return output_buffer  # type: ignore[return-value]
-                    else:
-                        # Old return value when using download_dataset (with Filetype.csv as default)
-                        raise InvalidFileType(
-                            "Can't download parquet files as string. Please use a different destination."
-                        )
-
-                # We return the buffer, as the user provided it
-                return None
-            else:
-                # Destination = None is the deprecated case
-                # TODO: Remove once deprecated
-                if destination is None:
-                    warnings.warn(
-                        DeprecationWarning(
-                            "Please specify the destination argument. The return type will change in the future."
-                        )
-                    )
-                    buffer = BytesIO()
-                    with open(download_path, "rb") as file:
-                        buffer.write(file.read())
-                    buffer.seek(0)
-
-                    if from_download_as_stream:
-                        # Old return value when using download_dataset_as_stream
-                        return buffer
-                    else:
-                        # Old return value when using download_dataset (with Filetype.csv as default)
-                        return buffer.read().decode()
-
-                if isinstance(destination, str):
-                    # We simply 'mv' the concatenated filename to the destination
-                    # desired by the user
-                    shutil.move(str(download_path), destination)
-                    return None
-
-                if is_file_like(destination):
-                    # User wants to save the file to a buffer he provided
-
-                    # We don't know the type of the buffer, so we try to write the text to it
-                    # and if it fails, we break and re-open the file as bytes
-                    is_string_buffer = True
-                    with open(download_path, "r") as file:
-                        while as_string := file.read(CHUNK_SIZE):
-                            try:
-                                destination.write(as_string)
-                            except TypeError as e:
-                                if "a bytes-like object is required" in str(e.args[0]):
-                                    is_string_buffer = False
-                                    break
-                                else:
-                                    raise e
-
-                    if is_string_buffer is False:
-                        with open(download_path, "rb") as file:
-                            while as_bytes := file.read(CHUNK_SIZE):
-                                # error: Argument 1 to "write" of "SupportsWrite"
-                                # has incompatible type "bytes";
-                                # expected "str"  [arg-type]
-                                destination.write(as_bytes)  # type: ignore[arg-type]
-
-                    destination.seek(0)
-                    return None
-
-                raise invalid_destination_error
-
-    def _collect_filenames(self, path: Path) -> List[Path]:
-        """
-        Collects and sorts the filenames in a given directory and its subdirectories.
-
-        This function collects all the filenames in the specified path and its subdirectories.
-        It then sorts the filenames based on the number at the end of the filename. The sorting
-        places 0 before 1, 2, etc., rather than 1, 10, 11, 12, etc. This is to ensure correct
-        order during upload and concatenation when downloaded.
-
-        Parameters
-        ----------
-        path
-            The directory path where the files are located.
-
-        Returns
-        -------
-        List[Path]
-            The list of sorted filenames.
-        """
-
-        filenames = [f for f in path.glob("**/*") if f.is_file()]
-
-        # Sort the file names to ensure correct order during upload and concatenation when downloaded.
-        # The files are named with a number at the end, so sorting is based on that number.
-        # Note that the sorting places 0 before 1, 2, etc., rather than 1, 10, 11, 12, etc.
-        # Example filename: /tmp/tmp1f1z2qk2/33e74308ee414f94885d99236d10faf7-0.parquet
-
-        filenames.sort(key=lambda x: int(x.name.split("-")[1].split(".")[0]))
-
-        return filenames
-
-    def _create_httpx_file_argument(
-        self,
-        source: Union[
-            str,
-            list[str],
-            "FileLikeInterface[bytes]",
-            "FileLikeInterface[str]",  # TODO: remove that possibility once deprecated
-        ],
-        stack: ExitStack,
-    ) -> List["HttpxFile"]:
-        if source and is_file_like(source):
-            # User provided file like object.
-            # It is his responsibility to close it.
-
-            # Get size of source, so we can check if it's too large
-            # and split it into multiple files
-
-            source_needs_to_be_split = False
-            if source.read(MAX_BYTES_PER_FILE) and source.read(1):
-                # There are more bytes to read, so the file is too large
-                # and we have to split it into multiple files
-                source_needs_to_be_split = True
-            source.seek(0)
-
-            if not source_needs_to_be_split:
-                # We try reading the file to check if it's binary or not
-                # HTTPX handles binary files only
-                single_char = source.read(1)
-
-                if isinstance(single_char, bytes):
-                    # undo previous read operation
-                    # can only be done on binary file handles
-                    source.seek(-1, os.SEEK_CUR)
-
-                    # File is binary, we can upload it as is
-                    return [("file", source)]
-
-                # TODO: When deprecating, raise an error instead of a warning
-                warnings.warn(
-                    DeprecationWarning(
-                        "You are trying to upload a text file. This is deprecated. "
-                        "Please open the file in binary mode."
-                    )
+        if destination is None:
+            warnings.warn(
+                DeprecationWarning(
+                    "Please specify the destination argument. The return type will change in the future."
                 )
-
-                # We create a new temporary file, write the content of the source file to it
-                # and then upload the temporary file, but this time opened in binary mode (default)
-                temporary_file = stack.enter_context(tempfile.NamedTemporaryFile())
-                temporary_file.write(single_char.encode())
-                temporary_file.write(source.read().encode())
-                temporary_file.seek(0)
-
-                return [("file", temporary_file)]
-
-            # TODO: Split the file without loading it all into memory.
-            try:
-                df = pd.read_parquet(source, engine="pyarrow")
-            except (TypeError, pyarrow.lib.ArrowInvalid) as e:
-                expected_messages = [
-                    "binary file expected",
-                    "this is not a parquet file",
-                ]
-                if any(m in str(e.args[0]) for m in expected_messages):
-                    source.seek(0)
-                    df = pd.read_csv(source)
-
-                else:
-                    raise e
-
-            temp_dir = stack.enter_context(tempfile.TemporaryDirectory())
-            # Write the table into multiple parquet files
-            pq.write_to_dataset(
-                pyarrow.Table.from_pandas(df),
-                root_path=temp_dir,
-                max_rows_per_file=MAX_ROWS_PER_FILE,
-                row_group_size=MAX_ROWS_PER_FILE,
-                max_rows_per_group=MAX_ROWS_PER_FILE,
             )
-            # Create a list, so that the if statement further down can upload the files
-            source = list(map(str, self._collect_filenames(Path(temp_dir))))
-
-        if isinstance(source, str):
-            source = [source]
-
-        if isinstance(source, list):
-            # List of files to upload as one dataset.
-            # This is especially useful for large parquet files.
-            return [
-                ("file", stack.enter_context(open(file_path, "rb")))
-                for file_path in source
-            ]
-
-        raise TypeError(
-            f"Expected source to be a string or a buffer, got {type(source)} instead."
-        )
-
-    def _split_parquet_file(self, content: bytes, parquet_dir: Path, count: int) -> int:
-        """
-        Splits a parquet file into multiple smaller parquet files.
-
-        This function recursively splits the input bytes content into separate parquet files.
-        The split is done based on the PARQUET_MAGIC_BYTES delimiter. The resulting files are
-        stored in the specified directory and are named as "file-{count}.parquet".
-
-        Parameters
-        ----------
-        content
-            The content of the parquet file to be split.
-        parquet_dir
-            The directory where the split parquet files will be stored.
-        count
-            The starting count for the file naming.
-
-        Returns
-        -------
-        int
-            The count of the last file written. This can be used to continue writing files
-            with sequential names in subsequent function calls.
-
-        """
 
-        delimiter = PARQUET_MAGIC_BYTES + PARQUET_MAGIC_BYTES
-        if delimiter in content:
-            all = content.split(delimiter, 1)
-            if len(all) > 1:
-                before, after = all
-                after = PARQUET_MAGIC_BYTES + after
-                before = before + PARQUET_MAGIC_BYTES
+            if not from_download_as_stream:
+                # Download as CSV was the old return value when using download_dataset
+                filetype = filetype or FileType.csv
             else:
-                before = all[0]
-                after = b""
+                # Download as the filetype of the dataset on the server was the old
+                # return value when using download_dataset_as_stream
+                filetype = filetype or dataset_info.filetype
         else:
-            before = content
-            after = b""
-
-        current_file = parquet_dir / f"file-{count}.parquet"
-        current_file.touch(exist_ok=True)
-        with open(current_file, "wb") as f:
-            f.write(before)
-
-        if after:
-            count = self._split_parquet_file(after, parquet_dir, count + 1)
+            if not (isinstance(destination, str) or is_file_like(destination)):
+                raise TypeError(
+                    f"Expected destination to be a string or a buffer, got {type(destination)} instead"
+                )
 
-        return count  # to be able to continue on the same file
+        return self.client.request(
+            method="get",
+            url=f"/datasets/{id}/download/stream",
+            params={"filetype": filetype},
+            should_stream=True,
+            want_content=destination is None and not from_download_as_stream,
+            timeout=timeout,
+            destination=destination,
+        )
 
     def find_all_datasets_by_user(
         self,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> List[Dataset]:
         """List all datasets of the current_user."""
@@ -768,15 +435,15 @@
             id,
         ]
 
         return _Datasets(self.client).get_dataset_correlations(*args, **kwargs)
 
 
 class Health:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def get_root(
         self,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Any:
@@ -818,15 +485,15 @@
 
         args: List[Any] = []
 
         return _Health(self.client).get_health_db(*args, **kwargs)
 
 
 class Jobs:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def find_all_jobs_by_user(
         self,
         nb_days: Optional[int] = None,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
@@ -1346,17 +1013,53 @@
 
         args: List[Any] = [
             id,
         ]
 
         return _Jobs(self.client).get_privacy_metrics_multi_table_job(*args, **kwargs)
 
+    def create_advice(
+        self,
+        request: AdviceJobCreate,
+        *,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AdviceJob:
+        """Create advice on anonymization parameters."""
+
+        kwargs: Dict[str, Any] = {
+            "timeout": timeout,
+        }
+
+        args: List[Any] = [
+            request,
+        ]
+
+        return _Jobs(self.client).create_advice(*args, **kwargs)
+
+    def get_advice(
+        self,
+        id: str,
+        *,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AdviceJob:
+        """Get advice result."""
+
+        kwargs: Dict[str, Any] = {
+            "timeout": timeout,
+        }
+
+        args: List[Any] = [
+            id,
+        ]
+
+        return _Jobs(self.client).get_advice(*args, **kwargs)
+
 
 class Metrics:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def get_job_projections(
         self,
         job_id: str,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
@@ -1380,15 +1083,14 @@
         ]
 
         return _Metrics(self.client).get_job_projections(*args, **kwargs)
 
     def get_variable_contributions(
         self,
         job_id: str,
-        dataset_id: str,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Contributions:
         """Get the contributions of the dataset variables within the fitted space.
 
         See https://saiph.readthedocs.io/en/latest for more information.
 
@@ -1400,15 +1102,14 @@
 
         kwargs: Dict[str, Any] = {
             "timeout": timeout,
         }
 
         args: List[Any] = [
             job_id,
-            dataset_id,
         ]
 
         return _Metrics(self.client).get_variable_contributions(*args, **kwargs)
 
     def get_explained_variance(
         self,
         job_id: str,
@@ -1433,15 +1134,15 @@
             job_id,
         ]
 
         return _Metrics(self.client).get_explained_variance(*args, **kwargs)
 
 
 class Reports:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def create_report(
         self,
         request: ReportCreate,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
@@ -1548,15 +1249,15 @@
             request,
         ]
 
         return _Reports(self.client).create_geolocation_privacy_report(*args, **kwargs)
 
 
 class Stats:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def get_cluster_stats(
         self,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> ClusterStats:
@@ -1568,15 +1269,15 @@
 
         args: List[Any] = []
 
         return _Stats(self.client).get_cluster_stats(*args, **kwargs)
 
 
 class Users:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def find_users(
         self,
         email: Optional[str] = None,
         username: Optional[str] = None,
         *,
@@ -1653,15 +1354,15 @@
             id,
         ]
 
         return _Users(self.client).get_user(*args, **kwargs)
 
 
 class PandasIntegration:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def upload_dataframe(
         self,
         request: "pd.DataFrame",
         name: Optional[str] = None,
         *,
@@ -1684,35 +1385,20 @@
             ):
                 raise ValueError(
                     f"Expected column '{col}' to have either str or numeric values."
                     " Consider harmonizing columns prior to upload."
                 )
 
         df_types = request.dtypes
-        table = pyarrow.Table.from_pandas(request)
-        del request
-
-        # Create a temporary directory to store the split parquet files
-        with tempfile.TemporaryDirectory() as temp_dir:
-            # Write the table into multiple parquet files
-            pq.write_to_dataset(
-                table,
-                root_path=temp_dir,
-                max_rows_per_file=MAX_ROWS_PER_FILE,
-                row_group_size=MAX_ROWS_PER_FILE,
-                max_rows_per_group=MAX_ROWS_PER_FILE,
-            )
 
-            dataset = self.client.datasets.create_dataset(
-                source=list(
-                    map(str, Datasets(self.client)._collect_filenames(Path(temp_dir)))
-                ),
-                name=name,
-                timeout=timeout,
-            )
+        dataset = self.client.datasets.create_dataset(
+            source=request,
+            name=name,
+            timeout=timeout,
+        )
 
         columns = []
         for index, dtype in zip(df_types.index, df_types):
             column_detail = ColumnDetail(
                 type=to_column_type(str(dtype)),
                 label=index,
                 is_identifier=index in identifier_variables,
@@ -1771,15 +1457,15 @@
 
         df[datetime_columns] = df[datetime_columns].astype("datetime64[ns]")
 
         return df
 
 
 class Pipelines:
-    def __init__(self, client: "ApiClient") -> None:
+    def __init__(self, client: ApiClient) -> None:
         self.client = client
 
     def avatarization_pipeline_with_processors(
         self,
         request: AvatarizationPipelineCreate,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
@@ -1914,15 +1600,15 @@
             avatarization_job_id=avatarization_job.id,
             signal_job_id=signal_job.id,
             privacy_job_id=privacy_job.id,
         )
 
 
 def upload_batch_and_get_order(
-    client: "ApiClient",
+    client: ApiClient,
     training: pd.DataFrame,
     splits: List[pd.DataFrame],
     timeout: int = DEFAULT_TIMEOUT,
 ) -> Tuple[UUID, List[UUID], Dict[UUID, pd.Index]]:
     """Upload batches to the server
     Arguments
     ---------
@@ -1953,15 +1639,15 @@
     for dataset, dataframe in zip(datasets_split_ids, splits):
         batch_mapping[dataset] = dataframe.index
 
     return training_dataset.id, datasets_split_ids, batch_mapping
 
 
 def download_avatar_dataframe_from_batch(
-    client: "ApiClient",
+    client: ApiClient,
     avatarization_batch_result: AvatarizationBatchResult,
     timeout: int = DEFAULT_TIMEOUT,
     filetype: FileType = FileType.parquet,
 ) -> pd.DataFrame:
     """Download the shuffled avatar dataframe from batch result.
     Arguments
     ---------
@@ -1986,15 +1672,15 @@
         )
         for batch_results in avatarization_batch_result.batch_results
     ]
     return pd.concat([training_df] + splits_df)
 
 
 def download_sensitive_unshuffled_avatar_dataframe_from_batch(
-    client: "ApiClient",
+    client: ApiClient,
     avatarization_batch_result: AvatarizationBatchResult,
     order: Dict[UUID, pd.Index],
     timeout: int = DEFAULT_TIMEOUT,
     filetype: FileType = FileType.parquet,
 ) -> pd.DataFrame:
     """Download the sensitive unshuffled avatar dataframe from batch result.
```

### Comparing `octopize_avatar-0.7.3/avatars/api_autogenerated.py` & `octopize_avatar-0.7.4/avatars/api_autogenerated.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,17 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
+# API Version : 2.2.1-dd82c574fc3448fe2f02c09f29b8136f63017c14
 
 
 import logging
 from io import BytesIO, StringIO
 from typing import TYPE_CHECKING, Any, Dict, List, Optional, TypeVar, Union
 
+from avatars.models import AdviceJob  # noqa: F401
+from avatars.models import AdviceJobCreate  # noqa: F401
 from avatars.models import AvatarizationBatchJob  # noqa: F401
 from avatars.models import AvatarizationBatchJobCreate  # noqa: F401
 from avatars.models import AvatarizationJob  # noqa: F401
 from avatars.models import AvatarizationJobCreate  # noqa: F401
 from avatars.models import AvatarizationMultiTableJob  # noqa: F401
 from avatars.models import AvatarizationMultiTableJobCreate  # noqa: F401
 from avatars.models import AvatarizationWithTimeSeriesJob  # noqa: F401
@@ -147,25 +149,27 @@
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
     def create_dataset_from_stream(
         self,
         request: Union[StringIO, BytesIO],
         name: Optional[str] = None,
+        filetype: Optional[FileType] = None,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Dataset:
         """Create a dataset by streaming chunks of the dataset."""
 
         kwargs: Dict[str, Any] = {
             "method": "post",
             "url": f"/datasets/stream",  # noqa: F541
             "timeout": timeout,
             "params": dict(
                 name=name,
+                filetype=filetype,
             ),
             "file": request,
         }
 
         return Dataset(**self.client.request(**kwargs))
 
     def find_all_datasets_by_user(
@@ -895,14 +899,50 @@
 
         return self.client.get_job(
             response_cls=PrivacyMetricsMultiTableJob,
             per_request_timeout=per_request_timeout,
             **kwargs,
         )
 
+    def create_advice(
+        self,
+        request: AdviceJobCreate,
+        *,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AdviceJob:
+        """Create advice on anonymization parameters."""
+
+        kwargs: Dict[str, Any] = {
+            "method": "post",
+            "url": f"/jobs/advice",  # noqa: F541
+            "timeout": timeout,
+            "json_data": request,
+        }
+
+        return AdviceJob(**self.client.request(**kwargs))
+
+    def get_advice(
+        self,
+        id: str,
+        *,
+        per_request_timeout: Optional[int] = DEFAULT_TIMEOUT,
+        timeout: Optional[int] = DEFAULT_TIMEOUT,
+    ) -> AdviceJob:
+        """Get advice result."""
+
+        kwargs: Dict[str, Any] = {
+            "method": "get",
+            "url": f"/jobs/advice/{id}",  # noqa: F541
+            "timeout": timeout,
+        }
+
+        return self.client.get_job(
+            response_cls=AdviceJob, per_request_timeout=per_request_timeout, **kwargs
+        )
+
 
 class Metrics:
     def __init__(self, client: "ApiClient") -> None:
         self.client = client
 
     def get_job_projections(
         self,
@@ -927,15 +967,14 @@
         }
 
         return Projections(**self.client.request(**kwargs))
 
     def get_variable_contributions(
         self,
         job_id: str,
-        dataset_id: str,
         *,
         timeout: Optional[int] = DEFAULT_TIMEOUT,
     ) -> Contributions:
         """Get the contributions of the dataset variables within the fitted space.
 
         See https://saiph.readthedocs.io/en/latest for more information.
 
@@ -947,15 +986,14 @@
 
         kwargs: Dict[str, Any] = {
             "method": "get",
             "url": f"/contributions",  # noqa: F541
             "timeout": timeout,
             "params": dict(
                 job_id=job_id,
-                dataset_id=dataset_id,
             ),
         }
 
         return Contributions(**self.client.request(**kwargs))
 
     def get_explained_variance(
         self,
```

### Comparing `octopize_avatar-0.7.3/avatars/api_batch_test.py` & `octopize_avatar-0.7.4/avatars/api_batch_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/api_custom_dataset_methods_test.py` & `octopize_avatar-0.7.4/avatars/api_custom_dataset_methods_test.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,22 @@
 import io
+import os
 import tempfile
 from pathlib import Path
 from typing import IO, Any, Dict, Iterator, Union
 from unittest.mock import patch
 from uuid import uuid4
 
 import httpx
 import pandas as pd
+import pyarrow.dataset as ds
 import pytest
 
 from avatars.api import Datasets, PandasIntegration
 from avatars.conftest import RequestHandle, api_client_factory
-from avatars.exceptions import InvalidFileType
 from avatars.models import Dataset, FileType
 
 TEST_MAX_BYTES_PER_FILE = 1 * 1024  # 1 KB
 
 
 @pytest.fixture(scope="session")
 def dataset_json() -> dict[str, Any]:
@@ -98,31 +99,30 @@
     def test_create_dataset_neither_request_nor_source_raises(self) -> None:
         client = api_client_factory()
         with pytest.raises(ValueError, match="You need to pass in a source"):
             Datasets(client).create_dataset()
 
     def test_create_dataset_with_unknown_source_type(self) -> None:
         client = api_client_factory()
-        with pytest.raises(
-            TypeError, match="Expected source to be a string or a buffer"
-        ):
-            Datasets(client).create_dataset(source=1)  # type:ignore[arg-type]
+        with pytest.raises(TypeError, match="Unsupported dataset source"):
+            Datasets(client).create_dataset(source=1)
 
     @pytest.mark.parametrize("content_fixture_name", ["csv_content", "parquet_content"])
     def test_create_dataset_using_source_argument_with_filename(
         self,
         create_dataset_response: RequestHandle,
         content_fixture_name: str,
         request: Any,
     ) -> None:
         # Arrange
         content = request.getfixturevalue(content_fixture_name)
         client = api_client_factory(create_dataset_response)
         with tempfile.NamedTemporaryFile() as tmp_file:
             tmp_file.write(content)
+            tmp_file.flush()
             result = Datasets(client).create_dataset(source=tmp_file.name)
 
         assert result.id
 
     @patch("avatars.api.MAX_BYTES_PER_FILE", TEST_MAX_BYTES_PER_FILE)
     @pytest.mark.parametrize(
         "filetype, buffer",
@@ -149,26 +149,25 @@
             filled_buffer = buffer(pd.read_csv(io.BytesIO(large_csv)).to_parquet())
 
         filled_buffer.seek(0)
         # TODO: Remove the type ignore when request is deprecated
         # error: Argument "source" to "create_dataset" of "Datasets"
         #        has incompatible type "Union[BytesIO, StringIO]"; expected
         #        "Union[str, list[str], FileLikeInterface[bytes], None]"  [arg-type]
-        res = Datasets(client).create_dataset(source=filled_buffer)  # type: ignore[arg-type]
+        res = Datasets(client).create_dataset(source=filled_buffer)
         assert res.id
 
     @patch("avatars.api.MAX_BYTES_PER_FILE", TEST_MAX_BYTES_PER_FILE)
     @pytest.mark.parametrize(
         "filetype, mode",
         [
             (
                 FileType.csv,
                 "rb",
             ),
-            (FileType.csv, "r"),
             (FileType.parquet, "rb"),
         ],
         ids=str,
     )
     def test_create_dataset_with_too_large_file_ok(
         self,
         create_dataset_response: RequestHandle,
@@ -244,30 +243,30 @@
     def test_create_dataset_with_deprecated_request_argument_calls_correct_request_method(
         self, create_dataset_response: RequestHandle, dataset_json: dict[str, Any]
     ) -> None:
         client = api_client_factory(create_dataset_response)
 
         with patch.object(client, "request", return_value=dataset_json) as mock_request:
             # as positional argument
-            Datasets(client).create_dataset(io.BytesIO(b"123"))
+            Datasets(client).create_dataset(io.BytesIO(b"123\n"))
             mock_request.assert_called_once()
 
-            file_ = mock_request.call_args.kwargs["file"][0][1]
+            ds_ = mock_request.call_args.kwargs["dataset"]
 
-            assert isinstance(file_, io.BytesIO)
+            assert isinstance(ds_, ds.Dataset)
 
             mock_request.reset_mock()
 
             # as keyword argument
-            Datasets(client).create_dataset(request=io.BytesIO(b"123"))
+            Datasets(client).create_dataset(request=io.BytesIO(b"123\n"))
             mock_request.assert_called_once()
 
-            file_ = mock_request.call_args.kwargs["file"][0][1]
+            ds_ = mock_request.call_args.kwargs["dataset"]
 
-            assert isinstance(file_, io.BytesIO)
+            assert isinstance(ds_, ds.Dataset)
 
 
 @pytest.fixture(scope="session")
 def download_dataset_parquet_response(parquet_content: bytes) -> httpx.Response:
     return httpx.Response(
         200,
         content=parquet_content,
@@ -452,14 +451,15 @@
             filetype=filetype,
         )
 
         assert response is None
 
         expected: bytes = request.getfixturevalue(expected_output_fixture_name)
 
+        destination.seek(0, os.SEEK_SET)
         actual = destination.read()
         actual = actual if isinstance(actual, bytes) else actual.encode()
 
         assert actual == expected
 
     def test_download_dataset_with_invalid_destination_fails(
         self,
@@ -471,43 +471,14 @@
             TypeError, match="Expected destination to be a string or a buffer"
         ):
             Datasets(client).download_dataset(
                 str(uuid4()),
                 destination=123,  # type: ignore[arg-type]
             )
 
-    @pytest.mark.parametrize(
-        "destination",
-        [
-            pytest.param(io.StringIO(), id="io.StringIO"),
-            pytest.param("file_handle"),
-        ],
-    )
-    def test_download_dataset_with_invalid_filetype_fails(
-        self,
-        download_dataset_response: RequestHandle,
-        destination: Union[str, IO[Union[str]]],
-        request: Any,
-    ) -> None:
-        """Verify that the method raises an error with an invalid filetype."""
-
-        # grab the open file handle fixture
-        destination = (
-            request.getfixturevalue(destination)
-            if isinstance(destination, str)
-            else destination
-        )
-        client = api_client_factory(download_dataset_response)
-        with pytest.raises(
-            InvalidFileType, match="Can't download parquet files as string."
-        ):
-            Datasets(client).download_dataset(
-                str(uuid4()), filetype=FileType.parquet, destination=destination
-            )
-
 
 class TestPandasIntegrationUploadDataframe:
     @pytest.fixture
     def dataframe(self) -> pd.DataFrame:
         return pd.DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]})
 
     @pytest.fixture(scope="session")
```

### Comparing `octopize_avatar-0.7.3/avatars/base_client.py` & `octopize_avatar-0.7.4/avatars/base_client.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,62 +1,78 @@
 from __future__ import annotations
 
 import itertools
 import logging
+import os
 import time
-from contextlib import contextmanager
+from contextlib import ExitStack, contextmanager
 from copy import deepcopy
 from dataclasses import dataclass, field
 from datetime import datetime
 from io import BytesIO
 from json import loads as json_loads
+from pathlib import Path
 from typing import (
-    TYPE_CHECKING,
     Any,
     Callable,
     Dict,
     Generator,
+    Iterable,
     Iterator,
     Mapping,
     Optional,
     Sequence,
     Type,
     TypeVar,
     Union,
     cast,
-    overload,
 )
 
 import httpx
 from httpx import ReadTimeout, Request, Response, WriteTimeout
 from pydantic import BaseModel
 
+from avatars.arrow_utils import (
+    ArrowStreamReader,
+    ArrowStreamWriter,
+    FileLike,
+    FileLikes,
+    is_text_file_or_buffer,
+)
 from avatars.models import JobStatus
-from avatars.utils import ensure_valid, pop_or, remove_optionals, validated
-
-if TYPE_CHECKING:
-    from avatars._typing import FileLikeInterface, HttpxFile
+from avatars.utils import ContentType, ensure_valid, pop_or, remove_optionals, validated
 
 logger = logging.getLogger(__name__)
 
 DEFAULT_RETRY_TIMEOUT = 60
 DEFAULT_RETRY_INTERVAL = 5
 DEFAULT_RETRY_COUNT = 20
 DEFAULT_TIMEOUT = 60 * 4
 DEFAULT_PER_CALL_TIMEOUT = 15
 
 IN_PROGRESS_STATUSES = (JobStatus.pending, JobStatus.started)
 
-DEFAULT_BINARY_CONTENT_TYPES = ("application/pdf", "application/octet-stream")
+DEFAULT_BINARY_CONTENT_TYPES = (
+    ContentType.PDF,
+    ContentType.OCTET_STREAM,
+    ContentType.ARROW_STREAM,
+)
+DEFAULT_TEXT_CONTENT_TYPES = (ContentType.CSV, ContentType.JSON)
 
 T = TypeVar("T")
 R = TypeVar("R")
 RequestClass = TypeVar("RequestClass", bound=BaseModel)
 ResponseClass = TypeVar("ResponseClass", bound=BaseModel)
 
+JsonLike = dict[str, Any]
+
+Content = Union[Iterable[bytes], bytes]
+UserContent = Union[JsonLike, str, bytes, Optional[BytesIO]]
+StreamedContent = Optional[Union[BytesIO, bytes, str]]
+
 
 def _get_nested_value(
     obj: Union[Mapping[Any, Any], Sequence[Any]], key: str, default: Any = None
 ) -> Any:
     """
     Return value from (possibly) nested key in JSON dictionary.
     """
@@ -134,18 +150,20 @@
     headers: dict[str, str]
     http_request: Optional[Request] = None
     http_response: Optional[Response] = None
     timeout: float = 0.0
     params: Optional[Dict[str, Any]] = None
     json_data: Optional[BaseModel] = None
     form_data: Optional[Union[BaseModel, Dict[str, Any]]] = None
-    file: Optional[Sequence["HttpxFile"]] = None
+    files: Optional[FileLikes] = None
+    content: Optional[Content] = None
     should_verify_auth: bool = True
     should_stream: bool = False
-    destination: Optional["FileLikeInterface[bytes]"] = None
+    destination: Optional[FileLike] = None
+    want_content: bool = False
 
     def update(self, **kwargs: Any) -> None:
         for k, v in kwargs.items():
             if hasattr(self, k):
                 setattr(self, k, v)
 
     def build_params_arg(self) -> Optional[Dict[str, Any]]:
@@ -187,30 +205,41 @@
             return True
 
         return False
 
     def get_header(self, header: str) -> Any:
         return ensure_valid(self.http_response).headers[header]
 
+    def content_type(self) -> ContentType:
+        return ContentType(self.get_header("content-type").split(";")[0].strip())
+
     def is_created(self) -> bool:
         return self.status_is(httpx.codes.CREATED) and self.has_header("location")
 
-    def ensure_created(self) -> None:
-        self.ensure_status_is(httpx.codes.CREATED) and self.has_header("location")
+    def ensure_created(self) -> bool:
+        return self.ensure_status_is(httpx.codes.CREATED) and self.has_header(
+            "location"
+        )
 
     def is_content_json(self) -> bool:
-        return bool(self.get_header("content-type") == "application/json")
+        return bool(self.content_type() == ContentType.JSON)
+
+    def is_content_arrow(self) -> bool:
+        return bool(self.content_type() == ContentType.ARROW_STREAM)
+
+    def is_content_text(self) -> bool:
+        return self.content_type() in DEFAULT_TEXT_CONTENT_TYPES
 
     def is_content_binary(self) -> bool:
-        return self.get_header("content-type") in DEFAULT_BINARY_CONTENT_TYPES
+        return self.content_type() in DEFAULT_BINARY_CONTENT_TYPES
 
-    def get_user_content(self) -> Any:
+    def get_user_content(self) -> UserContent:
         with validated(self.http_response, "response") as resp:
             if self.should_stream:
-                return self.stream_response(resp, destination=None)
+                return self.stream_response()
             else:
                 if self.is_content_json():
                     return self.response_to_json()
                 elif self.is_content_binary():
                     return resp.content
                 else:
                     return resp.text
@@ -221,25 +250,34 @@
         as_json: Dict[str, Any] = {}
 
         if self.is_content_json():
             as_json = resp.json()
 
         return as_json
 
-    @overload
-    def stream_response(self, resp: Response, destination: None) -> bytes: ...
-
-    @overload
-    def stream_response(
-        self, resp: Response, destination: "FileLikeInterface[bytes]"
-    ) -> None: ...
+    def stream_response_content(self, destination: FileLike) -> None:
+        with validated(self.http_response, "response") as resp:
+            if self.is_content_arrow():
+                with ArrowStreamReader() as reader:
+                    reader.write_parquet(destination, resp.iter_bytes())
+            else:
+                try:
+                    if is_text_file_or_buffer(destination):
+                        for chunk in resp.iter_text():
+                            destination.write(chunk)  # type: ignore[call-overload]
+                    else:
+                        # Assume bytes...
+                        for chunk in resp.iter_bytes():  # type: ignore[assignment]
+                            destination.write(chunk)  # type: ignore[call-overload]
+                finally:
+                    resp.close()
 
     def stream_response(
-        self, resp: Response, destination: Optional["FileLikeInterface[bytes]"] = None
-    ) -> Any:
+        self, destination: Optional[FileLike] = None
+    ) -> StreamedContent:
         """
         Handle the streaming of a response to a destination.
 
         If the destination is not provided, it returns the content as bytes.
 
         This needs the httpx.Client instance to remain open, even though no client
         is used in this function.
@@ -252,27 +290,35 @@
             The destination where the response will be streamed.
             If not provided, the content is returned as.
 
         Returns
         -------
             If no destination was provided, it returns the raw bytes.
         """
+        content: StreamedContent = None
+        buffer = BytesIO()
 
-        _destination: "FileLikeInterface[bytes]" = destination or BytesIO()
+        if isinstance(destination, str):
+            destination_data = open(destination, "wb")
+        else:
+            destination_data = destination or buffer
+
+        self.stream_response_content(destination_data)
+
+        buffer.seek(0, os.SEEK_SET)
+
+        content = buffer if not destination else None
 
-        try:
-            for chunk in resp.iter_bytes():
-                _destination.write(chunk)
-        finally:
-            resp.close()
+        if self.want_content and content:
+            content = content.read()
 
-        if not destination:
-            return _destination.read()
+            if self.is_content_text():
+                content = content.decode()
 
-        return None
+        return content
 
     def clone(self) -> ContextData:
         return deepcopy(self)
 
 
 @dataclass
 class OperationInfo:
@@ -291,15 +337,16 @@
     def build_request(self) -> Request:
         self.data.http_request = self.http_client.build_request(
             method=self.data.method,
             url=self.data.url,
             params=self.data.build_params_arg(),
             json=self.data.build_json_data_arg(),
             data=self.data.build_form_data_arg(),
-            files=self.data.file,  # type: ignore[arg-type]
+            files=self.data.files,  # type: ignore[arg-type]
+            content=self.data.content,
             headers=self.data.headers,
             timeout=DEFAULT_PER_CALL_TIMEOUT,
         )
 
         return ensure_valid(self.data.http_request)
 
     def send_request(self) -> Response:
@@ -519,37 +566,85 @@
         self.verify_auth = verify_auth
         self._http_client = http_client
         self._headers = {"Avatars-Accept-Created": "yes"} | headers
 
     def set_header(self, key: str, value: str) -> None:
         self._headers[key] = value
 
+    def prepare_files(
+        self, stack: ExitStack, headers: dict[str, Any], keyword_args: dict[str, Any]
+    ) -> Optional[FileLikes]:
+        files: Any = pop_or(keyword_args, "files", [])
+        files = files if isinstance(files, list) else [files]
+
+        if f := pop_or(keyword_args, "file", None):
+            files.append(f)
+
+        prepared_files: Optional[FileLikes] = None
+
+        if files:
+            prepared_files = []
+
+            for f in files:
+                if isinstance(f, str) and Path(f).is_file():
+                    prepared_files.append(stack.enter_context(open(f, "rb")))
+                else:
+                    raise ValueError(
+                        f"Expected streamable file-like object, got {f} instead"
+                    )
+
+        return prepared_files
+
+    def prepare_content(
+        self, stack: ExitStack, headers: dict[str, Any], keyword_args: dict[str, Any]
+    ) -> Optional[Content]:
+        content: Optional[Content] = None
+
+        if "dataset" in keyword_args:
+            content = ArrowStreamWriter(keyword_args.pop("dataset"))
+            headers["Content-Type"] = ContentType.ARROW_STREAM.value
+
+        return content
+
     @contextmanager
     def context(
         self, *, ctx: Optional[ClientContext] = None, **kwargs: Any
     ) -> Generator[ClientContext, None, None]:
-        http_client = self._http_client or httpx.Client(
-            base_url=self.base_url, timeout=self.timeout, verify=self.should_verify_ssl
-        )
-
-        # Grab special keys
-        headers: dict[str, Any] = pop_or(kwargs, "headers", {})
-
-        if not ctx:
-            ctx = ClientContext(
-                http_client=http_client,
-                data=ContextData(
-                    base_url=self.base_url, headers=self._headers.copy(), **kwargs
-                ),
+        with ExitStack() as stack:
+            http_client = self._http_client or httpx.Client(
+                base_url=self.base_url,
+                timeout=self.timeout,
+                verify=self.should_verify_ssl,
             )
 
-        ctx.data.update(**kwargs)
-        ctx.data.headers.update(headers)
+            # Grab special keys
+            headers: dict[str, Any] = pop_or(kwargs, "headers", {})
+            files = self.prepare_files(stack, headers, kwargs)
+            content = self.prepare_content(stack, headers, kwargs)
+            want_content: bool = pop_or(kwargs, "want_content", False)
+
+            if not ctx:
+                ctx = ClientContext(
+                    http_client=http_client,
+                    data=ContextData(
+                        base_url=self.base_url, headers=self._headers.copy(), **kwargs
+                    ),
+                )
 
-        yield ctx
+            ctx.data.update(**kwargs)
+            ctx.data.headers.update(headers)
+            ctx.data.files = files
+            ctx.data.content = content
+            ctx.data.want_content = want_content
+
+            yield ctx
+
+            ctx.data.files = None
+            ctx.data.content = None
+            ctx.data.want_content = False
 
     def create(
         self,
         *,
         url: str,
         request: RequestClass,
         response_cls: Type[ResponseClass],
@@ -569,21 +664,19 @@
 
             return cast(ResponseClass, info.response)
 
     def request(self, method: str, url: str, **kwargs: Any) -> Any:
         response: Any = None
 
         with self.context(method=method, url=url, **kwargs) as ctx:
-            response = ctx.build_and_send_request()
+            ctx.build_and_send_request()
+            destination = kwargs.get("destination", None)
 
-            if "destination" in kwargs:
-                ctx.data.stream_response(
-                    ensure_valid(ctx.data.http_response),
-                    destination=kwargs["destination"],
-                )
+            if destination:
+                response = ctx.data.stream_response(destination=destination)
             elif ctx.data.is_created():
                 response = self.wait_created(
                     url=ctx.data.get_header("location"),
                     update_func=update_request_op,
                     ctx=ctx,
                 ).response
             else:
```

### Comparing `octopize_avatar-0.7.3/avatars/client.py` & `octopize_avatar-0.7.4/avatars/client.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
+# API Version : 2.2.1-dd82c574fc3448fe2f02c09f29b8136f63017c14
 
 
 import warnings
 from typing import Optional
 from uuid import UUID
 
 import httpx
```

### Comparing `octopize_avatar-0.7.3/avatars/client_test.py` & `octopize_avatar-0.7.4/avatars/client_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/conftest.py` & `octopize_avatar-0.7.4/avatars/conftest.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/saferound.py` & `octopize_avatar-0.7.4/avatars/lib/saferound.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/saferound_test.py` & `octopize_avatar-0.7.4/avatars/lib/saferound_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/split.py` & `octopize_avatar-0.7.4/avatars/lib/split.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/split_column_types_test.py` & `octopize_avatar-0.7.4/avatars/lib/split_column_types_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/split_columns_types.py` & `octopize_avatar-0.7.4/avatars/lib/split_columns_types.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/lib/split_test.py` & `octopize_avatar-0.7.4/avatars/lib/split_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/models.py` & `octopize_avatar-0.7.4/avatars/models.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # This file has been generated - DO NOT MODIFY
-# API Version : 1.1.2-cc65e4fa4e46fdc93e7a375ce87a8202f1f59c1a
+# API Version : 2.2.1-dd82c574fc3448fe2f02c09f29b8136f63017c14
 
 
 from __future__ import annotations
 
 from datetime import datetime
 from enum import Enum
 from typing import Dict, List, Optional, Protocol, Union, runtime_checkable
@@ -11,15 +11,26 @@
 
 import pandas as pd
 from pydantic import BaseModel, ConfigDict, Field, RootModel
 from typing_extensions import Annotated, Literal
 
 # generated by datamodel-codegen:
 #   filename:  input.json
-#   timestamp: 2024-04-29T09:36:19+00:00
+#   timestamp: 2024-05-23T12:22:06+00:00
+
+
+class AdviceParameters(BaseModel):
+    """
+    Parameters for advice job.
+    """
+
+    model_config = ConfigDict(
+        extra="allow",
+    )
+    dataset_id: Annotated[UUID, Field(title="Dataset Id")]
 
 
 class AnalysisStatus(Enum):
     started = "started"
     done = "done"
 
 
@@ -246,14 +257,15 @@
     avatarization_with_time_series = "avatarization_with_time_series"
     avatarization_multi_table = "avatarization_multi_table"
     privacy_metrics_with_time_series = "privacy_metrics_with_time_series"
     privacy_metrics_multi_table = "privacy_metrics_multi_table"
     signal_metrics_with_time_series = "signal_metrics_with_time_series"
     privacy_metrics_geolocation = "privacy_metrics_geolocation"
     generic = "generic"
+    advice = "advice"
 
 
 class JobProgress(BaseModel):
     completion_rate_100: Annotated[int, Field(title="Completion Rate 100")]
     name: Annotated[Optional[str], Field(title="Name")] = None
     created_at: Annotated[datetime, Field(title="Created At")]
 
@@ -653,14 +665,18 @@
     error_message: Annotated[Optional[str], Field(title="Error Message")] = None
     traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[GenericResult] = None
     parameters: GenericParameters
     current_progress: Optional[JobProgress] = None
 
 
+class AdviceJobCreate(BaseModel):
+    parameters: AdviceParameters
+
+
 class AvatarizationTimeSeriesParameters(BaseModel):
     """
     Parameters for the anonymization of a Time Series.
     """
 
     dataset_id: Annotated[
         UUID,
@@ -1098,20 +1114,18 @@
     correlation_difference_ratio: Annotated[
         Optional[float], Field(title="Correlation Difference Ratio")
     ] = None
     targets: SignalMetricsTargets
 
 
 class SignalMetricsBatchJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.signal_metrics_batch
     parameters: SignalMetricsBatchParameters
 
 
 class SignalMetricsJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.signal_metrics
     parameters: SignalMetricsParameters
 
 
 class SignalMetricsPerBatchResult(BaseModel):
     hellinger_mean: Annotated[float, Field(title="Hellinger Mean")]
     hellinger_std: Annotated[float, Field(title="Hellinger Std")]
     correlation_difference_ratio: Annotated[
@@ -1130,15 +1144,14 @@
     ] = None
     targets: SignalMetricsTargets
     original_id: Annotated[UUID, Field(title="Original Id")]
     avatars_id: Annotated[UUID, Field(title="Avatars Id")]
 
 
 class SignalMetricsWithTimeSeriesJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.signal_metrics_with_time_series
     parameters: SignalMetricsWithTimeSeriesParameters
 
 
 class TableLink(BaseModel):
     parent_table: Annotated[
         TableReference, Field(description="Reference to the parent table.")
     ]
@@ -1756,15 +1769,14 @@
     privacy_metrics_per_scenario: Annotated[
         List[PrivacyMetricsGeolocationScenarioResult],
         Field(title="Privacy Metrics Per Scenario"),
     ]
 
 
 class PrivacyMetricsJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.privacy_metrics
     parameters: PrivacyMetricsParameters
 
 
 class PrivacyMetricsMultiTableParameters(BaseModel):
     """
     Parameters to create a multi table privacy metrics job.
 
@@ -1828,15 +1840,14 @@
     seed: Annotated[
         Optional[int],
         Field(description="Seed used to generate the random state.", title="Seed"),
     ] = None
 
 
 class PrivacyMetricsWithTimeSeriesJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.privacy_metrics_with_time_series
     parameters: PrivacyMetricsWithTimeSeriesParameters
 
 
 class SignalMetricsBatchResult(BaseModel):
     mean_metrics: SignalMetrics
     training_metrics: SignalMetricsPerBatchResult
     batch_metrics: Annotated[
@@ -1936,16 +1947,39 @@
     error_message: Annotated[Optional[str], Field(title="Error Message")] = None
     traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[MetaSignalMetrics] = None
     parameters: SignalMetricsWithTimeSeriesParameters
     current_progress: Optional[JobProgress] = None
 
 
+class Advice(BaseModel):
+    """
+    Recommendations on anonymization parameters and processors.
+    """
+
+    parameters: Annotated[
+        AvatarizationParameters, Field(description="Suggested AvatarizationParameters.")
+    ]
+    python_client_pipeline: Annotated[
+        str,
+        Field(
+            description="Pipeline ready to run, you need to replace <NAME_OF_YOUR_DF> by the name of your dataframe.",
+            title="Python Client Pipeline",
+        ),
+    ]
+    more_details: Annotated[
+        Optional[Dict[str, str]],
+        Field(
+            description="Additional information on parameters and processors.",
+            title="More Details",
+        ),
+    ] = {}
+
+
 class AvatarizationBatchJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.avatarization_batch
     parameters: AvatarizationBatchParameters
 
 
 class AvatarizationBatchResult(BaseModel):
     privacy_metrics: Optional[PrivacyMetrics] = None
     signal_metrics: Optional[SignalMetrics] = None
     training_result: AvatarizationPerBatchResult
@@ -2022,33 +2056,41 @@
 
 
 class AvatarizationMultiTableResult(BaseModel):
     datasets: Annotated[List[AvatarizationResultPerDataset], Field(title="Datasets")]
 
 
 class AvatarizationWithTimeSeriesJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.avatarization_with_time_series
     parameters: AvatarizationWithTimeSeriesParameters
 
 
 class PrivacyMetricsBatchJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.privacy_metrics_batch
     parameters: PrivacyMetricsBatchParameters
 
 
 class PrivacyMetricsGeolocationJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.privacy_metrics_geolocation
     parameters: PrivacyMetricsGeolocationParameters
 
 
 class PrivacyMetricsMultiTableJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.privacy_metrics_multi_table
     parameters: PrivacyMetricsMultiTableParameters
 
 
+class AdviceJob(BaseModel):
+    id: Annotated[UUID, Field(title="Id")]
+    kind: JobKind
+    created_at: Annotated[datetime, Field(title="Created At")]
+    status: JobStatus
+    error_message: Annotated[Optional[str], Field(title="Error Message")] = None
+    traceback: Annotated[Optional[str], Field(title="Traceback")] = None
+    result: Optional[Advice] = None
+    parameters: AdviceParameters
+    current_progress: Optional[JobProgress] = None
+
+
 class AvatarizationBatchJob(BaseModel):
     id: Annotated[UUID, Field(title="Id")]
     kind: JobKind
     created_at: Annotated[datetime, Field(title="Created At")]
     status: JobStatus
     error_message: Annotated[Optional[str], Field(title="Error Message")] = None
     traceback: Annotated[Optional[str], Field(title="Traceback")] = None
@@ -2066,15 +2108,14 @@
     traceback: Annotated[Optional[str], Field(title="Traceback")] = None
     result: Optional[AvatarizationMultiTableResult] = None
     parameters: AvatarizationMultiTableParameters
     current_progress: Optional[JobProgress] = None
 
 
 class AvatarizationMultiTableJobCreate(BaseModel):
-    kind: Optional[JobKind] = JobKind.avatarization_multi_table
     parameters: AvatarizationMultiTableParameters
 
 
 @runtime_checkable
 class Processor(Protocol):
     def preprocess(self, df: pd.DataFrame) -> pd.DataFrame: ...
```

### Comparing `octopize_avatar-0.7.3/avatars/processors/__init__.py` & `octopize_avatar-0.7.4/avatars/processors/__init__.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/conftest.py` & `octopize_avatar-0.7.4/avatars/processors/conftest.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/datetime.py` & `octopize_avatar-0.7.4/avatars/processors/datetime.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/datetime_test.py` & `octopize_avatar-0.7.4/avatars/processors/datetime_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/expected_mean.py` & `octopize_avatar-0.7.4/avatars/processors/expected_mean.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/expected_mean_test.py` & `octopize_avatar-0.7.4/avatars/processors/expected_mean_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/group_modalities.py` & `octopize_avatar-0.7.4/avatars/processors/group_modalities.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/group_modalities_test.py` & `octopize_avatar-0.7.4/avatars/processors/group_modalities_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_cumulated_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_cumulated_difference_test.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_cumulated_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_range_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_bounded_range_difference_test.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_bounded_range_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_cumulated_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_cumulated_difference_test.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_cumulated_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_range_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/inter_record_range_difference_test.py` & `octopize_avatar-0.7.4/avatars/processors/inter_record_range_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/perturbation.py` & `octopize_avatar-0.7.4/avatars/processors/perturbation.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/perturbation_test.py` & `octopize_avatar-0.7.4/avatars/processors/perturbation_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/proportions.py` & `octopize_avatar-0.7.4/avatars/processors/proportions.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/proportions_test.py` & `octopize_avatar-0.7.4/avatars/processors/proportions_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/relative_difference.py` & `octopize_avatar-0.7.4/avatars/processors/relative_difference.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/relative_difference_test.py` & `octopize_avatar-0.7.4/avatars/processors/relative_difference_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/to_categorical.py` & `octopize_avatar-0.7.4/avatars/processors/to_categorical.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/processors/to_categorical_test.py` & `octopize_avatar-0.7.4/avatars/processors/to_categorical_test.py`

 * *Files identical despite different names*

### Comparing `octopize_avatar-0.7.3/avatars/utils.py` & `octopize_avatar-0.7.4/avatars/utils.py`

 * *Files 22% similar despite different names*

```diff
@@ -7,14 +7,28 @@
 from toolz.dicttoolz import valfilter, valmap
 
 T = TypeVar("T")
 U = TypeVar("U")
 R = TypeVar("R")
 
 
+class ContentType(Enum):
+    MULTIPART_FROM_DATA = "multipart/form-data"
+    CSV = "text/csv"
+    PDF = "application/pdf"
+    JSON = "application/json"
+    OCTET_STREAM = "application/octet-stream"
+    ARROW_STREAM = "application/vnd.apache.arrow.stream"
+    UNSUPPORTED = "unsupported"
+
+    @classmethod
+    def _missing_(cls, value: object) -> ContentType:
+        return ContentType.UNSUPPORTED
+
+
 def ensure_valid(what: Optional[T], label: str = "") -> T:
     if what is None:
         msg = f"{label} " if label else ""
         raise RuntimeError(f"Expected valid {msg} argument, got None instead")
 
     return what
```

### Comparing `octopize_avatar-0.7.3/pyproject.toml` & `octopize_avatar-0.7.4/pyproject.toml`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 [build-system]
 requires = ["poetry-core>=1.0.0"]
 build-backend = "poetry.core.masonry.api"
 
 [tool.poetry]
 name = "octopize.avatar"
 # Also modify avatars/__init__.py
-version = "0.7.3"
+version = "0.7.4"
 description = "Python client for Octopize's avatar API"
 authors = ["Octopize <pypi-octopize@octopize.io>"]
 license = "Apache-2.0"
 packages = [{ include = "avatars" }]
 readme = "README.md"
 
 
@@ -39,14 +39,15 @@
 bandit = "^1.7.4"
 mypy = "^1.4"
 pytest = "^7.2.0"
 jupytext = "^1.14.2"
 nbconvert = "^7.2.6"
 flake8 = "^6.0.0"
 pytest-cov = "^4.1.0"
+structlog = "^24.1.0"
 
 [tool.poetry.group.tutorial.dependencies]
 seaborn = "^0.12.1"
 matplotlib = "^3.6.2"
 ipykernel = "^6.19.2"
 missingno = "^0.5.1"
 notebook = "^6.5.6"
@@ -90,7 +91,15 @@
 [[tool.mypy.overrides]]
 module = "toolz.*"
 ignore_missing_imports = true
 
 [[tool.mypy.overrides]]
 module = "pyarrow.*"
 ignore_missing_imports = true
+
+[tool.pytest.ini_options]
+filterwarnings = [
+  "error",
+  "ignore::pytest.PytestUnraisableExceptionWarning",
+  "ignore:np.find_common_type is deprecated:DeprecationWarning", # TODO: Remove this once we migrate to pandas v2,
+  "ignore::pluggy.PluggyTeardownRaisedWarning",                  # This because doubles and schemathesis use an old style hookwrapper teardown is outdated
+]
```

### Comparing `octopize_avatar-0.7.3/PKG-INFO` & `octopize_avatar-0.7.4/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: octopize.avatar
-Version: 0.7.3
+Version: 0.7.4
 Summary: Python client for Octopize's avatar API
 License: Apache-2.0
 Author: Octopize
 Author-email: pypi-octopize@octopize.io
 Requires-Python: >=3.9,<4.0
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: Python :: 3
```

